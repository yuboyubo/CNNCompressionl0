{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pamap2_dataset.ipynb","provenance":[],"collapsed_sections":["ZSWX9EHW97uU"],"machine_shape":"hm","authorship_tag":"ABX9TyMGy4biRGPFPnXTZGa+tOHc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"28xFqxTcxvpz"},"source":["## Connect Google Drive and GPU\n"]},{"cell_type":"code","metadata":{"id":"CexARwSGxGbm"},"source":["%reset\n","\n","# connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# connect colab gpu\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P8kydhB6xzBa"},"source":["## Import Needed Libraries, Paramaters and Functions"]},{"cell_type":"code","metadata":{"id":"7lbyFQSnxylK"},"source":["import sys\n","import csv\n","import time\n","import os.path\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.utils import prune\n","import torchvision\n","import matplotlib.pyplot as plt\n","from torch.utils.mobile_optimizer import optimize_for_mobile\n","from scipy import stats\n","from sklearn.utils import shuffle\n","\n","SEED = 10\n","WINDOW_SIZE = 128\n","FEATURE_SIZE = 40\n","LABEL_SIZE = 12\n","BATCH_SIZE = 3\n","PATH = '/content/drive/MyDrive/CNNPaper'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ca_WAZ8Qxdp3"},"source":["def dataCleaning(dataCollection):\n","  dataCollection = dataCollection.drop(['handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4',\n","                      'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4',\n","                      'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'],\n","                      axis = 1)  # removal of orientation columns as they are not needed\n","  dataCollection = dataCollection.drop(dataCollection[dataCollection.activityID == 0].index) #removal of any row of activity 0 as it is transient activity which it is not used\n","  dataCollection = dataCollection.apply(pd.to_numeric, errors = 'coerce') #removal of non numeric data in cells\n","  dataCollection = dataCollection.interpolate() #removal of any remaining NaN value cells by constructing new data points in known set of data points\n","  dataCollection.reset_index(drop = True, inplace = True)\n","  for i in range(0,4):\n","    dataCollection[\"heartrate\"].iloc[i]=100  \n","  return dataCollection\n","\n","def read_data():\n","  if os.path.isfile(PATH + '/data/PAMAP2/data.cvs'):\n","    print(\"Start reading data ...\")\n","    dataCollection = pd.read_csv(PATH + '/data/PAMAP2/data.cvs')\n","    print(\"Finish reading data ...\")\n","  else:\n","    print(\"Start reading data ...\")\n","    list_of_files = ['/data/PAMAP2/Protocol/subject101.dat',\n","              '/data/PAMAP2/Protocol/subject102.dat',\n","              '/data/PAMAP2/Protocol/subject103.dat',\n","              '/data/PAMAP2/Protocol/subject104.dat',\n","              '/data/PAMAP2/Protocol/subject105.dat',\n","              '/data/PAMAP2/Protocol/subject106.dat',\n","              '/data/PAMAP2/Protocol/subject107.dat',\n","              '/data/PAMAP2/Protocol/subject108.dat',\n","              '/data/PAMAP2/Protocol/subject109.dat' ]\n","    subjectID = [1,2,3,4,5,6,7,8,9]\n","    activityIDdict = {0: 'transient',\n","              1: 'lying',\n","              2: 'sitting',\n","              3: 'standing',\n","              4: 'walking',\n","              5: 'running',\n","              6: 'cycling',\n","              7: 'Nordic_walking',\n","              9: 'watching_TV',\n","              10: 'computer_work',\n","              11: 'car driving',\n","              12: 'ascending_stairs',\n","              13: 'descending_stairs',\n","              16: 'vacuum_cleaning',\n","              17: 'ironing',\n","              18: 'folding_laundry',\n","              19: 'house_cleaning',\n","              20: 'playing_soccer',\n","              24: 'rope_jumping'}\n","\n","    colNames = [\"timestamp\", \"activityID\",\"heartrate\"]\n","\n","    IMUhand = ['handTemperature', \n","          'handAcc16_1', 'handAcc16_2', 'handAcc16_3', \n","          'handAcc6_1', 'handAcc6_2', 'handAcc6_3', \n","          'handGyro1', 'handGyro2', 'handGyro3', \n","          'handMagne1', 'handMagne2', 'handMagne3',\n","          'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4']\n","\n","    IMUchest = ['chestTemperature', \n","          'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', \n","          'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', \n","          'chestGyro1', 'chestGyro2', 'chestGyro3', \n","          'chestMagne1', 'chestMagne2', 'chestMagne3',\n","          'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4']\n","\n","    IMUankle = ['ankleTemperature', \n","          'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', \n","          'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', \n","          'ankleGyro1', 'ankleGyro2', 'ankleGyro3', \n","          'ankleMagne1', 'ankleMagne2', 'ankleMagne3',\n","          'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4']\n","\n","    columns = colNames + IMUhand + IMUchest + IMUankle\n","\n","    dataCollection = pd.DataFrame()\n","    for file in list_of_files:\n","        procData = pd.read_table(PATH + file, header=None, sep='\\s+')\n","        procData.columns = columns\n","        procData['subject_id'] = int(file[-5])\n","        dataCollection = dataCollection.append(procData, ignore_index=True)\n","\n","    dataCollection.reset_index(drop=True, inplace=True)\n","    dataCollection = dataCleaning(dataCollection)\n","    dataCollection.to_csv('data.cvs', index=False)\n","    print(\"Finish reading data ...\")\n","  return dataCollection\n","\n","def feature_normalize(data):\n","  \"\"\"\n","    Normalize the feature data\n","    Paramater:\n","      data: a list of floats\n","    Return:\n","      a list of floats with normalized data\n","  \"\"\"\n","  mu = np.mean(data, axis=0)\n","  sigma = np.std(data, axis=0)\n","  return (data - mu) / sigma\n","\n","\n","def dataset_normalize(dataset):\n","  \"\"\"\n","    Normalize the whole dataset\n","    Paramater:\n","      dataset: a DataFrame with the data and labels \n","    Return:\n","      a DataFrame with the normalized data and labels \n","  \"\"\"\n","  dataset.dropna(axis=0, how='any', inplace=True)\n","  for col in dataset.columns:\n","    if col != 'timestamp' and col != 'activityID' and col != 'subject_id':\n","      dataset[col] = feature_normalize(dataset[col])\n","  return dataset\n","\n","def windows(data, size):\n","  \"\"\"\n","    Obatin the starting index and ending index according to window size\n","    Paramater:\n","      data: a list of floats\n","      size: int\n","    Return:\n","      Starting index, ending index\n","  \"\"\"\n","  start = 0\n","  while start < data.count():\n","    yield int(start), int(start + size)\n","    start += (size / 2)\n","\n","def dataset_segmentation(data):\n","  \"\"\"\n","    Dataset segmentation according the window size\n","    Paramater:\n","      data: a list of floats\n","    Return:\n","      segments and labels \n","  \"\"\"\n","  print(\"Start segmentation with window size: \", WINDOW_SIZE)\n","  segments = np.empty((0, WINDOW_SIZE, FEATURE_SIZE))\n","  labels = np.empty((0))\n","  size = data['timestamp'].count()\n","  for (start, end) in windows(data['timestamp'], WINDOW_SIZE):\n","      temp = []\n","      for col in data.columns:\n","        if col != 'timestamp' and col != 'activityID' and col != 'subject_id':\n","          array = data[col][start:end].to_numpy().tolist()\n","          temp.append(array)\n","      if len(data[\"timestamp\"][start:end]) == WINDOW_SIZE:\n","        segments = np.vstack([segments, np.dstack(temp)])\n","        labels = np.append(labels, stats.mode(data[\"activityID\"][start:end])[0][0])\n","  labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n","  segments = segments.reshape(len(segments), FEATURE_SIZE, WINDOW_SIZE)\n","  print(\"Finish segmentation ...\")\n","  return segments, labels\n","\n","def train_valid_test_split(segments, classes, test_x, test_y, k_fold):\n","  \"\"\"\n","    Split train, valid and test datase\n","    Paramater:\n","      segments: a list of input data\n","      classes: a list of classes data\n","      k: k fold cross validation\n","    Return:\n","      segments and labels \n","  \"\"\"\n","  print(\"Start dataset split... \")\n","  seg_len = len(segments)\n","  idx_val = [0, int(seg_len/5*1), int(seg_len/5*2), int(seg_len/5*3), int(seg_len/5*4), seg_len]\n","  train_range1 = range(0, idx_val[k_fold])\n","  valid_range = range(idx_val[k_fold], idx_val[k_fold+1])\n","  train_range2 = range(idx_val[k_fold+1], seg_len)\n","\n","  train_x = np.concatenate((segments[train_range1], segments[train_range2]), axis=0)\n","  train_y = np.concatenate((classes[train_range1], classes[train_range2]), axis=0)\n","  valid_x = segments[valid_range]\n","  valid_y = classes[valid_range]\n","\n","  # get train data\n","  train_data = []\n","  for i in range(len(train_x)):\n","    train_data.append([train_x[i], train_y[i]])\n","  \n","  # get valid data\n","  valid_data = []\n","  for i in range(len(valid_x)):\n","    valid_data.append([valid_x[i], valid_y[i]])\n","  \n","  # get test data\n","  test_data = []\n","  for i in range(len(test_x)):\n","    test_data.append([test_x[i], test_y[i]])\n","  print(len(train_data))\n","  print(len(valid_data))\n","  print(len(test_data))\n","\n","  # generate DataLoader for each dataset\n","  trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n","  validloader = torch.utils.data.DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n","  testloader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)\n","  \n","  print(\"Finish dataset split... \")\n","  return trainloader, validloader, testloader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load and Save Train, Test, Valid Dataset\n","\n","\n","\n"],"metadata":{"id":"ZSWX9EHW97uU"}},{"cell_type":"code","metadata":{"id":"dYOjY12143Iv"},"source":["TRAIN_LOADER_PATH = PATH + '/model/train_loader'\n","VALID_LOAER_PATH = PATH + '/model/valid_loader'\n","TEST_LOADER_PATH = PATH + '/model/test_loader'\n","\n","dataset = dataset_normalize(read_data())\n","segments, classes = dataset_segmentation(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(SEED)\n","segments, classes = shuffle(segments, classes)\n","test_x = segments[range(int(len(segments)*0.8), len(segments))]\n","test_y = classes[range(int(len(classes)*0.8), len(classes))]\n","total_x = segments[range(0, int(len(segments)*0.8))]\n","total_y = classes[range(0, int(len(classes)*0.8))]\n","\n","cross_valid_range = 5\n","\n","for k in range(cross_valid_range):\n","  print(\"Start spliting for k = \" + str(k))\n","  trainloader, validloader, testloader = train_valid_test_split(total_x, total_y, test_x, test_y, k)\n","  CROSS_TRAIN_LOADER_PATH = TRAIN_LOADER_PATH + str(k) + '.pkl'\n","  CROSS_VALID_LOADER_PATH = VALID_LOAER_PATH + str(k) + '.pkl'\n","  CROSS_TEST_LOADER_PATH = TEST_LOADER_PATH + str(k) + '.pkl'\n","  torch.save(trainloader, CROSS_TRAIN_LOADER_PATH)\n","  torch.save(validloader, CROSS_VALID_LOADER_PATH)\n","  torch.save(testloader, CROSS_TEST_LOADER_PATH)\n","  print(\"Finish data loading...\")"],"metadata":{"id":"Hqg4hp82-Q3Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load CNN Model and Other Helper Functions"],"metadata":{"id":"1EbXHbJ7-ae4"}},{"cell_type":"code","source":["# Here are the variables that you can modify\n","NODE_SIZE = 128\n","KERNAL_SIZE = 10\n","LEARNING_RATE = 0.0001\n","k = 0 # cross validation fold"],"metadata":{"id":"S74BaXLz9Ff3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TRAIN_LOADER_PATH = PATH + '/model/train_loader'\n","VALID_LOAER_PATH = PATH + '/model/valid_loader'\n","TEST_LOADER_PATH = PATH + '/model/test_loader'\n","CROSS_TRAIN_LOADER_PATH = TRAIN_LOADER_PATH + str(k) + '.pkl'\n","CROSS_VALID_LOADER_PATH = VALID_LOAER_PATH + str(k) + '.pkl'\n","CROSS_TEST_LOADER_PATH = TEST_LOADER_PATH + str(k) + '.pkl'\n","trainloader = torch.load(CROSS_TRAIN_LOADER_PATH)\n","validloader = torch.load(CROSS_VALID_LOADER_PATH)\n","testloader = torch.load(CROSS_TEST_LOADER_PATH)\n","\n","class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","\n","    # Convolutional Layers\n","    self.features = nn.Sequential(\n","      nn.Conv1d(FEATURE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","    )\n","  \n","    self.fc1 = nn.Linear(NODE_SIZE*(WINDOW_SIZE-5*(KERNAL_SIZE-1)), 100)\n","    self.fc2 = nn.Linear(100, LABEL_SIZE)\n","    self.max = nn.Softmax(dim=1)\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = x.view(x.shape[0], -1)\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)\n","    x = self.max(x)\n","    return x\n","\n","def train_save_CNN_model(TYPE, EPOCH_SIZE):\n","  # manually set random seed\n","  torch.backends.cudnn.deterministic = True\n","  torch.manual_seed(SEED)\n","\n","  # set gpu device\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  net = CNN().double().to(device)\n","\n","  # pick the criterion and optimizer\n","  criterion = nn.MultiLabelSoftMarginLoss()\n","  optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","\n","  print(\"Learning rate %.4f, batch size %d, node size %d, kernal size %d\" % (LEARNING_RATE, BATCH_SIZE, NODE_SIZE, KERNAL_SIZE))\n","\n","  # initialization\n","  train_acc_list = []\n","  val_acc_list = []\n","  test_acc_list = []\n","  accuray = 0\n","\n","  # start to train with epoches\n","  for epoch in range(EPOCH_SIZE):\n","    running_loss = 0.0\n","    train_total = 0\n","    train_correct = 0\n","    valid_total = 0\n","    valid_correct = 0\n","    test_total = 0\n","    test_correct = 0\n","\n","    # for the training dataset\n","    for i, data in enumerate(trainloader, 0):\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      optimizer.zero_grad()\n","      outputs = net(inputs)\n","      train_total += labels.size(0)\n","      train_correct += (torch.max(outputs, 1)[1] == torch.max(labels, 1)[1]).sum().item()\n","      loss = criterion(outputs, labels)\n","      if TYPE == 'l0_norm':\n","        # add group lasso regularization\n","        lgl = 1e-10\n","        cnt = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            cnt = cnt + param.detach().nonzero().size(0)\n","            #cnt = cnt + len(param.detach()[param.detach() > 1e-2]) + len(param.detach()[param.detach() < -1e-2])\n","        loss = loss + lgl * cnt\n","      elif TYPE == 'l1_norm':\n","        # add group lasso regularization\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            regularization = regularization + torch.norm(param, 1)\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'l2_norm':\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            regularization = regularization + torch.norm(param)\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'group_lasso':\n","        # add group lasso regularization\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              regularization = regularization + torch.norm(param[i,:,:])\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'l1_group_lasso':\n","        lgl = 0.000001\n","        alpha = 0.90\n","        group_lasso_regularization = torch.tensor([0]).cuda(0)\n","        lasso_regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              group_lasso_regularization = group_lasso_regularization + torch.norm(param[i,:,:])\n","            lasso_regularization = lasso_regularization + torch.norm(param, 1)\n","        loss = loss + (1-alpha) * lgl * group_lasso_regularization + alpha * lgl * lasso_regularization\n","      elif TYPE == 'l0_group_lasso':\n","        l0 = 1e-8\n","        lg = 0.4*1e-4\n","        cnt = torch.tensor([0]).cuda(0)\n","        group_lasso_regularization = torch.tensor([0]).cuda(0)\n","        lasso_regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              group_lasso_regularization = group_lasso_regularization + torch.norm(param[i,:,:])\n","            cnt += param.detach().nonzero().size(0)\n","        loss = loss + lg * group_lasso_regularization + l0 * cnt\n","      loss.backward()\n","      optimizer.step()\n","      running_loss += loss.item()\n","\n","    # for the validation dataset\n","    for data in validloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      valid_total += labels.size(0)\n","      valid_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    \n","    # for the test dataset\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    \n","    # obtain the results for training, validation, test dataset\n","    train_acc = 100 * train_correct / train_total\n","    valid_acc = 100 * valid_correct / valid_total\n","    test_acc = 100 * test_correct / test_total\n","    train_acc_list.append(train_acc)\n","    val_acc_list.append(valid_acc)\n","    test_acc_list.append(test_acc)\n","    print(\"epoch %d, loss %.3f, train acc %.2f%%, valid acc %.2f%%, test acc %.2f%%\" % (epoch+1, running_loss, train_acc, valid_acc, test_acc))\n","    \n","    # save the best model\n","    if valid_acc > accuray:\n","      accuray = valid_acc\n","      torch.save(net, PATH + '/model/' + TYPE + str(k) + \".ptl\")\n","    \n","  return train_acc_list, val_acc_list, test_acc_list"],"metadata":{"id":"JavmnjK--huZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results for CNN Model"],"metadata":{"id":"czSNT82R-ulP"}},{"cell_type":"code","source":["TYPE = 'no_penalty'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"_3c63xqyBD7n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l0_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"-uPhfeDeispe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l1_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"Zuipx8Leis0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l2_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"z4divxnQis-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"iMsQoLw_izl-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l1_group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"ZgFRk0pAi1C-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l0_group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)\n","print(train_acc)\n","print(valid_acc)\n","print(test_acc)"],"metadata":{"id":"I37ScZ1Y-477"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results after Pruning the above Models"],"metadata":{"id":"HN48kt-K2mpC"}},{"cell_type":"markdown","source":["Nonzero parameter results:"],"metadata":{"id":"zLMP44Jg_XIK"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = 0.01\n","\n","class ThresholdPruning(prune.BasePruningMethod):\n","    PRUNING_TYPE = \"unstructured\"\n","\n","    def __init__(self, threshold):\n","        self.threshold = threshold\n","\n","    def compute_mask(self, tensor, default_mask):\n","      return torch.abs(tensor) > self.threshold\n","\n","PATHS = { 'No penalty - 0 128 0.0001':     PATH + '/model/final/lr0.0001/no_penalty0.ptl',\n","      'l0 norm - 0 128 0.0001':       PATH + '/model/final/lr0.0001/l0_norm0.ptl',\n","      'l1 norm - 0 128 0.0001':       PATH + '/model/final/lr0.0001/l1_norm0.ptl',\n","      'l2 norm - 0 128 0.0001':       PATH + '/model/final/lr0.0001/l2_norm0.ptl',\n","      'group lasso - 0 128 0.0001':     PATH + '/model/final/lr0.0001/group_lasso0.ptl',\n","      'l1 group lasso - 0 128 0.0001':   PATH + '/model/final/lr0.0001/l1_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.0001 (*)': PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.00005':   PATH + '/model/final/lr0.00005/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.00015':   PATH + '/model/final/lr0.00015/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.0002':   PATH + '/model/final/lr0.0002/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.00001':   PATH + '/model/final/lr0.00001/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 256 0.0001':   PATH + '/model/final/256node/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 64 0.0001':   PATH + '/model/final/64node/l0_group_lasso0.ptl',\n","      'l0 group lasso - 1 128 0.0001':   PATH + '/model/final/lr0.0001/l0_group_lasso1.ptl',\n","      'l0 group lasso - 2 128 0.0001':   PATH + '/model/final/lr0.0001/l0_group_lasso2.ptl',\n","      'l0 group lasso - 3 128 0.0001':   PATH + '/model/final/lr0.0001/l0_group_lasso3.ptl',\n","      'l0 group lasso - 4 128 0.0001':   PATH + '/model/final/lr0.0001/l0_group_lasso4.ptl',\n","    }\n","\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net = torch.load(PATHS[name])\n","\n","  # display the results before compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% before compression' % (test_total, test_acc))\n","\n","  # prune the model\n","  parameters_to_prune = []\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      parameters_to_prune.append((child, \"weight\"))\n","  prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold=PRUNE_THRESHOLD)\n","\n","  # calculate the sparsity\n","  total_weight = 0\n","  total_nonzero = 0\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      total_weight += torch.numel(child.weight)\n","      total_nonzero += torch.count_nonzero(child.weight)\n","  print('Nonzero Parameter for the compressed model: %.2f %%' % (100*float(total_nonzero / total_weight)))\n","\n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n"],"metadata":{"id":"64evT_NP2sdg","executionInfo":{"status":"ok","timestamp":1640660537792,"user_tz":300,"elapsed":22370,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5b4b4abf-9ff0-4b0e-9911-1aee5ee9b69b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for No penalty - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 93.15 % before compression\n","Nonzero Parameter for the compressed model: 69.27 %\n","Accuracy of the network on the 6072 test data: 93.07 % after compression\n","\n","Here are the results for l0 norm - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 93.15 % before compression\n","Nonzero Parameter for the compressed model: 69.27 %\n","Accuracy of the network on the 6072 test data: 93.07 % after compression\n","\n","Here are the results for l1 norm - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 95.22 % before compression\n","Nonzero Parameter for the compressed model: 1.46 %\n","Accuracy of the network on the 6072 test data: 95.29 % after compression\n","\n","Here are the results for l2 norm - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 92.08 % before compression\n","Nonzero Parameter for the compressed model: 65.32 %\n","Accuracy of the network on the 6072 test data: 92.09 % after compression\n","\n","Here are the results for group lasso - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 93.30 % before compression\n","Nonzero Parameter for the compressed model: 61.78 %\n","Accuracy of the network on the 6072 test data: 93.28 % after compression\n","\n","Here are the results for l1 group lasso - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 96.87 % before compression\n","Nonzero Parameter for the compressed model: 2.67 %\n","Accuracy of the network on the 6072 test data: 97.20 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.0001 (*):\n","Accuracy of the network on the 6072 test data: 96.89 % before compression\n","Nonzero Parameter for the compressed model: 1.26 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.00005:\n","Accuracy of the network on the 6072 test data: 94.25 % before compression\n","Nonzero Parameter for the compressed model: 3.90 %\n","Accuracy of the network on the 6072 test data: 93.89 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.00015:\n","Accuracy of the network on the 6072 test data: 96.57 % before compression\n","Nonzero Parameter for the compressed model: 1.12 %\n","Accuracy of the network on the 6072 test data: 96.62 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.0002:\n","Accuracy of the network on the 6072 test data: 94.89 % before compression\n","Nonzero Parameter for the compressed model: 0.68 %\n","Accuracy of the network on the 6072 test data: 94.99 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.00001:\n","Accuracy of the network on the 6072 test data: 93.63 % before compression\n","Nonzero Parameter for the compressed model: 7.93 %\n","Accuracy of the network on the 6072 test data: 85.80 % after compression\n","\n","Here are the results for l0 group lasso - 0 256 0.0001:\n","Accuracy of the network on the 6072 test data: 94.35 % before compression\n","Nonzero Parameter for the compressed model: 0.46 %\n","Accuracy of the network on the 6072 test data: 93.87 % after compression\n","\n","Here are the results for l0 group lasso - 0 64 0.0001:\n","Accuracy of the network on the 6072 test data: 96.15 % before compression\n","Nonzero Parameter for the compressed model: 6.00 %\n","Accuracy of the network on the 6072 test data: 96.01 % after compression\n","\n","Here are the results for l0 group lasso - 1 128 0.0001:\n","Accuracy of the network on the 6072 test data: 92.29 % before compression\n","Nonzero Parameter for the compressed model: 1.27 %\n","Accuracy of the network on the 6072 test data: 92.28 % after compression\n","\n","Here are the results for l0 group lasso - 2 128 0.0001:\n","Accuracy of the network on the 6072 test data: 96.49 % before compression\n","Nonzero Parameter for the compressed model: 1.81 %\n","Accuracy of the network on the 6072 test data: 96.28 % after compression\n","\n","Here are the results for l0 group lasso - 3 128 0.0001:\n","Accuracy of the network on the 6072 test data: 95.08 % before compression\n","Nonzero Parameter for the compressed model: 1.20 %\n","Accuracy of the network on the 6072 test data: 94.99 % after compression\n","\n","Here are the results for l0 group lasso - 4 128 0.0001:\n","Accuracy of the network on the 6072 test data: 94.81 % before compression\n","Nonzero Parameter for the compressed model: 1.46 %\n","Accuracy of the network on the 6072 test data: 94.81 % after compression\n","\n"]}]},{"cell_type":"markdown","source":["Save pruning model to android model "],"metadata":{"id":"2KVDfdYpAGl_"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = 0.01\n","\n","class FooBarPruningMethod(prune.BasePruningMethod):\n","    \"\"\"Prune every other entry in a tensor\n","    \"\"\"\n","    PRUNING_TYPE = 'unstructured'\n","\n","    def compute_mask(self, t, default_mask):\n","      return torch.abs(t) > PRUNE_THRESHOLD\n","\n","def foobar_unstructured(module, name):\n","    FooBarPruningMethod.apply(module, name)\n","    return module\n","\n","PATHS = {'l0_norm':       PATH + '/model/final/lr0.0001/l0_norm',\n","      'l1_norm':       PATH + '/model/final/lr0.0001/l1_norm',\n","      'l2_norm':       PATH + '/model/final/lr0.0001/l2_norm',\n","      'group_lasso':     PATH + '/model/final/lr0.0001/group_lasso',\n","      'l1_group_lasso':   PATH + '/model/final/lr0.0001/l1_group_lasso',\n","      'l0_group_lasso': PATH + '/model/final/lr0.0001/l0_group_lasso',\n","    }\n","\n","for name in PATHS:\n","  # load the model\n","  net_uncompress = torch.load(PATHS[name] + \"0.ptl\")\n","  torch.save(net_uncompress, PATH + '/model/android_model/cuda_model/' + name + \"_uncompressed.ptl\")\n","\n","  foobar_unstructured(net_uncompress.features[0], \"weight\")\n","  foobar_unstructured(net_uncompress.features[2], \"weight\")\n","  foobar_unstructured(net_uncompress.features[4], \"weight\")\n","  foobar_unstructured(net_uncompress.features[6], \"weight\")\n","  foobar_unstructured(net_uncompress.features[8], \"weight\")\n","\n","  net_compress = torch.load(PATHS[name] + \"0.ptl\")\n","  net_compress.features[0].weight = torch.nn.Parameter(net_uncompress.features[0].weight)\n","  net_compress.features[2].weight = torch.nn.Parameter(net_uncompress.features[2].weight)\n","  net_compress.features[4].weight = torch.nn.Parameter(net_uncompress.features[4].weight)\n","  net_compress.features[6].weight = torch.nn.Parameter(net_uncompress.features[6].weight)\n","  net_compress.features[8].weight = torch.nn.Parameter(net_uncompress.features[8].weight)\n","  \n","  torch.save(net_compress, PATH + '/model/android_model/cuda_model/' + name + \"_compressed.ptl\")\n"],"metadata":{"id":"r49GBD1d_SV6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save cuda model to cpu model"],"metadata":{"id":"XqyEndbFAvL7"}},{"cell_type":"code","source":["PATHS = {'l0_norm':       PATH + '/model/android_model/cuda_model/l0_norm',\n","      'l1_norm':       PATH + '/model/android_model/cuda_model/l1_norm',\n","      'l2_norm':       PATH + '/model/android_model/cuda_model/l2_norm',\n","      'group_lasso':     PATH + '/model/android_model/cuda_model/group_lasso',\n","      'l1_group_lasso':   PATH + '/model/android_model/cuda_model/l1_group_lasso',\n","      'l0_group_lasso': PATH + '/model/android_model/cuda_model/l0_group_lasso',\n","    }\n","\n","for name in PATHS:\n","  # load the model\n","  net_uncompress = torch.load(PATHS[name] + \"_uncompressed.ptl\", map_location='cpu')\n","  torch.jit.script(net_uncompress)._save_for_lite_interpreter(PATH + '/model/android_model/cpu_model/' + name + \"_uncompressed.ptl\")\n","\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  torch.jit.script(net_compress)._save_for_lite_interpreter(PATH + '/model/android_model/cpu_model/' + name + \"_compressed.ptl\")\n"],"metadata":{"id":"E2V6Z4PiAoUr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Obtain the pruning results for l0 norm, l1 norm, l2 norm, group lasso, l0 group lasso and l1 group lasso"],"metadata":{"id":"V0D3U4OIBqxL"}},{"cell_type":"code","source":["PATHS = {'l0_group_lasso': PATH + '/model/android_model/cuda_model/l0_group_lasso',\n","      'l0_norm':       PATH + '/model/android_model/cuda_model/l0_norm',\n","      'l1_norm':       PATH + '/model/android_model/cuda_model/l1_norm',\n","      'l2_norm':       PATH + '/model/android_model/cuda_model/l2_norm',\n","      'group_lasso':     PATH + '/model/android_model/cuda_model/group_lasso',\n","      'l1_group_lasso':   PATH + '/model/android_model/cuda_model/l1_group_lasso',\n","    }\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  \n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = net_compress(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression' % (test_total, test_acc))\n","\n","  total_paramaeters_uncompressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * net_compress.features[i].weight.data.size()[j]\n","    total_paramaeters_uncompressed += weight_count\n","\n","  new_layer_node = []\n","  for layer in range(0,8,2):\n","    node_count = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    else:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    new_layer_node.append(node_count)\n","    \n","  new_compress_net = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  index = 0\n","  for i in range(0,10,2):\n","    if i == 0:\n","      new_compress_net.features[i] = nn.Conv1d(FEATURE_SIZE, new_layer_node[index], kernel_size=KERNAL_SIZE, bias=False)\n","    elif i == 8:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False)\n","    else:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], new_layer_node[index+1], kernel_size=KERNAL_SIZE, bias=False)\n","      index+=1  \n","  node_indexes = []\n","  for layer in range(0,10,2):\n","    index = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(FEATURE_SIZE):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][j]\n","          index+=1\n","          node_indexes.append(i)\n","    elif layer == 8:\n","      for i in range(NODE_SIZE):\n","        for j in range(len(node_indexes)):\n","          new_compress_net.features[layer].weight.data[i][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","    else:\n","      temp_indexes = []\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(len(node_indexes)):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","          index+=1\n","          temp_indexes.append(i)\n","      node_indexes = temp_indexes\n","\n","  # display the results after compressed model\n","  new_compress_net = new_compress_net.double()\n","  total_paramaeters_compressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * new_compress_net.features[i].weight.data.size()[j]\n","    total_paramaeters_compressed += weight_count\n","\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_paramaeters_compressed) / total_paramaeters_uncompressed))\n","\n","  # display node remainng\n","  node_count = 0\n","  for i in range(len(new_layer_node)): \n","    node_count += new_layer_node[i]\n","  print('Sparity for the node remaining: %.2f %%' % (100*float(node_count) / (4.0*NODE_SIZE)))\n","\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = new_compress_net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n","  torch.jit.script(new_compress_net)._save_for_lite_interpreter(PATH + '/model/android_model/android_final_model/' + name + \"_compressed.ptl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXJXoMQdBeur","executionInfo":{"status":"ok","timestamp":1640661480672,"user_tz":300,"elapsed":186001,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"outputId":"98ea592f-cd12-4225-b554-713528194a4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for l0_group_lasso:\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","Sparity for the compressed model: 3.72 %\n","Sparity for the node remaining: 10.74 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for l0_norm:\n","Accuracy of the network on the 6072 test data: 93.07 % after compression\n","Sparity for the compressed model: 100.00 %\n","Sparity for the node remaining: 100.00 %\n","Accuracy of the network on the 6072 test data: 93.07 % after compression\n","\n","Here are the results for l1_norm:\n","Accuracy of the network on the 6072 test data: 95.29 % after compression\n","Sparity for the compressed model: 7.28 %\n","Sparity for the node remaining: 19.73 %\n","Accuracy of the network on the 6072 test data: 95.29 % after compression\n","\n","Here are the results for l2_norm:\n","Accuracy of the network on the 6072 test data: 92.09 % after compression\n","Sparity for the compressed model: 94.93 %\n","Sparity for the node remaining: 97.27 %\n","Accuracy of the network on the 6072 test data: 92.09 % after compression\n","\n","Here are the results for group_lasso:\n","Accuracy of the network on the 6072 test data: 93.28 % after compression\n","Sparity for the compressed model: 100.00 %\n","Sparity for the node remaining: 100.00 %\n","Accuracy of the network on the 6072 test data: 93.28 % after compression\n","\n","Here are the results for l1_group_lasso:\n","Accuracy of the network on the 6072 test data: 97.20 % after compression\n","Sparity for the compressed model: 9.72 %\n","Sparity for the node remaining: 26.17 %\n","Accuracy of the network on the 6072 test data: 97.20 % after compression\n","\n"]}]},{"cell_type":"markdown","source":["# Other Results -- learning rate, cross validation and threshold"],"metadata":{"id":"_BjvUYgIpQr_"}},{"cell_type":"markdown","source":["k-fold cross validation results"],"metadata":{"id":"69gV4j2DDIA7"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = 0.01\n","\n","class FooBarPruningMethod(prune.BasePruningMethod):\n","    \"\"\"Prune every other entry in a tensor\n","    \"\"\"\n","    PRUNING_TYPE = 'unstructured'\n","\n","    def compute_mask(self, t, default_mask):\n","      return torch.abs(t) > PRUNE_THRESHOLD\n","\n","def foobar_unstructured(module, name):\n","    FooBarPruningMethod.apply(module, name)\n","    return module\n","\n","PATHS = {'0':   PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl',\n","      '1':   PATH + '/model/final/lr0.0001/l0_group_lasso1.ptl',\n","      '2':   PATH + '/model/final/lr0.0001/l0_group_lasso2.ptl',\n","      '3':   PATH + '/model/final/lr0.0001/l0_group_lasso3.ptl',\n","      '4':   PATH + '/model/final/lr0.0001/l0_group_lasso4.ptl',\n","    }\n","\n","for name in PATHS:\n","  # load the model\n","  net_uncompress = torch.load(PATHS[name])\n","  torch.save(net_uncompress, PATH + '/model/android_model/kfold/' + name + \"_uncompressed.ptl\")\n","\n","  foobar_unstructured(net_uncompress.features[0], \"weight\")\n","  foobar_unstructured(net_uncompress.features[2], \"weight\")\n","  foobar_unstructured(net_uncompress.features[4], \"weight\")\n","  foobar_unstructured(net_uncompress.features[6], \"weight\")\n","  foobar_unstructured(net_uncompress.features[8], \"weight\")\n","\n","  net_compress = torch.load(PATHS[name])\n","  net_compress.features[0].weight = torch.nn.Parameter(net_uncompress.features[0].weight)\n","  net_compress.features[2].weight = torch.nn.Parameter(net_uncompress.features[2].weight)\n","  net_compress.features[4].weight = torch.nn.Parameter(net_uncompress.features[4].weight)\n","  net_compress.features[6].weight = torch.nn.Parameter(net_uncompress.features[6].weight)\n","  net_compress.features[8].weight = torch.nn.Parameter(net_uncompress.features[8].weight)\n","\n","  torch.save(net_compress, PATH + '/model/android_model/kfold/' + name + \"_compressed.ptl\")\n"],"metadata":{"id":"L0Nb17pd-kAa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PATHS = {'0':  PATH + '/model/android_model/kfold/0',\n","      '1':  PATH + '/model/android_model/kfold/1',\n","      '2':  PATH + '/model/android_model/kfold/2',\n","      '3':  PATH + '/model/android_model/kfold/3',\n","      '4':  PATH + '/model/android_model/kfold/4',\n","    }\n","for name in PATHS:\n","  print('Here are the results for fold {}:'.format(name))\n","  # load the model\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  \n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = net_compress(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression' % (test_total, test_acc))\n","\n","  total_paramaeters_uncompressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * net_compress.features[i].weight.data.size()[j]\n","    total_paramaeters_uncompressed += weight_count\n","\n","  new_layer_node = []\n","  for layer in range(0,8,2):\n","    node_count = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    else:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    new_layer_node.append(node_count)\n","    \n","  new_compress_net = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  index = 0\n","  for i in range(0,10,2):\n","    if i == 0:\n","      new_compress_net.features[i] = nn.Conv1d(FEATURE_SIZE, new_layer_node[index], kernel_size=KERNAL_SIZE, bias=False)\n","    elif i == 8:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False)\n","    else:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], new_layer_node[index+1], kernel_size=KERNAL_SIZE, bias=False)\n","      index+=1  \n","  node_indexes = []\n","  for layer in range(0,10,2):\n","    index = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(FEATURE_SIZE):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][j]\n","          index+=1\n","          node_indexes.append(i)\n","    elif layer == 8:\n","      for i in range(NODE_SIZE):\n","        for j in range(len(node_indexes)):\n","          new_compress_net.features[layer].weight.data[i][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","    else:\n","      temp_indexes = []\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(len(node_indexes)):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","          index+=1\n","          temp_indexes.append(i)\n","      node_indexes = temp_indexes\n","\n","  # display the results after compressed model\n","  new_compress_net = new_compress_net.double()\n","  total_paramaeters_compressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * new_compress_net.features[i].weight.data.size()[j]\n","    total_paramaeters_compressed += weight_count\n","\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_paramaeters_compressed) / total_paramaeters_uncompressed))\n","\n","  # display node remainng\n","  node_count = 0\n","  for i in range(len(new_layer_node)): \n","    node_count += new_layer_node[i]\n","  print('Sparity for the node remaining: %.2f %%' % (100*float(node_count) / (4.0*NODE_SIZE)))\n","\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = new_compress_net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n","  torch.jit.script(new_compress_net)._save_for_lite_interpreter(PATH + '/model/android_model/android_final_model/' + name + \"_compressed.ptl\")\n"],"metadata":{"id":"RnT_OB27_Krr","executionInfo":{"status":"ok","timestamp":1640662311325,"user_tz":300,"elapsed":118153,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7be47de6-558e-48c2-eebd-33b410fff832"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for fold 0:\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","Sparity for the compressed model: 3.72 %\n","Sparity for the node remaining: 10.74 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for fold 1:\n","Accuracy of the network on the 6072 test data: 92.28 % after compression\n","Sparity for the compressed model: 3.15 %\n","Sparity for the node remaining: 10.35 %\n","Accuracy of the network on the 6072 test data: 92.28 % after compression\n","\n","Here are the results for fold 2:\n","Accuracy of the network on the 6072 test data: 96.28 % after compression\n","Sparity for the compressed model: 4.74 %\n","Sparity for the node remaining: 14.84 %\n","Accuracy of the network on the 6072 test data: 96.28 % after compression\n","\n","Here are the results for fold 3:\n","Accuracy of the network on the 6072 test data: 94.99 % after compression\n","Sparity for the compressed model: 3.42 %\n","Sparity for the node remaining: 10.55 %\n","Accuracy of the network on the 6072 test data: 94.99 % after compression\n","\n","Here are the results for fold 4:\n","Accuracy of the network on the 6072 test data: 94.81 % after compression\n","Sparity for the compressed model: 3.71 %\n","Sparity for the node remaining: 11.52 %\n","Accuracy of the network on the 6072 test data: 94.81 % after compression\n","\n"]}]},{"cell_type":"markdown","source":["Results for different learning rate."],"metadata":{"id":"R4_igxccDrxY"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = 0.01\n","\n","class FooBarPruningMethod(prune.BasePruningMethod):\n","    \"\"\"Prune every other entry in a tensor\n","    \"\"\"\n","    PRUNING_TYPE = 'unstructured'\n","\n","    def compute_mask(self, t, default_mask):\n","      return torch.abs(t) > PRUNE_THRESHOLD\n","\n","def foobar_unstructured(module, name):\n","    FooBarPruningMethod.apply(module, name)\n","    return module\n","\n","PATHS = {'0_0001':   PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl',\n","      '0_00015':  PATH + '/model/final/lr0.00015/l0_group_lasso0.ptl',\n","      '0_00001':  PATH + '/model/final/lr0.00001/l0_group_lasso0.ptl',\n","      '0_00005':  PATH + '/model/final/lr0.00005/l0_group_lasso0.ptl',\n","      '0_0002':  PATH + '/model/final/lr0.0002/l0_group_lasso0.ptl',\n","    }\n","\n","for name in PATHS:\n","  # load the model\n","  net_uncompress = torch.load(PATHS[name])\n","  torch.save(net_uncompress, PATH + '/model/android_model/learning_rate/' + name + \"_uncompressed.ptl\")\n","\n","  foobar_unstructured(net_uncompress.features[0], \"weight\")\n","  foobar_unstructured(net_uncompress.features[2], \"weight\")\n","  foobar_unstructured(net_uncompress.features[4], \"weight\")\n","  foobar_unstructured(net_uncompress.features[6], \"weight\")\n","  foobar_unstructured(net_uncompress.features[8], \"weight\")\n","\n","  net_compress = torch.load(PATHS[name])\n","  net_compress.features[0].weight = torch.nn.Parameter(net_uncompress.features[0].weight)\n","  net_compress.features[2].weight = torch.nn.Parameter(net_uncompress.features[2].weight)\n","  net_compress.features[4].weight = torch.nn.Parameter(net_uncompress.features[4].weight)\n","  net_compress.features[6].weight = torch.nn.Parameter(net_uncompress.features[6].weight)\n","  net_compress.features[8].weight = torch.nn.Parameter(net_uncompress.features[8].weight)\n","\n","  torch.save(net_compress, PATH + '/model/android_model/learning_rate/' + name + \"_compressed.ptl\")\n"],"metadata":{"id":"SLMcRQfy_2fR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PATHS = {'0_0001':       PATH + '/model/android_model/learning_rate/0_0001',\n","      '0_00015':       PATH + '/model/android_model/learning_rate/0_00015',\n","      '0_00001':        PATH + '/model/android_model/learning_rate/0_00001',\n","      '0_00005':     PATH + '/model/android_model/learning_rate/0_00005',\n","      '0_0002':    PATH + '/model/android_model/learning_rate/0_0002',\n","    }\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  \n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = net_compress(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression' % (test_total, test_acc))\n","\n","  total_paramaeters_uncompressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * net_compress.features[i].weight.data.size()[j]\n","    total_paramaeters_uncompressed += weight_count\n","\n","  new_layer_node = []\n","  for layer in range(0,8,2):\n","    node_count = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    else:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    new_layer_node.append(node_count)\n","    \n","  new_compress_net = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  index = 0\n","  for i in range(0,10,2):\n","    if i == 0:\n","      new_compress_net.features[i] = nn.Conv1d(FEATURE_SIZE, new_layer_node[index], kernel_size=KERNAL_SIZE, bias=False)\n","    elif i == 8:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False)\n","    else:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], new_layer_node[index+1], kernel_size=KERNAL_SIZE, bias=False)\n","      index+=1  \n","  node_indexes = []\n","  for layer in range(0,10,2):\n","    index = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(FEATURE_SIZE):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][j]\n","          index+=1\n","          node_indexes.append(i)\n","    elif layer == 8:\n","      for i in range(NODE_SIZE):\n","        for j in range(len(node_indexes)):\n","          new_compress_net.features[layer].weight.data[i][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","    else:\n","      temp_indexes = []\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(len(node_indexes)):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","          index+=1\n","          temp_indexes.append(i)\n","      node_indexes = temp_indexes\n","\n","  # display the results after compressed model\n","  new_compress_net = new_compress_net.double()\n","  total_paramaeters_compressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * new_compress_net.features[i].weight.data.size()[j]\n","    total_paramaeters_compressed += weight_count\n","\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_paramaeters_compressed) / total_paramaeters_uncompressed))\n","\n","  # display node remainng\n","  node_count = 0\n","  for i in range(len(new_layer_node)): \n","    node_count += new_layer_node[i]\n","  print('Sparity for the node remaining: %.2f %%' % (100*float(node_count) / (4.0*NODE_SIZE)))\n","\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = new_compress_net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n","  torch.jit.script(new_compress_net)._save_for_lite_interpreter(PATH + '/model/android_model/android_final_model/' + name + \"_compressed.ptl\")\n"],"metadata":{"id":"p1umYPciAKRy","executionInfo":{"status":"ok","timestamp":1640662182805,"user_tz":300,"elapsed":117505,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"24d5fc3c-c4d4-4b47-e71d-881fb1e14a2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for 0_0001:\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","Sparity for the compressed model: 3.72 %\n","Sparity for the node remaining: 10.74 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for 0_00015:\n","Accuracy of the network on the 6072 test data: 96.62 % after compression\n","Sparity for the compressed model: 2.36 %\n","Sparity for the node remaining: 7.62 %\n","Accuracy of the network on the 6072 test data: 96.62 % after compression\n","\n","Here are the results for 0_00001:\n","Accuracy of the network on the 6072 test data: 85.80 % after compression\n","Sparity for the compressed model: 28.61 %\n","Sparity for the node remaining: 49.22 %\n","Accuracy of the network on the 6072 test data: 85.80 % after compression\n","\n","Here are the results for 0_00005:\n","Accuracy of the network on the 6072 test data: 93.89 % after compression\n","Sparity for the compressed model: 11.81 %\n","Sparity for the node remaining: 28.32 %\n","Accuracy of the network on the 6072 test data: 93.89 % after compression\n","\n","Here are the results for 0_0002:\n","Accuracy of the network on the 6072 test data: 94.99 % after compression\n","Sparity for the compressed model: 2.02 %\n","Sparity for the node remaining: 7.62 %\n","Accuracy of the network on the 6072 test data: 94.99 % after compression\n","\n"]}]},{"cell_type":"markdown","source":["Results for different threshold"],"metadata":{"id":"KRsRA_b-Fmss"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = np.arange(0.005, 0.03, 0.005)\n","\n","def foobar_unstructured(module, name):\n","    FooBarPruningMethod.apply(module, name)\n","    return module\n","\n","PATHS = {'l0_group_lasso':   PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl',}\n","\n","for threshold in PRUNE_THRESHOLD:\n","  class FooBarPruningMethod(prune.BasePruningMethod):\n","    \"\"\"Prune every other entry in a tensor\n","    \"\"\"\n","    PRUNING_TYPE = 'unstructured'\n","\n","    def compute_mask(self, t, default_mask):\n","      return torch.abs(t) > threshold\n","\n","  for name in PATHS:\n","    # load the model\n","    net_uncompress = torch.load(PATHS[name])\n","    torch.save(net_uncompress, PATH + '/model/android_model/threshold/' + str(threshold) + \"_uncompressed.ptl\")\n","\n","    foobar_unstructured(net_uncompress.features[0], \"weight\")\n","    foobar_unstructured(net_uncompress.features[2], \"weight\")\n","    foobar_unstructured(net_uncompress.features[4], \"weight\")\n","    foobar_unstructured(net_uncompress.features[6], \"weight\")\n","    foobar_unstructured(net_uncompress.features[8], \"weight\")\n","\n","    net_compress = torch.load(PATHS[name])\n","    net_compress.features[0].weight = torch.nn.Parameter(net_uncompress.features[0].weight)\n","    net_compress.features[2].weight = torch.nn.Parameter(net_uncompress.features[2].weight)\n","    net_compress.features[4].weight = torch.nn.Parameter(net_uncompress.features[4].weight)\n","    net_compress.features[6].weight = torch.nn.Parameter(net_uncompress.features[6].weight)\n","    net_compress.features[8].weight = torch.nn.Parameter(net_uncompress.features[8].weight)\n","\n","    torch.save(net_compress, PATH + '/model/android_model/threshold/' + str(threshold) + \"_compressed.ptl\")"],"metadata":{"id":"TU9uzXo_FvoM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PATHS = {'0.005':       PATH + '/model/android_model/threshold/0.005',\n","      '0.01':       PATH + '/model/android_model/threshold/0.01',\n","      '0.015':        PATH + '/model/android_model/threshold/0.015',\n","      '0.02':     PATH + '/model/android_model/threshold/0.02',\n","      '0.025':    PATH + '/model/android_model/threshold/0.025',\n","    }\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  \n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = net_compress(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression' % (test_total, test_acc))\n","\n","  total_paramaeters_uncompressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * net_compress.features[i].weight.data.size()[j]\n","    total_paramaeters_uncompressed += weight_count\n","\n","  new_layer_node = []\n","  for layer in range(0,8,2):\n","    node_count = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    else:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    new_layer_node.append(node_count)\n","    \n","  new_compress_net = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  index = 0\n","  for i in range(0,10,2):\n","    if i == 0:\n","      new_compress_net.features[i] = nn.Conv1d(FEATURE_SIZE, new_layer_node[index], kernel_size=KERNAL_SIZE, bias=False)\n","    elif i == 8:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False)\n","    else:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], new_layer_node[index+1], kernel_size=KERNAL_SIZE, bias=False)\n","      index+=1  \n","  node_indexes = []\n","  for layer in range(0,10,2):\n","    index = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(FEATURE_SIZE):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][j]\n","          index+=1\n","          node_indexes.append(i)\n","    elif layer == 8:\n","      for i in range(NODE_SIZE):\n","        for j in range(len(node_indexes)):\n","          new_compress_net.features[layer].weight.data[i][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","    else:\n","      temp_indexes = []\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(len(node_indexes)):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","          index+=1\n","          temp_indexes.append(i)\n","      node_indexes = temp_indexes\n","\n","  # display the results after compressed model\n","  new_compress_net = new_compress_net.double()\n","  total_paramaeters_compressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * new_compress_net.features[i].weight.data.size()[j]\n","    total_paramaeters_compressed += weight_count\n","\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_paramaeters_compressed) / total_paramaeters_uncompressed))\n","\n","  # display node remainng\n","  node_count = 0\n","  for i in range(len(new_layer_node)): \n","    node_count += new_layer_node[i]\n","  print('Sparity for the node remaining: %.2f %%' % (100*float(node_count) / (4.0*NODE_SIZE)))\n","\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = new_compress_net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n","  torch.jit.script(new_compress_net)._save_for_lite_interpreter(PATH + '/model/android_model/android_final_model/' + name + \"_compressed.ptl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npCYI-6IGrRQ","executionInfo":{"status":"ok","timestamp":1640662819758,"user_tz":300,"elapsed":114875,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"outputId":"4853f08f-90d8-4ca3-aaec-54f195b5a3d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for 0.005:\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","Sparity for the compressed model: 5.49 %\n","Sparity for the node remaining: 15.04 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for 0.01:\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","Sparity for the compressed model: 3.72 %\n","Sparity for the node remaining: 10.74 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for 0.015:\n","Accuracy of the network on the 6072 test data: 96.72 % after compression\n","Sparity for the compressed model: 3.58 %\n","Sparity for the node remaining: 10.35 %\n","Accuracy of the network on the 6072 test data: 96.72 % after compression\n","\n","Here are the results for 0.02:\n","Accuracy of the network on the 6072 test data: 96.39 % after compression\n","Sparity for the compressed model: 3.56 %\n","Sparity for the node remaining: 10.16 %\n","Accuracy of the network on the 6072 test data: 96.39 % after compression\n","\n","Here are the results for 0.025:\n","Accuracy of the network on the 6072 test data: 94.58 % after compression\n","Sparity for the compressed model: 3.56 %\n","Sparity for the node remaining: 10.16 %\n","Accuracy of the network on the 6072 test data: 94.58 % after compression\n","\n"]}]}]}