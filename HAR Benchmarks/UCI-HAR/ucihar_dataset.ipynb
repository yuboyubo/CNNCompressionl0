{"cells":[{"cell_type":"markdown","metadata":{"id":"JD_16kJ_BWqg"},"source":["## Connect Google Drive and GPU\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neFdxlyCA4Wt"},"outputs":[],"source":["%reset\n","\n","# connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# connect colab gpu\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","metadata":{"id":"RE7AbBJmBjJo"},"source":["## Import Needed Libraries, Paramaters and Functions"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5694,"status":"ok","timestamp":1640626811738,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"},"user_tz":300},"id":"0HZHzTwBs4Nl"},"outputs":[],"source":["import sys\n","import time\n","import os.path\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.utils import prune\n","import torchvision\n","import matplotlib.pyplot as plt\n","from torch.utils.mobile_optimizer import optimize_for_mobile\n","from scipy import stats\n","from sklearn.utils import shuffle\n","\n","SEED = 10\n","WINDOW_SIZE = 128\n","FEATURE_SIZE = 9\n","LABEL_SIZE = 6\n","BATCH_SIZE = 32\n","PATH = '/content/drive/MyDrive/CNNPaper'\n","TRAIN_LOADER_PATH = PATH + '/model/final/loader/train_loader'\n","VALID_LOAER_PATH = PATH + '/model/final/loader/valid_loader'\n","TEST_LOADER_PATH = PATH + '/model/final/loader/test_loader'\n","TRAIN_DATA_DIR_PATH = PATH + '/data/UCIHAR/train/'\n","TEST_DATA_DIR_PATH = PATH + '/data/UCIHAR/test/'\n","TRAIN_DATA_PATH = PATH + '/data/UCIHAR/train_data.cvs'\n","TEST_DATA_PATH = PATH + '/data/UCIHAR/test_data.cvs'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":161,"status":"ok","timestamp":1640626811896,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"},"user_tz":300},"id":"RtMUnj56BjlX"},"outputs":[],"source":["def read_data(file_path, TYPE):\n","  \"\"\"\n","    Read data from file_path\n","    Paramater:\n","      file_path: str\n","    Return:\n","      a DataFrame with the data and labels \n","  \"\"\"\n","  if os.path.isfile(TRAIN_DATA_PATH) and TYPE == 'train':\n","    print(\"Start reading data ...\")\n","    df = pd.read_csv(TRAIN_DATA_PATH)\n","    print(\"Finish reading data ...\")\n","  elif os.path.isfile(TEST_DATA_PATH) and TYPE == 'test':\n","    print(\"Start reading data ...\")\n","    df = pd.read_csv(TEST_DATA_PATH)\n","    print(\"Finish reading data ...\")\n","  else:\n","    print(\"Start reading data ...\")\n","    x_accel = pd.read_csv(file_path + 'Inertial Signals/body_acc_x_'+TYPE+'.txt', header=None, names=['x-accel'])\n","    y_accel = pd.read_csv(file_path + 'Inertial Signals/body_acc_y_'+TYPE+'.txt', header=None, names=['y-accel'])\n","    z_accel = pd.read_csv(file_path + 'Inertial Signals/body_acc_z_'+TYPE+'.txt', header=None, names=['z-accel'])\n","    x_gyro = pd.read_csv(file_path + 'Inertial Signals/body_gyro_x_'+TYPE+'.txt', header=None, names=['x-gyro'])\n","    y_gyro = pd.read_csv(file_path + 'Inertial Signals/body_gyro_y_'+TYPE+'.txt', header=None, names=['y-gyro'])\n","    z_gyro = pd.read_csv(file_path + 'Inertial Signals/body_gyro_z_'+TYPE+'.txt', header=None, names=['z-gyro'])\n","    x_gyro_total = pd.read_csv(file_path + 'Inertial Signals/total_acc_x_'+TYPE+'.txt', header=None, names=['total-x-gyro'])\n","    y_gyro_total = pd.read_csv(file_path + 'Inertial Signals/total_acc_y_'+TYPE+'.txt', header=None, names=['total-y-gyro'])\n","    z_gyro_total = pd.read_csv(file_path + 'Inertial Signals/total_acc_z_'+TYPE+'.txt', header=None, names=['total-z-gyro'])\n","    activity = pd.read_csv(file_path + 'y_'+TYPE+'.txt', header=None, names=['activity'])\n","    length = len(x_accel)\n","    df = pd.DataFrame(columns=['user', 'activity', 'timestamp', 'x-accel', 'y-accel', 'z-accel', 'total-x-gyro', 'total-y-gyro', 'total-z-gyro'])\n","    \n","    for i in range(length):\n","      x_acc_data = x_accel['x-accel'][i].split()\n","      y_acc_data = y_accel['y-accel'][i].split()\n","      z_acc_data = z_accel['z-accel'][i].split()\n","      x_gyro_data = x_gyro['x-gyro'][i].split()\n","      y_gyro_data = y_gyro['y-gyro'][i].split()\n","      z_gyro_data = z_gyro['z-gyro'][i].split()\n","      total_gyro_x_data = x_gyro_total['total-x-gyro'][i].split()\n","      total_gyro_y_data = y_gyro_total['total-y-gyro'][i].split()\n","      total_gyro_z_data = z_gyro_total['total-z-gyro'][i].split()\n","      activity_data = activity['activity'][i]\n","      print(\"index \" + str(i))\n","      size = len(x_acc_data)\n","      for j in range(size):\n","        df = df.append({'user': i, 'activity': activity_data, 'timestamp': j, \n","                                'x-accel': float(x_acc_data[j]),\n","                                'y-accel': float(y_acc_data[j]),\n","                                'z-accel': float(z_acc_data[j]),\n","                                'x-gyro': float(x_gyro_data[j]),\n","                                'y-gyro': float(y_gyro_data[j]),\n","                                'z-gyro': float(z_gyro_data[j]),\n","                                'total-x-gyro': float(total_gyro_x_data[j]),\n","                                'total-y-gyro': float(total_gyro_y_data[j]),\n","                                'total-z-gyro': float(total_gyro_z_data[j])}, ignore_index=True)\n","    print(\"Finish reading data ...\")\n","    if TYPE == 'train':\n","      df.to_csv('train_data.cvs', index=False)\n","    else:\n","      df.to_csv('test_data.cvs', index=False)\n","  return df\n","\n","def feature_normalize(data):\n","  \"\"\"\n","    Normalize the feature data\n","    Paramater:\n","      data: a list of floats\n","    Return:\n","      a list of floats with normalized data\n","  \"\"\"\n","  mu = np.mean(data, axis=0)\n","  sigma = np.std(data, axis=0)\n","  return (data - mu) / sigma\n","\n","def dataset_normalize(dataset):\n","  \"\"\"\n","    Normalize the whole dataset\n","    Paramater:\n","      dataset: a DataFrame with the data and labels \n","    Return:\n","      a DataFrame with the normalized data and labels \n","  \"\"\"\n","  dataset.dropna(axis=0, how='any', inplace=True)\n","  print(\"Normalizing x-accel ...\")\n","  dataset['x-accel'] = feature_normalize(dataset['x-accel'])\n","  print(\"Normalizing y-accel ...\")\n","  dataset['y-accel'] = feature_normalize(dataset['y-accel'])\n","  print(\"Normalizing z-accel ...\")\n","  dataset['z-accel'] = feature_normalize(dataset['z-accel'])\n","  print(\"Normalizing x-gyro ...\")\n","  dataset['x-gyro'] = feature_normalize(dataset['x-gyro'])\n","  print(\"Normalizing y-gyro ...\")\n","  dataset['y-gyro'] = feature_normalize(dataset['y-gyro'])\n","  print(\"Normalizing z-gyro ...\")\n","  dataset['z-gyro'] = feature_normalize(dataset['z-gyro'])\n","  print(\"Normalizing total-x-gyro ...\")\n","  dataset['total-x-gyro'] = feature_normalize(dataset['total-x-gyro'])\n","  print(\"Normalizing total-y-gyro ...\")\n","  dataset['total-x-gyro'] = feature_normalize(dataset['total-x-gyro'])\n","  print(\"Normalizing total-z-gyro ...\")\n","  dataset['total-x-gyro'] = feature_normalize(dataset['total-x-gyro'])\n","  return dataset\n","\n","def dataset_segmentation(data):\n","  \"\"\"\n","    Dataset segmentation according the window size\n","    Paramater:\n","      data: a list of floats\n","    Return:\n","      segments and labels \n","  \"\"\"\n","  print(\"Start segmentation with window size: \", WINDOW_SIZE)\n","  segments = np.empty((0, WINDOW_SIZE, FEATURE_SIZE))\n","  labels = np.empty((0))\n","  size = data['timestamp'].count()\n","  for start in range(0, size, WINDOW_SIZE):\n","      x1 = data[\"x-accel\"][start:start+WINDOW_SIZE]\n","      y1 = data[\"y-accel\"][start:start+WINDOW_SIZE]\n","      z1 = data[\"z-accel\"][start:start+WINDOW_SIZE]\n","      x2 = data[\"x-gyro\"][start:start+WINDOW_SIZE]\n","      y2 = data[\"y-gyro\"][start:start+WINDOW_SIZE]\n","      z2 = data[\"z-gyro\"][start:start+WINDOW_SIZE]\n","      x3 = data[\"total-x-gyro\"][start:start+WINDOW_SIZE]\n","      y3 = data[\"total-y-gyro\"][start:start+WINDOW_SIZE]\n","      z3 = data[\"total-z-gyro\"][start:start+WINDOW_SIZE]\n","      if len(data[\"timestamp\"][start:start+WINDOW_SIZE]) == WINDOW_SIZE:\n","        segments = np.vstack([segments, np.dstack([x1,y1,z1,x2,y2,z2,x3,y3,z3])])\n","        labels = np.append(labels, stats.mode(data[\"activity\"][start:start+WINDOW_SIZE])[0][0])\n","  labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n","  segments = segments.reshape(len(segments), FEATURE_SIZE, WINDOW_SIZE)\n","  print(\"Finish segmentation ...\")\n","  return segments, labels\n","\n","def train_valid_test_split(segments, classes, test_x, test_y, k_fold):\n","  \"\"\"\n","    Split train, valid and test datase\n","    Paramater:\n","      segments: a list of input data\n","      classes: a list of classes data\n","      k: k fold cross validation\n","    Return:\n","      segments and labels \n","  \"\"\"\n","  print(\"Start dataset split... \")\n","  seg_len = len(segments)\n","  idx_val = [0, int(seg_len/5*1), int(seg_len/5*2), int(seg_len/5*3), int(seg_len/5*4), seg_len]\n","  train_range1 = range(0, idx_val[k_fold])\n","  valid_range = range(idx_val[k_fold], idx_val[k_fold+1])\n","  train_range2 = range(idx_val[k_fold+1], seg_len)\n","\n","  train_x = np.concatenate((segments[train_range1], segments[train_range2]), axis=0)\n","  train_y = np.concatenate((classes[train_range1], classes[train_range2]), axis=0)\n","  valid_x = segments[valid_range]\n","  valid_y = classes[valid_range]\n","\n","  # get train data\n","  train_data = []\n","  for i in range(len(train_x)):\n","    train_data.append([train_x[i], train_y[i]])\n","  \n","  # get valid data\n","  valid_data = []\n","  for i in range(len(valid_x)):\n","    valid_data.append([valid_x[i], valid_y[i]])\n","  \n","  # get test data\n","  test_data = []\n","  for i in range(len(test_x)):\n","    test_data.append([test_x[i], test_y[i]])\n","  print(len(train_data))\n","  print(len(valid_data))\n","  print(len(test_data))\n","\n","  # generate DataLoader for each dataset\n","  trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n","  validloader = torch.utils.data.DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n","  testloader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)\n","  \n","  print(\"Finish dataset split... \")\n","  return trainloader, validloader, testloader\n"]},{"cell_type":"markdown","metadata":{"id":"ZBGhSZcP1DwW"},"source":["## Load and Save Train, Test, Valid Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEpJxsFGtlxM"},"outputs":[],"source":["train_dataset = dataset_normalize(read_data(TRAIN_DATA_DIR_PATH, 'train'))\n","test_dataset = dataset_normalize(read_data(TEST_DATA_DIR_PATH, 'test'))\n","segments, classes = dataset_segmentation(train_dataset)\n","test_x, test_y = dataset_segmentation(test_dataset)\n","np.random.seed(SEED)\n","total_x, total_y = shuffle(segments, classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfzoKAPptmBU"},"outputs":[],"source":["cross_valid_range = 5\n","\n","for k in range(cross_valid_range):\n","  print(\"Start spliting for k = \" + str(k))\n","  trainloader, validloader, testloader = train_valid_test_split(total_x, total_y, test_x, test_y, k)\n","  CROSS_TRAIN_LOADER_PATH = TRAIN_LOADER_PATH + str(k) + '.pkl'\n","  CROSS_VALID_LOADER_PATH = VALID_LOAER_PATH + str(k) + '.pkl'\n","  CROSS_TEST_LOADER_PATH = TEST_LOADER_PATH + str(k) + '.pkl'\n","  torch.save(trainloader, CROSS_TRAIN_LOADER_PATH)\n","  torch.save(validloader, CROSS_VALID_LOADER_PATH)\n","  torch.save(testloader, CROSS_TEST_LOADER_PATH)\n","  print(\"Finish data loading...\")"]},{"cell_type":"markdown","metadata":{"id":"ggHEAvHi1Hib"},"source":["## Load CNN Model and Other Helper Functions\n"]},{"cell_type":"code","source":["# Here are the variables that you can modify\n","NODE_SIZE = 128\n","KERNAL_SIZE = 5\n","LEARNING_RATE = 0.0001\n","k = 4 # cross validation fold"],"metadata":{"id":"Wt-JjtFo-rXN","executionInfo":{"status":"ok","timestamp":1640626850229,"user_tz":300,"elapsed":169,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4785,"status":"ok","timestamp":1640626863453,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"},"user_tz":300},"id":"rd1t0tSTvTuA"},"outputs":[],"source":["CROSS_TRAIN_LOADER_PATH = TRAIN_LOADER_PATH + str(k) + '.pkl'\n","CROSS_VALID_LOADER_PATH = VALID_LOAER_PATH + str(k) + '.pkl'\n","CROSS_TEST_LOADER_PATH = TEST_LOADER_PATH + str(k) + '.pkl'\n","trainloader = torch.load(CROSS_TRAIN_LOADER_PATH)\n","validloader = torch.load(CROSS_VALID_LOADER_PATH)\n","testloader = torch.load(CROSS_TEST_LOADER_PATH)\n","\n","class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","\n","    # Convolutional Layers\n","    self.features = nn.Sequential(\n","      nn.Conv1d(FEATURE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","    )\n","  \n","    self.fc1 = nn.Linear(NODE_SIZE*(WINDOW_SIZE-5*(KERNAL_SIZE-1)), 100)\n","    self.fc2 = nn.Linear(100, LABEL_SIZE)\n","    self.max = nn.Softmax(dim=1)\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = x.view(x.shape[0], -1)\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)\n","    x = self.max(x)\n","    return x\n","\n","def train_save_CNN_model(TYPE, EPOCH_SIZE):\n","  # manually set random seed\n","  torch.backends.cudnn.deterministic = True\n","  torch.manual_seed(SEED)\n","\n","  # set gpu device\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  net = CNN().double().to(device)\n","\n","  # pick the criterion and optimizer\n","  criterion = nn.MultiLabelSoftMarginLoss()\n","  optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","\n","  print(\"Learning rate %.5f, batch size %d, node size %d, kernal size %d\" % (LEARNING_RATE, BATCH_SIZE, NODE_SIZE, KERNAL_SIZE))\n","\n","  # initialization\n","  train_acc_list = []\n","  val_acc_list = []\n","  test_acc_list = []\n","  accuray = 0\n","\n","  # start to train with epoches\n","  for epoch in range(EPOCH_SIZE):\n","    running_loss = 0.0\n","    train_total = 0\n","    train_correct = 0\n","    valid_total = 0\n","    valid_correct = 0\n","    test_total = 0\n","    test_correct = 0\n","\n","    # for the training dataset\n","    for i, data in enumerate(trainloader, 0):\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      optimizer.zero_grad()\n","      outputs = net(inputs)\n","      train_total += labels.size(0)\n","      train_correct += (torch.max(outputs, 1)[1] == torch.max(labels, 1)[1]).sum().item()\n","      loss = criterion(outputs, labels)\n","      if TYPE == 'l0_norm':\n","        # add group lasso regularization\n","        lgl = 1e-8\n","        cnt = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            cnt = cnt + param.detach().nonzero().size(0)\n","            #cnt = cnt + len(param.detach()[param.detach() > 1e-2]) + len(param.detach()[param.detach() < -1e-2])\n","        loss = loss + lgl * cnt\n","      elif TYPE == 'l1_norm':\n","        # add group lasso regularization\n","        lgl = 1e-8\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            regularization = regularization + torch.norm(param, 1)\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'l2_norm':\n","        lgl = 1e-8\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            regularization = regularization + torch.norm(param)\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'group_lasso':\n","        # add group lasso regularization\n","        lgl = 1e-8\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              regularization = regularization + torch.norm(param[i,:,:])\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'l1_group_lasso':\n","        lgl = 1e-8\n","        alpha = 0.5\n","        group_lasso_regularization = torch.tensor([0]).cuda(0)\n","        lasso_regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              group_lasso_regularization = group_lasso_regularization + torch.norm(param[i,:,:])\n","            lasso_regularization = lasso_regularization + torch.norm(param, 1)\n","        loss = loss + (1-alpha) * lgl * group_lasso_regularization + alpha * lgl * lasso_regularization\n","      elif TYPE == 'l0_group_lasso':\n","        #lgl = 0.000001\n","        #alpha = 0.90\n","        l0 = 1e-8\n","        lg = 0.4*1e-4\n","        cnt = torch.tensor([0]).cuda(0)\n","        group_lasso_regularization = torch.tensor([0]).cuda(0)\n","        lasso_regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              group_lasso_regularization = group_lasso_regularization + torch.norm(param[i,:,:])\n","            cnt += param.detach().nonzero().size(0)\n","            #cnt = cnt + len(param.detach()[param.detach() > 2*1e-2]) + len(param.detach()[param.detach() < 2*-1e-2])\n","            #lasso_regularization = lasso_regularization + torch.norm(param, 0)\n","        loss = loss + lg * group_lasso_regularization + l0 * cnt\n","        #loss = loss + (1-alpha) * lgl * group_lasso_regularization + alpha * lgl * lasso_regularization\n","      loss.backward()\n","      optimizer.step()\n","      running_loss += loss.item()\n","\n","    # for the validation dataset\n","    for data in validloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      valid_total += labels.size(0)\n","      valid_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    \n","    # for the test dataset\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    \n","    # obtain the results for training, validation, test dataset\n","    train_acc = 100 * train_correct / train_total\n","    valid_acc = 100 * valid_correct / valid_total\n","    test_acc = 100 * test_correct / test_total\n","    train_acc_list.append(train_acc)\n","    val_acc_list.append(valid_acc)\n","    test_acc_list.append(test_acc)\n","    print(\"epoch %d, loss %.3f, train acc %.2f%%, valid acc %.2f%%, test acc %.2f%%\" % (epoch+1, running_loss, train_acc, valid_acc, test_acc))\n","    \n","    # save the best model\n","    if valid_acc >= accuray:\n","      accuray = valid_acc\n","      torch.save(net, PATH + '/model/' + TYPE + str(k) + \".ptl\")\n","    \n","  return train_acc_list, val_acc_list, test_acc_list"]},{"cell_type":"markdown","metadata":{"id":"QvBRzZPo1Qqb"},"source":["## Results for CNN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lmjtMiIvcJ3"},"outputs":[],"source":["TYPE = 'no_penalty'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YmnxB-RLWPZ"},"outputs":[],"source":["TYPE = 'l0_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyuH3sWELbS6"},"outputs":[],"source":["TYPE = 'l1_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nXMs5bpPLcpe"},"outputs":[],"source":["TYPE = 'l2_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAMlleZxLdCx"},"outputs":[],"source":["TYPE = 'group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tG4kwbQFLdag"},"outputs":[],"source":["TYPE = 'l1_group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OunIW7B2LjvZ"},"outputs":[],"source":["TYPE = 'l0_group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)\n","print(train_acc)\n","print(valid_acc)\n","print(test_acc)"]},{"cell_type":"markdown","metadata":{"id":"iJdjJI71Lwax"},"source":["## Results after Pruning the above Models"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28522,"status":"ok","timestamp":1640626976596,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"},"user_tz":300},"id":"Pql44kXZLw8B","outputId":"7d3f5388-9fea-40fb-ecb8-60b228ea9609"},"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for No penalty - 4 128 0.0001:\n","Accuracy of the network on the 2947 test data: 91.52 % before compression\n","Nonzero Parameter for the compressed model: 88.49 %\n","Accuracy of the network on the 2947 test data: 91.48 % after compression\n","\n","Here are the results for l0 norm - 4 128 0.0001:\n","Accuracy of the network on the 2947 test data: 91.52 % before compression\n","Nonzero Parameter for the compressed model: 88.49 %\n","Accuracy of the network on the 2947 test data: 91.48 % after compression\n","\n","Here are the results for l1 norm - 4 128 0.0001:\n","Accuracy of the network on the 2947 test data: 90.46 % before compression\n","Nonzero Parameter for the compressed model: 81.58 %\n","Accuracy of the network on the 2947 test data: 90.33 % after compression\n","\n","Here are the results for l2 norm - 4 128 0.0001:\n","Accuracy of the network on the 2947 test data: 91.01 % before compression\n","Nonzero Parameter for the compressed model: 88.35 %\n","Accuracy of the network on the 2947 test data: 90.94 % after compression\n","\n","Here are the results for group lasso - 4 128 0.0001:\n","Accuracy of the network on the 2947 test data: 90.80 % before compression\n","Nonzero Parameter for the compressed model: 82.91 %\n","Accuracy of the network on the 2947 test data: 90.84 % after compression\n","\n","Here are the results for l1 group lasso - 4 128 0.0001:\n","Accuracy of the network on the 2947 test data: 91.11 % before compression\n","Nonzero Parameter for the compressed model: 81.21 %\n","Accuracy of the network on the 2947 test data: 91.04 % after compression\n","\n","Here are the results for l0 group lasso - 4 128 0.0001 (BEST):\n","Accuracy of the network on the 2947 test data: 90.06 % before compression\n","Nonzero Parameter for the compressed model: 23.00 %\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","\n","Here are the results for l0 group lasso - 4 128 0.00005:\n","Accuracy of the network on the 2947 test data: 89.38 % before compression\n","Nonzero Parameter for the compressed model: 51.65 %\n","Accuracy of the network on the 2947 test data: 89.24 % after compression\n","\n","Here are the results for l0 group lasso - 4 128 0.00015:\n","Accuracy of the network on the 2947 test data: 90.94 % before compression\n","Nonzero Parameter for the compressed model: 16.69 %\n","Accuracy of the network on the 2947 test data: 90.91 % after compression\n","\n","Here are the results for l0 group lasso - 4 128 0.0002:\n","Accuracy of the network on the 2947 test data: 90.40 % before compression\n","Nonzero Parameter for the compressed model: 13.24 %\n","Accuracy of the network on the 2947 test data: 90.43 % after compression\n","\n","Here are the results for l0 group lasso - 4 128 0.00001:\n","Accuracy of the network on the 2947 test data: 85.27 % before compression\n","Nonzero Parameter for the compressed model: 77.98 %\n","Accuracy of the network on the 2947 test data: 85.51 % after compression\n","\n","Here are the results for l0 group lasso - 4 256 0.0001:\n","Accuracy of the network on the 2947 test data: 90.43 % before compression\n","Nonzero Parameter for the compressed model: 84.15 %\n","Accuracy of the network on the 2947 test data: 90.40 % after compression\n","\n","Here are the results for l0 group lasso - 4 64 0.0001:\n","Accuracy of the network on the 2947 test data: 62.78 % before compression\n","Nonzero Parameter for the compressed model: 92.17 %\n","Accuracy of the network on the 2947 test data: 62.74 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.0001:\n","Accuracy of the network on the 2947 test data: 78.42 % before compression\n","Nonzero Parameter for the compressed model: 15.53 %\n","Accuracy of the network on the 2947 test data: 78.08 % after compression\n","\n","Here are the results for l0 group lasso - 1 128 0.0001:\n","Accuracy of the network on the 2947 test data: 89.89 % before compression\n","Nonzero Parameter for the compressed model: 32.49 %\n","Accuracy of the network on the 2947 test data: 89.28 % after compression\n","\n","Here are the results for l0 group lasso - 2 128 0.0001:\n","Accuracy of the network on the 2947 test data: 79.13 % before compression\n","Nonzero Parameter for the compressed model: 16.02 %\n","Accuracy of the network on the 2947 test data: 79.37 % after compression\n","\n","Here are the results for l0 group lasso - 3 128 0.0001:\n","Accuracy of the network on the 2947 test data: 78.22 % before compression\n","Nonzero Parameter for the compressed model: 18.69 %\n","Accuracy of the network on the 2947 test data: 78.22 % after compression\n","\n"]}],"source":["PRUNE_THRESHOLD = 0.005\n","\n","class ThresholdPruning(prune.BasePruningMethod):\n","    PRUNING_TYPE = \"unstructured\"\n","\n","    def __init__(self, threshold):\n","        self.threshold = threshold\n","\n","    def compute_mask(self, tensor, default_mask):\n","      return torch.abs(tensor) > self.threshold\n","\n","PATHS = {'No penalty - 4 128 0.0001':            PATH + '/model/final/lr0.0001/no_penalty4.ptl',\n","         'l0 norm - 4 128 0.0001':               PATH + '/model/final/lr0.0001/l0_norm4.ptl',\n","         'l1 norm - 4 128 0.0001':               PATH + '/model/final/lr0.0001/l1_norm4.ptl',\n","         'l2 norm - 4 128 0.0001':               PATH + '/model/final/lr0.0001/l2_norm4.ptl',\n","         'group lasso - 4 128 0.0001':           PATH + '/model/final/lr0.0001/group_lasso4.ptl',\n","         'l1 group lasso - 4 128 0.0001':        PATH + '/model/final/lr0.0001/l1_group_lasso4.ptl',\n","         'l0 group lasso - 4 128 0.0001 (BEST)': PATH + '/model/final/lr0.0001/l0_group_lasso4.ptl',\n","         'l0 group lasso - 4 128 0.00005':       PATH + '/model/final/lr0.00005/l0_group_lasso4.ptl',\n","         'l0 group lasso - 4 128 0.00015':       PATH + '/model/final/lr0.00015/l0_group_lasso4.ptl',\n","         'l0 group lasso - 4 128 0.0002':        PATH + '/model/final/lr0.0002/l0_group_lasso4.ptl',\n","         'l0 group lasso - 4 128 0.00001':       PATH + '/model/final/lr0.00001/l0_group_lasso4.ptl',\n","         'l0 group lasso - 4 256 0.0001':        PATH + '/model/final/256node/l0_group_lasso4.ptl',\n","         'l0 group lasso - 4 64 0.0001':         PATH + '/model/final/64node/l0_group_lasso4.ptl',\n","         'l0 group lasso - 0 128 0.0001':        PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl',\n","         'l0 group lasso - 1 128 0.0001':        PATH + '/model/final/lr0.0001/l0_group_lasso1.ptl',\n","         'l0 group lasso - 2 128 0.0001':        PATH + '/model/final/lr0.0001/l0_group_lasso2.ptl',\n","         'l0 group lasso - 3 128 0.0001':        PATH + '/model/final/lr0.0001/l0_group_lasso3.ptl',\n","        }\n","\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net = torch.load(PATHS[name])\n","\n","  # display the results before compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% before compression' % (test_total, test_acc))\n","\n","  # prune the model\n","  parameters_to_prune = []\n","  for na, child in net.features.named_children():\n","    if int(na) % 2 == 0:\n","      parameters_to_prune.append((child, \"weight\"))\n","  if name == 'l0 group lasso 256':\n","    prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold=PRUNE_THRESHOLD)\n","  else:\n","    prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold=PRUNE_THRESHOLD)\n","\n","  # calculate the sparsity\n","  total_weight = 0\n","  total_nonzero = 0\n","  for na, child in net.features.named_children():\n","    if int(na) % 2 == 0:\n","      total_weight += torch.numel(child.weight)\n","      total_nonzero += torch.count_nonzero(child.weight)\n","  print('Nonzero Parameter for the compressed model: %.2f %%' % (100*float(total_nonzero / total_weight)))\n","\n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))"]},{"cell_type":"markdown","source":["Save pruning model to android model\n","\n"],"metadata":{"id":"P5K2BX5v_WTE"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = 0.005\n","\n","class FooBarPruningMethod(prune.BasePruningMethod):\n","    \"\"\"Prune every other entry in a tensor\n","    \"\"\"\n","    PRUNING_TYPE = 'unstructured'\n","\n","    def compute_mask(self, t, default_mask):\n","      return torch.abs(t) > PRUNE_THRESHOLD\n","\n","def foobar_unstructured(module, name):\n","    FooBarPruningMethod.apply(module, name)\n","    return module\n","\n","PATHS = {'l0_norm':       PATH + '/model/final/lr0.0001/l0_norm',\n","      'l1_norm':       PATH + '/model/final/lr0.0001/l1_norm',\n","      'l2_norm':       PATH + '/model/final/lr0.0001/l2_norm',\n","      'group_lasso':     PATH + '/model/final/lr0.0001/group_lasso',\n","      'l1_group_lasso':   PATH + '/model/final/lr0.0001/l1_group_lasso',\n","      'l0_group_lasso': PATH + '/model/final/lr0.0001/l0_group_lasso',\n","    }\n","\n","for name in PATHS:\n","  # load the model\n","  net_uncompress = torch.load(PATHS[name] + \"4.ptl\")\n","  torch.save(net_uncompress, PATH + '/model/android_model/cuda_model/' + name + \"_uncompressed.ptl\")\n","\n","  foobar_unstructured(net_uncompress.features[0], \"weight\")\n","  foobar_unstructured(net_uncompress.features[2], \"weight\")\n","  foobar_unstructured(net_uncompress.features[4], \"weight\")\n","  foobar_unstructured(net_uncompress.features[6], \"weight\")\n","  foobar_unstructured(net_uncompress.features[8], \"weight\")\n","\n","  net_compress = torch.load(PATHS[name] + \"4.ptl\")\n","  net_compress.features[0].weight = torch.nn.Parameter(net_uncompress.features[0].weight)\n","  net_compress.features[2].weight = torch.nn.Parameter(net_uncompress.features[2].weight)\n","  net_compress.features[4].weight = torch.nn.Parameter(net_uncompress.features[4].weight)\n","  net_compress.features[6].weight = torch.nn.Parameter(net_uncompress.features[6].weight)\n","  net_compress.features[8].weight = torch.nn.Parameter(net_uncompress.features[8].weight)\n","  \n","  torch.save(net_compress, PATH + '/model/android_model/cuda_model/' + name + \"_compressed.ptl\")\n"],"metadata":{"id":"XddjIO3L_W6z","executionInfo":{"status":"ok","timestamp":1640627071345,"user_tz":300,"elapsed":1413,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Save cuda model to cpu model\n","\n"],"metadata":{"id":"EIG_SoKy_nUF"}},{"cell_type":"code","source":["PATHS = {'l0_norm':       PATH + '/model/android_model/cuda_model/l0_norm',\n","      'l1_norm':       PATH + '/model/android_model/cuda_model/l1_norm',\n","      'l2_norm':       PATH + '/model/android_model/cuda_model/l2_norm',\n","      'group_lasso':     PATH + '/model/android_model/cuda_model/group_lasso',\n","      'l1_group_lasso':   PATH + '/model/android_model/cuda_model/l1_group_lasso',\n","      'l0_group_lasso': PATH + '/model/android_model/cuda_model/l0_group_lasso',\n","    }\n","\n","for name in PATHS:\n","  # load the model\n","  net_uncompress = torch.load(PATHS[name] + \"_uncompressed.ptl\", map_location='cpu')\n","  torch.jit.script(net_uncompress)._save_for_lite_interpreter(PATH + '/model/android_model/cpu_model/' + name + \"_uncompressed.ptl\")\n","\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  torch.jit.script(net_compress)._save_for_lite_interpreter(PATH + '/model/android_model/cpu_model/' + name + \"_compressed.ptl\")"],"metadata":{"id":"YbEuN0hP_pKv","executionInfo":{"status":"ok","timestamp":1640627107128,"user_tz":300,"elapsed":7431,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Obtain the pruning results for l0 norm, l1 norm, l2 norm, group lasso, l0 group lasso and l1 group lasso\n","\n"],"metadata":{"id":"qzpqehhy_ujG"}},{"cell_type":"code","source":["PATHS = {'l0_group_lasso': PATH + '/model/android_model/cuda_model/l0_group_lasso',\n","      'l0_norm':       PATH + '/model/android_model/cuda_model/l0_norm',\n","      'l1_norm':       PATH + '/model/android_model/cuda_model/l1_norm',\n","      'l2_norm':       PATH + '/model/android_model/cuda_model/l2_norm',\n","      'group_lasso':     PATH + '/model/android_model/cuda_model/group_lasso',\n","      'l1_group_lasso':   PATH + '/model/android_model/cuda_model/l1_group_lasso',\n","    }\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  \n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = net_compress(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression' % (test_total, test_acc))\n","\n","  total_paramaeters_uncompressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * net_compress.features[i].weight.data.size()[j]\n","    total_paramaeters_uncompressed += weight_count\n","\n","  new_layer_node = []\n","  for layer in range(0,8,2):\n","    node_count = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    else:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    new_layer_node.append(node_count)\n","    \n","  new_compress_net = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  index = 0\n","  for i in range(0,10,2):\n","    if i == 0:\n","      new_compress_net.features[i] = nn.Conv1d(FEATURE_SIZE, new_layer_node[index], kernel_size=KERNAL_SIZE, bias=False)\n","    elif i == 8:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False)\n","    else:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], new_layer_node[index+1], kernel_size=KERNAL_SIZE, bias=False)\n","      index+=1  \n","  node_indexes = []\n","  for layer in range(0,10,2):\n","    index = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(FEATURE_SIZE):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][j]\n","          index+=1\n","          node_indexes.append(i)\n","    elif layer == 8:\n","      for i in range(NODE_SIZE):\n","        for j in range(len(node_indexes)):\n","          new_compress_net.features[layer].weight.data[i][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","    else:\n","      temp_indexes = []\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(len(node_indexes)):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","          index+=1\n","          temp_indexes.append(i)\n","      node_indexes = temp_indexes\n","\n","  # display the results after compressed model\n","  new_compress_net = new_compress_net.double()\n","  total_paramaeters_compressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * new_compress_net.features[i].weight.data.size()[j]\n","    total_paramaeters_compressed += weight_count\n","\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_paramaeters_compressed) / total_paramaeters_uncompressed))\n","\n","  # display node remainng\n","  node_count = 0\n","  for i in range(len(new_layer_node)): \n","    node_count += new_layer_node[i]\n","  print('Sparity for the node remaining: %.2f %%' % (100*float(node_count) / (4.0*NODE_SIZE)))\n","\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = new_compress_net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n","  torch.jit.script(new_compress_net)._save_for_lite_interpreter(PATH + '/model/android_model/android_final_model/' + name + \"_compressed.ptl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYNyX4L1_w3u","executionInfo":{"status":"ok","timestamp":1640627224276,"user_tz":300,"elapsed":69952,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}},"outputId":"3b02fb4c-8e9c-4f33-9acc-1c1da30e1e01"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for l0_group_lasso:\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","Sparity for the compressed model: 46.83 %\n","Sparity for the node remaining: 68.75 %\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","\n","Here are the results for l0_norm:\n","Accuracy of the network on the 2947 test data: 91.48 % after compression\n","Sparity for the compressed model: 100.00 %\n","Sparity for the node remaining: 100.00 %\n","Accuracy of the network on the 2947 test data: 91.48 % after compression\n","\n","Here are the results for l1_norm:\n","Accuracy of the network on the 2947 test data: 90.33 % after compression\n","Sparity for the compressed model: 98.47 %\n","Sparity for the node remaining: 99.22 %\n","Accuracy of the network on the 2947 test data: 90.33 % after compression\n","\n","Here are the results for l2_norm:\n","Accuracy of the network on the 2947 test data: 90.94 % after compression\n","Sparity for the compressed model: 100.00 %\n","Sparity for the node remaining: 100.00 %\n","Accuracy of the network on the 2947 test data: 90.94 % after compression\n","\n","Here are the results for group_lasso:\n","Accuracy of the network on the 2947 test data: 90.84 % after compression\n","Sparity for the compressed model: 100.00 %\n","Sparity for the node remaining: 100.00 %\n","Accuracy of the network on the 2947 test data: 90.84 % after compression\n","\n","Here are the results for l1_group_lasso:\n","Accuracy of the network on the 2947 test data: 91.04 % after compression\n","Sparity for the compressed model: 97.70 %\n","Sparity for the node remaining: 98.83 %\n","Accuracy of the network on the 2947 test data: 91.04 % after compression\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"8_-XXmzxzNng"},"source":["# Other Results -- learning rate, cross validation and threshold\n"]},{"cell_type":"markdown","source":["k-fold cross validation results\n","\n"],"metadata":{"id":"i5Gb94K_AM4I"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = 0.005\n","\n","class FooBarPruningMethod(prune.BasePruningMethod):\n","    \"\"\"Prune every other entry in a tensor\n","    \"\"\"\n","    PRUNING_TYPE = 'unstructured'\n","\n","    def compute_mask(self, t, default_mask):\n","      return torch.abs(t) > PRUNE_THRESHOLD\n","\n","def foobar_unstructured(module, name):\n","    FooBarPruningMethod.apply(module, name)\n","    return module\n","\n","PATHS = {'0':   PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl',\n","      '1':   PATH + '/model/final/lr0.0001/l0_group_lasso1.ptl',\n","      '2':   PATH + '/model/final/lr0.0001/l0_group_lasso2.ptl',\n","      '3':   PATH + '/model/final/lr0.0001/l0_group_lasso3.ptl',\n","      '4':   PATH + '/model/final/lr0.0001/l0_group_lasso4.ptl',\n","    }\n","\n","for name in PATHS:\n","  # load the model\n","  net_uncompress = torch.load(PATHS[name])\n","  torch.save(net_uncompress, PATH + '/model/android_model/kfold/' + name + \"_uncompressed.ptl\")\n","\n","  foobar_unstructured(net_uncompress.features[0], \"weight\")\n","  foobar_unstructured(net_uncompress.features[2], \"weight\")\n","  foobar_unstructured(net_uncompress.features[4], \"weight\")\n","  foobar_unstructured(net_uncompress.features[6], \"weight\")\n","  foobar_unstructured(net_uncompress.features[8], \"weight\")\n","\n","  net_compress = torch.load(PATHS[name])\n","  net_compress.features[0].weight = torch.nn.Parameter(net_uncompress.features[0].weight)\n","  net_compress.features[2].weight = torch.nn.Parameter(net_uncompress.features[2].weight)\n","  net_compress.features[4].weight = torch.nn.Parameter(net_uncompress.features[4].weight)\n","  net_compress.features[6].weight = torch.nn.Parameter(net_uncompress.features[6].weight)\n","  net_compress.features[8].weight = torch.nn.Parameter(net_uncompress.features[8].weight)\n","\n","  torch.save(net_compress, PATH + '/model/android_model/kfold/' + name + \"_compressed.ptl\")"],"metadata":{"id":"JqBAmnQKAHX9","executionInfo":{"status":"ok","timestamp":1640627253173,"user_tz":300,"elapsed":4335,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["PATHS = {'0':  PATH + '/model/android_model/kfold/0',\n","      '1':  PATH + '/model/android_model/kfold/1',\n","      '2':  PATH + '/model/android_model/kfold/2',\n","      '3':  PATH + '/model/android_model/kfold/3',\n","      '4':  PATH + '/model/android_model/kfold/4',\n","    }\n","for name in PATHS:\n","  print('Here are the results for fold {}:'.format(name))\n","  # load the model\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  \n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = net_compress(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression' % (test_total, test_acc))\n","\n","  total_paramaeters_uncompressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * net_compress.features[i].weight.data.size()[j]\n","    total_paramaeters_uncompressed += weight_count\n","\n","  new_layer_node = []\n","  for layer in range(0,8,2):\n","    node_count = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    else:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    new_layer_node.append(node_count)\n","    \n","  new_compress_net = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  index = 0\n","  for i in range(0,10,2):\n","    if i == 0:\n","      new_compress_net.features[i] = nn.Conv1d(FEATURE_SIZE, new_layer_node[index], kernel_size=KERNAL_SIZE, bias=False)\n","    elif i == 8:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False)\n","    else:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], new_layer_node[index+1], kernel_size=KERNAL_SIZE, bias=False)\n","      index+=1  \n","  node_indexes = []\n","  for layer in range(0,10,2):\n","    index = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(FEATURE_SIZE):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][j]\n","          index+=1\n","          node_indexes.append(i)\n","    elif layer == 8:\n","      for i in range(NODE_SIZE):\n","        for j in range(len(node_indexes)):\n","          new_compress_net.features[layer].weight.data[i][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","    else:\n","      temp_indexes = []\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(len(node_indexes)):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","          index+=1\n","          temp_indexes.append(i)\n","      node_indexes = temp_indexes\n","\n","  # display the results after compressed model\n","  new_compress_net = new_compress_net.double()\n","  total_paramaeters_compressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * new_compress_net.features[i].weight.data.size()[j]\n","    total_paramaeters_compressed += weight_count\n","\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_paramaeters_compressed) / total_paramaeters_uncompressed))\n","\n","  # display node remainng\n","  node_count = 0\n","  for i in range(len(new_layer_node)): \n","    node_count += new_layer_node[i]\n","  print('Sparity for the node remaining: %.2f %%' % (100*float(node_count) / (4.0*NODE_SIZE)))\n","\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = new_compress_net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n","  torch.jit.script(new_compress_net)._save_for_lite_interpreter(PATH + '/model/android_model/android_final_model/' + name + \"_compressed.ptl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJqRTOEMASwG","executionInfo":{"status":"ok","timestamp":1640627338185,"user_tz":300,"elapsed":49366,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}},"outputId":"52e5ef09-a89d-4a42-dbc4-305e642493db"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for fold 0:\n","Accuracy of the network on the 2947 test data: 78.08 % after compression\n","Sparity for the compressed model: 31.99 %\n","Sparity for the node remaining: 56.64 %\n","Accuracy of the network on the 2947 test data: 78.08 % after compression\n","\n","Here are the results for fold 1:\n","Accuracy of the network on the 2947 test data: 89.28 % after compression\n","Sparity for the compressed model: 64.29 %\n","Sparity for the node remaining: 80.27 %\n","Accuracy of the network on the 2947 test data: 89.28 % after compression\n","\n","Here are the results for fold 2:\n","Accuracy of the network on the 2947 test data: 79.37 % after compression\n","Sparity for the compressed model: 32.25 %\n","Sparity for the node remaining: 56.84 %\n","Accuracy of the network on the 2947 test data: 79.37 % after compression\n","\n","Here are the results for fold 3:\n","Accuracy of the network on the 2947 test data: 78.22 % after compression\n","Sparity for the compressed model: 40.02 %\n","Sparity for the node remaining: 63.48 %\n","Accuracy of the network on the 2947 test data: 78.22 % after compression\n","\n","Here are the results for fold 4:\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","Sparity for the compressed model: 46.83 %\n","Sparity for the node remaining: 68.75 %\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","\n"]}]},{"cell_type":"markdown","source":["Results for different learning rate.\n","\n"],"metadata":{"id":"7BrO4NDZArf0"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = 0.005\n","\n","class FooBarPruningMethod(prune.BasePruningMethod):\n","    \"\"\"Prune every other entry in a tensor\n","    \"\"\"\n","    PRUNING_TYPE = 'unstructured'\n","\n","    def compute_mask(self, t, default_mask):\n","      return torch.abs(t) > PRUNE_THRESHOLD\n","\n","def foobar_unstructured(module, name):\n","    FooBarPruningMethod.apply(module, name)\n","    return module\n","\n","PATHS = {'0_0001':   PATH + '/model/final/lr0.0001/l0_group_lasso4.ptl',\n","      '0_00015':  PATH + '/model/final/lr0.00015/l0_group_lasso4.ptl',\n","      '0_00001':  PATH + '/model/final/lr0.00001/l0_group_lasso4.ptl',\n","      '0_00005':  PATH + '/model/final/lr0.00005/l0_group_lasso4.ptl',\n","      '0_0002':  PATH + '/model/final/lr0.0002/l0_group_lasso4.ptl',\n","    }\n","\n","for name in PATHS:\n","  # load the model\n","  net_uncompress = torch.load(PATHS[name])\n","  torch.save(net_uncompress, PATH + '/model/android_model/learning_rate/' + name + \"_uncompressed.ptl\")\n","\n","  foobar_unstructured(net_uncompress.features[0], \"weight\")\n","  foobar_unstructured(net_uncompress.features[2], \"weight\")\n","  foobar_unstructured(net_uncompress.features[4], \"weight\")\n","  foobar_unstructured(net_uncompress.features[6], \"weight\")\n","  foobar_unstructured(net_uncompress.features[8], \"weight\")\n","\n","  net_compress = torch.load(PATHS[name])\n","  net_compress.features[0].weight = torch.nn.Parameter(net_uncompress.features[0].weight)\n","  net_compress.features[2].weight = torch.nn.Parameter(net_uncompress.features[2].weight)\n","  net_compress.features[4].weight = torch.nn.Parameter(net_uncompress.features[4].weight)\n","  net_compress.features[6].weight = torch.nn.Parameter(net_uncompress.features[6].weight)\n","  net_compress.features[8].weight = torch.nn.Parameter(net_uncompress.features[8].weight)\n","\n","  torch.save(net_compress, PATH + '/model/android_model/learning_rate/' + name + \"_compressed.ptl\")\n"],"metadata":{"id":"iARB6CfXAv_3","executionInfo":{"status":"ok","timestamp":1640627390160,"user_tz":300,"elapsed":5395,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["PATHS = {'0_0001':       PATH + '/model/android_model/learning_rate/0_0001',\n","      '0_00015':       PATH + '/model/android_model/learning_rate/0_00015',\n","      '0_00001':        PATH + '/model/android_model/learning_rate/0_00001',\n","      '0_00005':     PATH + '/model/android_model/learning_rate/0_00005',\n","      '0_0002':    PATH + '/model/android_model/learning_rate/0_0002',\n","    }\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  \n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = net_compress(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression' % (test_total, test_acc))\n","\n","  total_paramaeters_uncompressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * net_compress.features[i].weight.data.size()[j]\n","    total_paramaeters_uncompressed += weight_count\n","\n","  new_layer_node = []\n","  for layer in range(0,8,2):\n","    node_count = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    else:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    new_layer_node.append(node_count)\n","  new_compress_net = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  index = 0\n","  for i in range(0,10,2):\n","    if i == 0:\n","      new_compress_net.features[i] = nn.Conv1d(FEATURE_SIZE, new_layer_node[index], kernel_size=KERNAL_SIZE, bias=False)\n","    elif i == 8:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False)\n","    else:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], new_layer_node[index+1], kernel_size=KERNAL_SIZE, bias=False)\n","      index+=1  \n","  node_indexes = []\n","  for layer in range(0,10,2):\n","    index = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(FEATURE_SIZE):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][j]\n","          index+=1\n","          node_indexes.append(i)\n","    elif layer == 8:\n","      for i in range(NODE_SIZE):\n","        for j in range(len(node_indexes)):\n","          new_compress_net.features[layer].weight.data[i][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","    else:\n","      temp_indexes = []\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(len(node_indexes)):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","          index+=1\n","          temp_indexes.append(i)\n","      node_indexes = temp_indexes\n","\n","  # display the results after compressed model\n","  new_compress_net = new_compress_net.double()\n","  total_paramaeters_compressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * new_compress_net.features[i].weight.data.size()[j]\n","    total_paramaeters_compressed += weight_count\n","\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_paramaeters_compressed) / total_paramaeters_uncompressed))\n","\n","  # display node remainng\n","  node_count = 0\n","  for i in range(len(new_layer_node)): \n","    node_count += new_layer_node[i]\n","  print('Sparity for the node remaining: %.2f %%' % (100*float(node_count) / (4.0*NODE_SIZE)))\n","\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = new_compress_net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n","  torch.jit.script(new_compress_net)._save_for_lite_interpreter(PATH + '/model/android_model/android_final_model/' + name + \"_compressed.ptl\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fi6RGN_hA1NQ","executionInfo":{"status":"ok","timestamp":1640627487142,"user_tz":300,"elapsed":63093,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}},"outputId":"80e28d11-e424-4883-bb84-949225be7942"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for 0_0001:\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","Sparity for the compressed model: 46.83 %\n","Sparity for the node remaining: 68.75 %\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","\n","Here are the results for 0_00015:\n","Accuracy of the network on the 2947 test data: 90.91 % after compression\n","Sparity for the compressed model: 31.04 %\n","Sparity for the node remaining: 56.45 %\n","Accuracy of the network on the 2947 test data: 90.91 % after compression\n","\n","Here are the results for 0_00001:\n","Accuracy of the network on the 2947 test data: 85.51 % after compression\n","Sparity for the compressed model: 94.66 %\n","Sparity for the node remaining: 97.27 %\n","Accuracy of the network on the 2947 test data: 85.51 % after compression\n","\n","Here are the results for 0_00005:\n","Accuracy of the network on the 2947 test data: 89.24 % after compression\n","Sparity for the compressed model: 85.77 %\n","Sparity for the node remaining: 92.58 %\n","Accuracy of the network on the 2947 test data: 89.24 % after compression\n","\n","Here are the results for 0_0002:\n","Accuracy of the network on the 2947 test data: 90.43 % after compression\n","Sparity for the compressed model: 29.10 %\n","Sparity for the node remaining: 54.10 %\n","Accuracy of the network on the 2947 test data: 90.43 % after compression\n","\n"]}]},{"cell_type":"markdown","source":["Results for different threshold\n","\n"],"metadata":{"id":"znC39ZMYA-I0"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = np.arange(0.001, 0.010, 0.002)\n","\n","def foobar_unstructured(module, name):\n","    FooBarPruningMethod.apply(module, name)\n","    return module\n","\n","PATHS = {'l0_group_lasso':   PATH + '/model/final/lr0.0001/l0_group_lasso4.ptl',}\n","\n","for threshold in PRUNE_THRESHOLD:\n","  class FooBarPruningMethod(prune.BasePruningMethod):\n","    \"\"\"Prune every other entry in a tensor\n","    \"\"\"\n","    PRUNING_TYPE = 'unstructured'\n","\n","    def compute_mask(self, t, default_mask):\n","      return torch.abs(t) > threshold\n","\n","  for name in PATHS:\n","    # load the model\n","    net_uncompress = torch.load(PATHS[name])\n","    torch.save(net_uncompress, PATH + '/model/android_model/threshold/' + str(threshold) + \"_uncompressed.ptl\")\n","\n","    foobar_unstructured(net_uncompress.features[0], \"weight\")\n","    foobar_unstructured(net_uncompress.features[2], \"weight\")\n","    foobar_unstructured(net_uncompress.features[4], \"weight\")\n","    foobar_unstructured(net_uncompress.features[6], \"weight\")\n","    foobar_unstructured(net_uncompress.features[8], \"weight\")\n","\n","    net_compress = torch.load(PATHS[name])\n","    net_compress.features[0].weight = torch.nn.Parameter(net_uncompress.features[0].weight)\n","    net_compress.features[2].weight = torch.nn.Parameter(net_uncompress.features[2].weight)\n","    net_compress.features[4].weight = torch.nn.Parameter(net_uncompress.features[4].weight)\n","    net_compress.features[6].weight = torch.nn.Parameter(net_uncompress.features[6].weight)\n","    net_compress.features[8].weight = torch.nn.Parameter(net_uncompress.features[8].weight)\n","\n","    torch.save(net_compress, PATH + '/model/android_model/threshold/' + str(threshold) + \"_compressed.ptl\")\n"],"metadata":{"id":"a3RIRlWrA_-F","executionInfo":{"status":"ok","timestamp":1640627496899,"user_tz":300,"elapsed":735,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["PATHS = {'0.001':       PATH + '/model/android_model/threshold/0.001',\n","      '0.003':       PATH + '/model/android_model/threshold/0.003',\n","      '0.005':        PATH + '/model/android_model/threshold/0.005',\n","      '0.007':     PATH + '/model/android_model/threshold/0.007',\n","      '0.009':    PATH + '/model/android_model/threshold/0.009',\n","    }\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net_compress = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  \n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = net_compress(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression' % (test_total, test_acc))\n","\n","  total_paramaeters_uncompressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * net_compress.features[i].weight.data.size()[j]\n","    total_paramaeters_uncompressed += weight_count\n","\n","  new_layer_node = []\n","  for layer in range(0,8,2):\n","    node_count = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    else:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          node_count+=1\n","    new_layer_node.append(node_count)\n","    \n","  new_compress_net = torch.load(PATHS[name] + \"_compressed.ptl\", map_location='cpu')\n","  index = 0\n","  for i in range(0,10,2):\n","    if i == 0:\n","      new_compress_net.features[i] = nn.Conv1d(FEATURE_SIZE, new_layer_node[index], kernel_size=KERNAL_SIZE, bias=False)\n","    elif i == 8:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False)\n","    else:\n","      new_compress_net.features[i] = nn.Conv1d(new_layer_node[index], new_layer_node[index+1], kernel_size=KERNAL_SIZE, bias=False)\n","      index+=1  \n","  node_indexes = []\n","  for layer in range(0,10,2):\n","    index = 0\n","    if layer == 0:\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(FEATURE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(FEATURE_SIZE):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][j]\n","          index+=1\n","          node_indexes.append(i)\n","    elif layer == 8:\n","      for i in range(NODE_SIZE):\n","        for j in range(len(node_indexes)):\n","          new_compress_net.features[layer].weight.data[i][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","    else:\n","      temp_indexes = []\n","      for i in range(NODE_SIZE):\n","        nonzero_count = False\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer].weight.data[i][j])):\n","              nonzero_count = True\n","              break\n","        for j in range(NODE_SIZE):\n","          if torch.is_nonzero(torch.sum(net_compress.features[layer+2].weight.data[j][i])):\n","              nonzero_count = True\n","              break\n","        if nonzero_count == True:\n","          for j in range(len(node_indexes)):\n","            new_compress_net.features[layer].weight.data[index][j] = net_compress.features[layer].weight.data[i][node_indexes[j]]\n","          index+=1\n","          temp_indexes.append(i)\n","      node_indexes = temp_indexes\n","\n","  # display the results after compressed model\n","  new_compress_net = new_compress_net.double()\n","  total_paramaeters_compressed = 0\n","  for i in range(0,10,2):\n","    weight_count = 1\n","    for j in range(3):\n","      weight_count = weight_count * new_compress_net.features[i].weight.data.size()[j]\n","    total_paramaeters_compressed += weight_count\n","\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_paramaeters_compressed) / total_paramaeters_uncompressed))\n","\n","  # display node remainng\n","  node_count = 0\n","  for i in range(len(new_layer_node)): \n","    node_count += new_layer_node[i]\n","  print('Sparity for the node remaining: %.2f %%' % (100*float(node_count) / (4.0*NODE_SIZE)))\n","\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      outputs = new_compress_net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n","  torch.jit.script(new_compress_net)._save_for_lite_interpreter(PATH + '/model/android_model/android_final_model/' + name + \"_compressed.ptl\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kV2BLx45BPSu","executionInfo":{"status":"ok","timestamp":1640627640661,"user_tz":300,"elapsed":49164,"user":{"displayName":"Shao Yubo","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14299479550668088872"}},"outputId":"fe8e598a-4571-4d1d-b34a-74a1143eea0b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for 0.001:\n","Accuracy of the network on the 2947 test data: 90.02 % after compression\n","Sparity for the compressed model: 54.68 %\n","Sparity for the node remaining: 74.02 %\n","Accuracy of the network on the 2947 test data: 90.02 % after compression\n","\n","Here are the results for 0.003:\n","Accuracy of the network on the 2947 test data: 90.06 % after compression\n","Sparity for the compressed model: 48.55 %\n","Sparity for the node remaining: 70.12 %\n","Accuracy of the network on the 2947 test data: 90.06 % after compression\n","\n","Here are the results for 0.005:\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","Sparity for the compressed model: 46.83 %\n","Sparity for the node remaining: 68.75 %\n","Accuracy of the network on the 2947 test data: 89.96 % after compression\n","\n","Here are the results for 0.007:\n","Accuracy of the network on the 2947 test data: 89.14 % after compression\n","Sparity for the compressed model: 44.66 %\n","Sparity for the node remaining: 67.38 %\n","Accuracy of the network on the 2947 test data: 89.14 % after compression\n","\n","Here are the results for 0.009:\n","Accuracy of the network on the 2947 test data: 87.28 % after compression\n","Sparity for the compressed model: 44.36 %\n","Sparity for the node remaining: 67.19 %\n","Accuracy of the network on the 2947 test data: 87.28 % after compression\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["RE7AbBJmBjJo","ZBGhSZcP1DwW","ggHEAvHi1Hib","QvBRzZPo1Qqb"],"machine_shape":"hm","name":"ucihar_dataset.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}