{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pamap2_dataset.ipynb","provenance":[],"collapsed_sections":["ZSWX9EHW97uU"],"machine_shape":"hm","authorship_tag":"ABX9TyOrK27OCV+ew7Dv9L38PbzU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"28xFqxTcxvpz"},"source":["## Connect Google Drive and GPU\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CexARwSGxGbm","executionInfo":{"status":"ok","timestamp":1639871913605,"user_tz":300,"elapsed":17064,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"outputId":"bbf71f30-fd5e-4a45-9eff-3e0cf4e83219"},"source":["%reset\n","\n","# connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# connect colab gpu\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n","Mounted at /content/drive\n","Sat Dec 18 23:58:33 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"P8kydhB6xzBa"},"source":["## Import Needed Libraries, Paramaters and Functions"]},{"cell_type":"code","metadata":{"id":"7lbyFQSnxylK","executionInfo":{"status":"ok","timestamp":1639871919977,"user_tz":300,"elapsed":6379,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}}},"source":["import sys\n","import csv\n","import time\n","import os.path\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.utils import prune\n","import torchvision\n","import matplotlib.pyplot as plt\n","from torch.utils.mobile_optimizer import optimize_for_mobile\n","from scipy import stats\n","from sklearn.utils import shuffle\n","\n","SEED = 10\n","WINDOW_SIZE = 128\n","FEATURE_SIZE = 40\n","LABEL_SIZE = 12\n","BATCH_SIZE = 3\n","PATH = '/content/drive/MyDrive/CNNPaper'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ca_WAZ8Qxdp3","executionInfo":{"status":"ok","timestamp":1639871920137,"user_tz":300,"elapsed":168,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}}},"source":["def dataCleaning(dataCollection):\n","  dataCollection = dataCollection.drop(['handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4',\n","                      'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4',\n","                      'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'],\n","                      axis = 1)  # removal of orientation columns as they are not needed\n","  dataCollection = dataCollection.drop(dataCollection[dataCollection.activityID == 0].index) #removal of any row of activity 0 as it is transient activity which it is not used\n","  dataCollection = dataCollection.apply(pd.to_numeric, errors = 'coerce') #removal of non numeric data in cells\n","  dataCollection = dataCollection.interpolate() #removal of any remaining NaN value cells by constructing new data points in known set of data points\n","  dataCollection.reset_index(drop = True, inplace = True)\n","  for i in range(0,4):\n","    dataCollection[\"heartrate\"].iloc[i]=100  \n","  return dataCollection\n","\n","def read_data():\n","  if os.path.isfile(PATH + '/data/PAMAP2/data.cvs'):\n","    print(\"Start reading data ...\")\n","    dataCollection = pd.read_csv(PATH + '/data/PAMAP2/data.cvs')\n","    print(\"Finish reading data ...\")\n","  else:\n","    print(\"Start reading data ...\")\n","    list_of_files = ['/data/PAMAP2/Protocol/subject101.dat',\n","              '/data/PAMAP2/Protocol/subject102.dat',\n","              '/data/PAMAP2/Protocol/subject103.dat',\n","              '/data/PAMAP2/Protocol/subject104.dat',\n","              '/data/PAMAP2/Protocol/subject105.dat',\n","              '/data/PAMAP2/Protocol/subject106.dat',\n","              '/data/PAMAP2/Protocol/subject107.dat',\n","              '/data/PAMAP2/Protocol/subject108.dat',\n","              '/data/PAMAP2/Protocol/subject109.dat' ]\n","    subjectID = [1,2,3,4,5,6,7,8,9]\n","    activityIDdict = {0: 'transient',\n","              1: 'lying',\n","              2: 'sitting',\n","              3: 'standing',\n","              4: 'walking',\n","              5: 'running',\n","              6: 'cycling',\n","              7: 'Nordic_walking',\n","              9: 'watching_TV',\n","              10: 'computer_work',\n","              11: 'car driving',\n","              12: 'ascending_stairs',\n","              13: 'descending_stairs',\n","              16: 'vacuum_cleaning',\n","              17: 'ironing',\n","              18: 'folding_laundry',\n","              19: 'house_cleaning',\n","              20: 'playing_soccer',\n","              24: 'rope_jumping'}\n","\n","    colNames = [\"timestamp\", \"activityID\",\"heartrate\"]\n","\n","    IMUhand = ['handTemperature', \n","          'handAcc16_1', 'handAcc16_2', 'handAcc16_3', \n","          'handAcc6_1', 'handAcc6_2', 'handAcc6_3', \n","          'handGyro1', 'handGyro2', 'handGyro3', \n","          'handMagne1', 'handMagne2', 'handMagne3',\n","          'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4']\n","\n","    IMUchest = ['chestTemperature', \n","          'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', \n","          'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', \n","          'chestGyro1', 'chestGyro2', 'chestGyro3', \n","          'chestMagne1', 'chestMagne2', 'chestMagne3',\n","          'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4']\n","\n","    IMUankle = ['ankleTemperature', \n","          'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', \n","          'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', \n","          'ankleGyro1', 'ankleGyro2', 'ankleGyro3', \n","          'ankleMagne1', 'ankleMagne2', 'ankleMagne3',\n","          'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4']\n","\n","    columns = colNames + IMUhand + IMUchest + IMUankle\n","\n","    dataCollection = pd.DataFrame()\n","    for file in list_of_files:\n","        procData = pd.read_table(PATH + file, header=None, sep='\\s+')\n","        procData.columns = columns\n","        procData['subject_id'] = int(file[-5])\n","        dataCollection = dataCollection.append(procData, ignore_index=True)\n","\n","    dataCollection.reset_index(drop=True, inplace=True)\n","    dataCollection = dataCleaning(dataCollection)\n","    dataCollection.to_csv('data.cvs', index=False)\n","    print(\"Finish reading data ...\")\n","  return dataCollection\n","\n","def feature_normalize(data):\n","  \"\"\"\n","    Normalize the feature data\n","    Paramater:\n","      data: a list of floats\n","    Return:\n","      a list of floats with normalized data\n","  \"\"\"\n","  mu = np.mean(data, axis=0)\n","  sigma = np.std(data, axis=0)\n","  return (data - mu) / sigma\n","\n","\n","def dataset_normalize(dataset):\n","  \"\"\"\n","    Normalize the whole dataset\n","    Paramater:\n","      dataset: a DataFrame with the data and labels \n","    Return:\n","      a DataFrame with the normalized data and labels \n","  \"\"\"\n","  dataset.dropna(axis=0, how='any', inplace=True)\n","  for col in dataset.columns:\n","    if col != 'timestamp' and col != 'activityID' and col != 'subject_id':\n","      dataset[col] = feature_normalize(dataset[col])\n","  return dataset\n","\n","def windows(data, size):\n","  \"\"\"\n","    Obatin the starting index and ending index according to window size\n","    Paramater:\n","      data: a list of floats\n","      size: int\n","    Return:\n","      Starting index, ending index\n","  \"\"\"\n","  start = 0\n","  while start < data.count():\n","    yield int(start), int(start + size)\n","    start += (size / 2)\n","\n","def dataset_segmentation(data):\n","  \"\"\"\n","    Dataset segmentation according the window size\n","    Paramater:\n","      data: a list of floats\n","    Return:\n","      segments and labels \n","  \"\"\"\n","  print(\"Start segmentation with window size: \", WINDOW_SIZE)\n","  segments = np.empty((0, WINDOW_SIZE, FEATURE_SIZE))\n","  labels = np.empty((0))\n","  size = data['timestamp'].count()\n","  for (start, end) in windows(data['timestamp'], WINDOW_SIZE):\n","      temp = []\n","      for col in data.columns:\n","        if col != 'timestamp' and col != 'activityID' and col != 'subject_id':\n","          array = data[col][start:end].to_numpy().tolist()\n","          temp.append(array)\n","      if len(data[\"timestamp\"][start:end]) == WINDOW_SIZE:\n","        segments = np.vstack([segments, np.dstack(temp)])\n","        labels = np.append(labels, stats.mode(data[\"activityID\"][start:end])[0][0])\n","  labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n","  segments = segments.reshape(len(segments), FEATURE_SIZE, WINDOW_SIZE)\n","  print(\"Finish segmentation ...\")\n","  return segments, labels\n","\n","def train_valid_test_split(segments, classes, test_x, test_y, k_fold):\n","  \"\"\"\n","    Split train, valid and test datase\n","    Paramater:\n","      segments: a list of input data\n","      classes: a list of classes data\n","      k: k fold cross validation\n","    Return:\n","      segments and labels \n","  \"\"\"\n","  print(\"Start dataset split... \")\n","  seg_len = len(segments)\n","  idx_val = [0, int(seg_len/5*1), int(seg_len/5*2), int(seg_len/5*3), int(seg_len/5*4), seg_len]\n","  train_range1 = range(0, idx_val[k_fold])\n","  valid_range = range(idx_val[k_fold], idx_val[k_fold+1])\n","  train_range2 = range(idx_val[k_fold+1], seg_len)\n","\n","  train_x = np.concatenate((segments[train_range1], segments[train_range2]), axis=0)\n","  train_y = np.concatenate((classes[train_range1], classes[train_range2]), axis=0)\n","  valid_x = segments[valid_range]\n","  valid_y = classes[valid_range]\n","\n","  # get train data\n","  train_data = []\n","  for i in range(len(train_x)):\n","    train_data.append([train_x[i], train_y[i]])\n","  \n","  # get valid data\n","  valid_data = []\n","  for i in range(len(valid_x)):\n","    valid_data.append([valid_x[i], valid_y[i]])\n","  \n","  # get test data\n","  test_data = []\n","  for i in range(len(test_x)):\n","    test_data.append([test_x[i], test_y[i]])\n","  print(len(train_data))\n","  print(len(valid_data))\n","  print(len(test_data))\n","\n","  # generate DataLoader for each dataset\n","  trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n","  validloader = torch.utils.data.DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n","  testloader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)\n","  \n","  print(\"Finish dataset split... \")\n","  return trainloader, validloader, testloader\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Load and Save Train, Test, Valid Dataset\n","\n","\n","\n"],"metadata":{"id":"ZSWX9EHW97uU"}},{"cell_type":"code","metadata":{"id":"dYOjY12143Iv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639658455357,"user_tz":300,"elapsed":4953997,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"outputId":"054323fe-bf9b-4b9e-c85f-cfbbf6c10f28"},"source":["TRAIN_LOADER_PATH = PATH + '/model/train_loader'\n","VALID_LOAER_PATH = PATH + '/model/valid_loader'\n","TEST_LOADER_PATH = PATH + '/model/test_loader'\n","\n","dataset = dataset_normalize(read_data())\n","segments, classes = dataset_segmentation(dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Start reading data ...\n","Finish reading data ...\n","Start segmentation with window size:  128\n","Finish segmentation ...\n"]}]},{"cell_type":"code","source":["np.random.seed(SEED)\n","segments, classes = shuffle(segments, classes)\n","test_x = segments[range(int(len(segments)*0.8), len(segments))]\n","test_y = classes[range(int(len(classes)*0.8), len(classes))]\n","total_x = segments[range(0, int(len(segments)*0.8))]\n","total_y = classes[range(0, int(len(classes)*0.8))]\n","print(len(test_x))\n","print(len(test_y))\n","print(len(total_x))\n","print(len(total_y))\n","\n","cross_valid_range = 5\n","\n","for k in range(cross_valid_range):\n","  print(\"Start spliting for k = \" + str(k))\n","  trainloader, validloader, testloader = train_valid_test_split(total_x, total_y, test_x, test_y, k)\n","  CROSS_TRAIN_LOADER_PATH = TRAIN_LOADER_PATH + str(k) + '.pkl'\n","  CROSS_VALID_LOADER_PATH = VALID_LOAER_PATH + str(k) + '.pkl'\n","  CROSS_TEST_LOADER_PATH = TEST_LOADER_PATH + str(k) + '.pkl'\n","  torch.save(trainloader, CROSS_TRAIN_LOADER_PATH)\n","  torch.save(validloader, CROSS_VALID_LOADER_PATH)\n","  torch.save(testloader, CROSS_TEST_LOADER_PATH)\n","  print(\"Finish data loading...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hqg4hp82-Q3Y","executionInfo":{"status":"ok","timestamp":1639658773604,"user_tz":300,"elapsed":118401,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"outputId":"488c6b56-87d0-43a3-c7a7-a131c284d532"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6072\n","6072\n","24284\n","24284\n","Start spliting for k = 0\n","Start dataset split... \n","19428\n","4856\n","6072\n","Finish dataset split... \n","Finish data loading...\n","Start spliting for k = 1\n","Start dataset split... \n","19427\n","4857\n","6072\n","Finish dataset split... \n","Finish data loading...\n","Start spliting for k = 2\n","Start dataset split... \n","19427\n","4857\n","6072\n","Finish dataset split... \n","Finish data loading...\n","Start spliting for k = 3\n","Start dataset split... \n","19427\n","4857\n","6072\n","Finish dataset split... \n","Finish data loading...\n","Start spliting for k = 4\n","Start dataset split... \n","19427\n","4857\n","6072\n","Finish dataset split... \n","Finish data loading...\n"]}]},{"cell_type":"markdown","source":["# Load CNN Model and Other Helper Functions"],"metadata":{"id":"1EbXHbJ7-ae4"}},{"cell_type":"code","source":["NODE_SIZE = 128\n","KERNAL_SIZE = 10\n","LEARNING_RATE = 0.0001\n","\n","k = 0\n","\n","TRAIN_LOADER_PATH = PATH + '/model/train_loader'\n","VALID_LOAER_PATH = PATH + '/model/valid_loader'\n","TEST_LOADER_PATH = PATH + '/model/test_loader'\n","CROSS_TRAIN_LOADER_PATH = TRAIN_LOADER_PATH + str(k) + '.pkl'\n","CROSS_VALID_LOADER_PATH = VALID_LOAER_PATH + str(k) + '.pkl'\n","CROSS_TEST_LOADER_PATH = TEST_LOADER_PATH + str(k) + '.pkl'\n","trainloader = torch.load(CROSS_TRAIN_LOADER_PATH)\n","validloader = torch.load(CROSS_VALID_LOADER_PATH)\n","testloader = torch.load(CROSS_TEST_LOADER_PATH)\n","\n","class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","\n","    # Convolutional Layers\n","    self.features = nn.Sequential(\n","      nn.Conv1d(FEATURE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","    )\n","  \n","    self.fc1 = nn.Linear(NODE_SIZE*(WINDOW_SIZE-5*(KERNAL_SIZE-1)), 100)\n","    self.fc2 = nn.Linear(100, LABEL_SIZE)\n","    self.max = nn.Softmax(dim=1)\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = x.view(x.shape[0], -1)\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)\n","    x = self.max(x)\n","    return x\n","\n","def train_save_CNN_model(TYPE, EPOCH_SIZE):\n","  # manually set random seed\n","  torch.backends.cudnn.deterministic = True\n","  torch.manual_seed(SEED)\n","\n","  # set gpu device\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  net = CNN().double().to(device)\n","\n","  # pick the criterion and optimizer\n","  criterion = nn.MultiLabelSoftMarginLoss()\n","  optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","\n","  print(\"Learning rate %.4f, batch size %d, node size %d, kernal size %d\" % (LEARNING_RATE, BATCH_SIZE, NODE_SIZE, KERNAL_SIZE))\n","\n","  # initialization\n","  train_acc_list = []\n","  val_acc_list = []\n","  test_acc_list = []\n","  accuray = 0\n","\n","  # start to train with epoches\n","  for epoch in range(EPOCH_SIZE):\n","    running_loss = 0.0\n","    train_total = 0\n","    train_correct = 0\n","    valid_total = 0\n","    valid_correct = 0\n","    test_total = 0\n","    test_correct = 0\n","\n","    # for the training dataset\n","    for i, data in enumerate(trainloader, 0):\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      optimizer.zero_grad()\n","      outputs = net(inputs)\n","      train_total += labels.size(0)\n","      train_correct += (torch.max(outputs, 1)[1] == torch.max(labels, 1)[1]).sum().item()\n","      loss = criterion(outputs, labels)\n","      if TYPE == 'l0_norm':\n","        # add group lasso regularization\n","        lgl = 1e-10\n","        cnt = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            cnt = cnt + param.detach().nonzero().size(0)\n","            #cnt = cnt + len(param.detach()[param.detach() > 1e-2]) + len(param.detach()[param.detach() < -1e-2])\n","        loss = loss + lgl * cnt\n","      elif TYPE == 'l1_norm':\n","        # add group lasso regularization\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            regularization = regularization + torch.norm(param, 1)\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'l2_norm':\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            regularization = regularization + torch.norm(param)\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'group_lasso':\n","        # add group lasso regularization\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              regularization = regularization + torch.norm(param[i,:,:])\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'l1_group_lasso':\n","        lgl = 0.000001\n","        alpha = 0.90\n","        group_lasso_regularization = torch.tensor([0]).cuda(0)\n","        lasso_regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              group_lasso_regularization = group_lasso_regularization + torch.norm(param[i,:,:])\n","            lasso_regularization = lasso_regularization + torch.norm(param, 1)\n","        loss = loss + (1-alpha) * lgl * group_lasso_regularization + alpha * lgl * lasso_regularization\n","      elif TYPE == 'l0_group_lasso':\n","        l0 = 1e-8\n","        lg = 0.4*1e-4\n","        cnt = torch.tensor([0]).cuda(0)\n","        group_lasso_regularization = torch.tensor([0]).cuda(0)\n","        lasso_regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              group_lasso_regularization = group_lasso_regularization + torch.norm(param[i,:,:])\n","            cnt += param.detach().nonzero().size(0)\n","        loss = loss + lg * group_lasso_regularization + l0 * cnt\n","      loss.backward()\n","      optimizer.step()\n","      running_loss += loss.item()\n","\n","    # for the validation dataset\n","    for data in validloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      valid_total += labels.size(0)\n","      valid_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    \n","    # for the test dataset\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    \n","    # obtain the results for training, validation, test dataset\n","    train_acc = 100 * train_correct / train_total\n","    valid_acc = 100 * valid_correct / valid_total\n","    test_acc = 100 * test_correct / test_total\n","    train_acc_list.append(train_acc)\n","    val_acc_list.append(valid_acc)\n","    test_acc_list.append(test_acc)\n","    print(\"epoch %d, loss %.3f, train acc %.2f%%, valid acc %.2f%%, test acc %.2f%%\" % (epoch+1, running_loss, train_acc, valid_acc, test_acc))\n","    \n","    # save the best model\n","    if valid_acc > accuray:\n","      accuray = valid_acc\n","      torch.save(net, PATH + '/model/' + TYPE + str(k) + \".ptl\")\n","      torch.jit.save(torch.jit.script(net), PATH + '/model/' + TYPE + str(k) + \"_git.ptl\")\n","    \n","  return train_acc_list, val_acc_list, test_acc_list"],"metadata":{"id":"JavmnjK--huZ","executionInfo":{"status":"ok","timestamp":1639871948315,"user_tz":300,"elapsed":26622,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Results for CNN Model"],"metadata":{"id":"czSNT82R-ulP"}},{"cell_type":"code","source":["TYPE = 'no_penalty'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"_3c63xqyBD7n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l0_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"-uPhfeDeispe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l1_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"Zuipx8Leis0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l2_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"z4divxnQis-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"iMsQoLw_izl-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l1_group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"metadata":{"id":"ZgFRk0pAi1C-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TYPE = 'l0_group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)\n","print(train_acc)\n","print(valid_acc)\n","print(test_acc)\n","file_name = 'data'+str(k)+'.txt'\n","with open(file_name, 'w') as f:\n","    for i in train_acc:\n","      f.write(\"%f \" % i)\n","    f.write('\\n')\n","    for i in valid_acc:\n","      f.write(\"%f \" % i)\n","    f.write('\\n')\n","    for i in test_acc:\n","      f.write(\"%f \" % i)\n","    f.write('\\n')"],"metadata":{"id":"I37ScZ1Y-477"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results after Pruning the above Models"],"metadata":{"id":"HN48kt-K2mpC"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = 0.01\n","\n","class ThresholdPruning(prune.BasePruningMethod):\n","    PRUNING_TYPE = \"unstructured\"\n","\n","    def __init__(self, threshold):\n","        self.threshold = threshold\n","\n","    def compute_mask(self, tensor, default_mask):\n","      return torch.abs(tensor) > self.threshold\n","\n","PATHS = { 'No penalty - 0 128 0.0001':     PATH + '/model/final/lr0.0001/no_penalty0.ptl',\n","      'l0 norm - 0 128 0.0001':       PATH + '/model/final/lr0.0001/l0_norm0.ptl',\n","      'l1 norm - 0 128 0.0001':       PATH + '/model/final/lr0.0001/l1_norm0.ptl',\n","      'l2 norm - 0 128 0.0001':       PATH + '/model/final/lr0.0001/l2_norm0.ptl',\n","      'group lasso - 0 128 0.0001':     PATH + '/model/final/lr0.0001/group_lasso0.ptl',\n","      'l1 group lasso - 0 128 0.0001':   PATH + '/model/final/lr0.0001/l1_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.0001 (*)': PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.00005':   PATH + '/model/final/lr0.00005/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.00015':   PATH + '/model/final/lr0.00015/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.0002':   PATH + '/model/final/lr0.0002/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 128 0.00001':   PATH + '/model/final/lr0.00001/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 256 0.0001':   PATH + '/model/final/256node/l0_group_lasso0.ptl',\n","      'l0 group lasso - 0 64 0.0001':   PATH + '/model/final/64node/l0_group_lasso0.ptl',\n","      'l0 group lasso - 1 128 0.0001':   PATH + '/model/final/lr0.0001/l0_group_lasso1.ptl',\n","      'l0 group lasso - 2 128 0.0001':   PATH + '/model/final/lr0.0001/l0_group_lasso2.ptl',\n","      'l0 group lasso - 3 128 0.0001':   PATH + '/model/final/lr0.0001/l0_group_lasso3.ptl',\n","      'l0 group lasso - 4 128 0.0001':   PATH + '/model/final/lr0.0001/l0_group_lasso4.ptl',\n","    }\n","\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net = torch.load(PATHS[name])\n","\n","  # display the results before compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% before compression' % (test_total, test_acc))\n","\n","  # prune the model\n","  parameters_to_prune = []\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      parameters_to_prune.append((child, \"weight\"))\n","  prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold=PRUNE_THRESHOLD)\n","\n","  # calculate the sparsity\n","  total_weight = 0\n","  total_nonzero = 0\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      total_weight += torch.numel(child.weight)\n","      total_nonzero += torch.count_nonzero(child.weight)\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_nonzero / total_weight)))\n","\n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"64evT_NP2sdg","executionInfo":{"status":"ok","timestamp":1639871992315,"user_tz":300,"elapsed":44027,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"outputId":"0903c542-d292-46f8-bb2e-7c3e3d896534"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for No penalty - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 93.15 % before compression\n","Sparity for the compressed model: 69.27 %\n","Accuracy of the network on the 6072 test data: 93.07 % after compression\n","\n","Here are the results for l0 norm - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 93.15 % before compression\n","Sparity for the compressed model: 69.27 %\n","Accuracy of the network on the 6072 test data: 93.07 % after compression\n","\n","Here are the results for l1 norm - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 95.22 % before compression\n","Sparity for the compressed model: 1.46 %\n","Accuracy of the network on the 6072 test data: 95.29 % after compression\n","\n","Here are the results for l2 norm - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 92.08 % before compression\n","Sparity for the compressed model: 65.32 %\n","Accuracy of the network on the 6072 test data: 92.09 % after compression\n","\n","Here are the results for group lasso - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 93.30 % before compression\n","Sparity for the compressed model: 61.78 %\n","Accuracy of the network on the 6072 test data: 93.28 % after compression\n","\n","Here are the results for l1 group lasso - 0 128 0.0001:\n","Accuracy of the network on the 6072 test data: 96.87 % before compression\n","Sparity for the compressed model: 2.67 %\n","Accuracy of the network on the 6072 test data: 97.20 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.0001 (*):\n","Accuracy of the network on the 6072 test data: 96.89 % before compression\n","Sparity for the compressed model: 1.26 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.00005:\n","Accuracy of the network on the 6072 test data: 94.25 % before compression\n","Sparity for the compressed model: 3.90 %\n","Accuracy of the network on the 6072 test data: 93.89 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.00015:\n","Accuracy of the network on the 6072 test data: 96.57 % before compression\n","Sparity for the compressed model: 1.12 %\n","Accuracy of the network on the 6072 test data: 96.62 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.0002:\n","Accuracy of the network on the 6072 test data: 94.89 % before compression\n","Sparity for the compressed model: 0.68 %\n","Accuracy of the network on the 6072 test data: 94.99 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.00001:\n","Accuracy of the network on the 6072 test data: 93.63 % before compression\n","Sparity for the compressed model: 7.93 %\n","Accuracy of the network on the 6072 test data: 85.80 % after compression\n","\n","Here are the results for l0 group lasso - 0 256 0.0001:\n","Accuracy of the network on the 6072 test data: 94.35 % before compression\n","Sparity for the compressed model: 0.46 %\n","Accuracy of the network on the 6072 test data: 93.87 % after compression\n","\n","Here are the results for l0 group lasso - 0 64 0.0001:\n","Accuracy of the network on the 6072 test data: 96.15 % before compression\n","Sparity for the compressed model: 6.00 %\n","Accuracy of the network on the 6072 test data: 96.01 % after compression\n","\n","Here are the results for l0 group lasso - 1 128 0.0001:\n","Accuracy of the network on the 6072 test data: 92.29 % before compression\n","Sparity for the compressed model: 1.27 %\n","Accuracy of the network on the 6072 test data: 92.28 % after compression\n","\n","Here are the results for l0 group lasso - 2 128 0.0001:\n","Accuracy of the network on the 6072 test data: 96.49 % before compression\n","Sparity for the compressed model: 1.81 %\n","Accuracy of the network on the 6072 test data: 96.28 % after compression\n","\n","Here are the results for l0 group lasso - 3 128 0.0001:\n","Accuracy of the network on the 6072 test data: 95.08 % before compression\n","Sparity for the compressed model: 1.20 %\n","Accuracy of the network on the 6072 test data: 94.99 % after compression\n","\n","Here are the results for l0 group lasso - 4 128 0.0001:\n","Accuracy of the network on the 6072 test data: 94.81 % before compression\n","Sparity for the compressed model: 1.46 %\n","Accuracy of the network on the 6072 test data: 94.81 % after compression\n","\n"]}]},{"cell_type":"markdown","source":["# Threshold check"],"metadata":{"id":"UtWDze4tBGIh"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = np.arange(0.005, 0.03, 0.005)\n","\n","class ThresholdPruning(prune.BasePruningMethod):\n","    PRUNING_TYPE = \"unstructured\"\n","\n","    def __init__(self, threshold):\n","        self.threshold = threshold\n","\n","    def compute_mask(self, tensor, default_mask):\n","      return torch.abs(tensor) > self.threshold\n","\n","PATHS = PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl'\n","\n","for threshold in PRUNE_THRESHOLD:\n","  print('Here are the results for threshold {}:'.format(threshold))\n","  # load the model\n","  net = torch.load(PATHS)\n","\n","  # display the results before compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% before compression' % (test_total, test_acc))\n","\n","  # prune the model\n","  parameters_to_prune = []\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      parameters_to_prune.append((child, \"weight\"))\n","  prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold=threshold)\n","\n","  # calculate the sparsity\n","  total_weight = 0\n","  total_nonzero = 0\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      total_weight += torch.numel(child.weight)\n","      total_nonzero += torch.count_nonzero(child.weight)\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_nonzero / total_weight)))\n","\n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSTCI4oMBF3G","executionInfo":{"status":"ok","timestamp":1639840406501,"user_tz":300,"elapsed":7337,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18198364614828690729"}},"outputId":"7ec62ea1-f672-4810-e53d-f0d6c11c655d"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for threshold 0.005:\n","Accuracy of the network on the 6072 test data: 96.89 % before compression\n","Sparity for the compressed model: 1.81 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for threshold 0.01:\n","Accuracy of the network on the 6072 test data: 96.89 % before compression\n","Sparity for the compressed model: 1.26 %\n","Accuracy of the network on the 6072 test data: 96.95 % after compression\n","\n","Here are the results for threshold 0.015:\n","Accuracy of the network on the 6072 test data: 96.89 % before compression\n","Sparity for the compressed model: 0.87 %\n","Accuracy of the network on the 6072 test data: 96.72 % after compression\n","\n","Here are the results for threshold 0.02:\n","Accuracy of the network on the 6072 test data: 96.89 % before compression\n","Sparity for the compressed model: 0.61 %\n","Accuracy of the network on the 6072 test data: 96.39 % after compression\n","\n","Here are the results for threshold 0.025:\n","Accuracy of the network on the 6072 test data: 96.89 % before compression\n","Sparity for the compressed model: 0.45 %\n","Accuracy of the network on the 6072 test data: 94.58 % after compression\n","\n"]}]}]}