{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wisdm_dataset.ipynb","provenance":[],"collapsed_sections":["0PNtCeGOFRWG","qlB-dplZHA2T","vWcs68OG2AiN"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8c3DHKg8D94g"},"source":["## Connect Google Drive and GPU\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EL60RJZDguh","executionInfo":{"status":"ok","timestamp":1639868653408,"user_tz":300,"elapsed":19059,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"338394e2-7c74-41f4-d889-c88f1d637c0c"},"source":["%reset\n","\n","# connect google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# connect colab gpu\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n","Mounted at /content/drive\n","Sat Dec 18 23:04:12 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    32W / 250W |   1275MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"0PNtCeGOFRWG"},"source":["## Import Needed Libraries, Paramaters and Functions"]},{"cell_type":"code","source":["import sys\n","import time\n","import os.path\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.utils import prune\n","import torchvision\n","import matplotlib.pyplot as plt\n","from torch.utils.mobile_optimizer import optimize_for_mobile\n","from scipy import stats\n","from sklearn.utils import shuffle\n","\n","SEED = 10\n","WINDOW_SIZE = 80\n","FEATURE_SIZE = 3\n","LABEL_SIZE = 6\n","BATCH_SIZE = 32\n","PATH = '/content/drive/MyDrive/CNNPaper'\n","RAW_DATA_PATH = PATH + '/data/WISDM/WISDM_ar_v1.1_raw.txt'"],"metadata":{"id":"pdrc0WqPjqpx","executionInfo":{"status":"ok","timestamp":1639868656689,"user_tz":300,"elapsed":853,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9Z3s5y5FO_n","executionInfo":{"status":"ok","timestamp":1639868658514,"user_tz":300,"elapsed":2,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}}},"source":["def read_data(file_path):\n","  \"\"\"\n","    Read data from file_path\n","    Paramater:\n","      file_path: str\n","    Return:\n","      a DataFrame with the data and labels \n","  \"\"\"\n","  print(\"Start reading data ...\")\n","  column_names = ['user', 'activity', 'timestamp', 'x-accel', 'y-accel', 'z-accel']\n","  data = pd.read_csv(file_path, header=None, names=column_names)\n","  print(\"Finish reading data ...\")\n","  return data\n","\n","def feature_normalize(data):\n","  \"\"\"\n","    Normalize the feature data\n","    Paramater:\n","      data: a list of floats\n","    Return:\n","      a list of floats with normalized data\n","  \"\"\"\n","  mu = np.mean(data, axis=0)\n","  sigma = np.std(data, axis=0)\n","  return (data - mu) / sigma\n","\n","def dataset_normalize(dataset):\n","  \"\"\"\n","    Normalize the whole dataset\n","    Paramater:\n","      dataset: a DataFrame with the data and labels \n","    Return:\n","      a DataFrame with the normalized data and labels \n","  \"\"\"\n","  dataset.dropna(axis=0, how='any', inplace=True)\n","  print(\"Normalizing x-accel ...\")\n","  dataset['x-accel'] = feature_normalize(dataset['x-accel'])\n","  print(\"Normalizing y-accel ...\")\n","  dataset['y-accel'] = feature_normalize(dataset['y-accel'])\n","  print(\"Normalizing z-accel ...\")\n","  dataset['z-accel'] = feature_normalize(dataset['z-accel'])\n","  return dataset\n","\n","def windows(data, size):\n","  \"\"\"\n","    Obatin the starting index and ending index according to window size\n","    Paramater:\n","      data: a list of floats\n","      size: int\n","    Return:\n","      Starting index, ending index\n","  \"\"\"\n","  start = 0\n","  while start < data.count():\n","    yield int(start), int(start + size)\n","    start += (size / 2)\n","      \n","def dataset_segmentation(data):\n","  \"\"\"\n","    Dataset segmentation according the window size\n","    Paramater:\n","      data: a list of floats\n","    Return:\n","      segments and labels \n","  \"\"\"\n","  print(\"Start segmentation with window size: \", WINDOW_SIZE)\n","  segments = np.empty((0, WINDOW_SIZE, FEATURE_SIZE))\n","  labels = np.empty((0))\n","  for (start, end) in windows(data['timestamp'], WINDOW_SIZE):\n","      x = data[\"x-accel\"][start:end]\n","      y = data[\"y-accel\"][start:end]\n","      z = data[\"z-accel\"][start:end]\n","      if len(data[\"timestamp\"][start:end]) == WINDOW_SIZE:\n","        segments = np.vstack([segments, np.dstack([x,y,z])])\n","        labels = np.append(labels, stats.mode(data[\"activity\"][start:end])[0][0])\n","  labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n","  segments = segments.reshape(len(segments), FEATURE_SIZE, WINDOW_SIZE)\n","  print(\"Finish segmentation ...\")\n","  return segments, labels\n","\n","def train_valid_test_split(segments, classes, test_x, test_y, k_fold):\n","  \"\"\"\n","    Split train, valid and test datase\n","    Paramater:\n","      segments: a list of input data\n","      classes: a list of classes data\n","      k: k fold cross validation\n","    Return:\n","      segments and labels \n","  \"\"\"\n","  print(\"Start dataset split... \")\n","  seg_len = len(segments)\n","  idx_val = [0, int(seg_len/5*1), int(seg_len/5*2), int(seg_len/5*3), int(seg_len/5*4), seg_len]\n","  train_range1 = range(0, idx_val[k_fold])\n","  valid_range = range(idx_val[k_fold], idx_val[k_fold+1])\n","  train_range2 = range(idx_val[k_fold+1], seg_len)\n","\n","  train_x = np.concatenate((segments[train_range1], segments[train_range2]), axis=0)\n","  train_y = np.concatenate((classes[train_range1], classes[train_range2]), axis=0)\n","  valid_x = segments[valid_range]\n","  valid_y = classes[valid_range]\n","\n","  # get train data\n","  train_data = []\n","  for i in range(len(train_x)):\n","    train_data.append([train_x[i], train_y[i]])\n","  \n","  # get valid data\n","  valid_data = []\n","  for i in range(len(valid_x)):\n","    valid_data.append([valid_x[i], valid_y[i]])\n","  \n","  # get test data\n","  test_data = []\n","  for i in range(len(test_x)):\n","    test_data.append([test_x[i], test_y[i]])\n","  print(len(train_data))\n","  print(len(valid_data))\n","  print(len(test_data))\n","\n","  # generate DataLoader for each dataset\n","  trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n","  validloader = torch.utils.data.DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE)\n","  testloader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)\n","  \n","  print(\"Finish dataset split... \")\n","  return trainloader, validloader, testloader"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qlB-dplZHA2T"},"source":["## Load and Save Train, Test, Valid Dataset"]},{"cell_type":"code","metadata":{"id":"bQjBsq3KHCPW"},"source":["TRAIN_LOADER_PATH = PATH + '/model/train_loader'\n","VALID_LOAER_PATH = PATH + '/model/valid_loader'\n","TEST_LOADER_PATH = PATH + '/model/test_loader'\n","\n","dataset = dataset_normalize(read_data(RAW_DATA_PATH))\n","segments, classes = dataset_segmentation(dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.random.seed(SEED)\n","segments, classes = shuffle(segments, classes)\n","test_x = segments[range(int(len(segments)*0.8), len(segments))]\n","test_y = classes[range(int(len(classes)*0.8), len(classes))]\n","total_x = segments[range(0, int(len(segments)*0.8))]\n","total_y = classes[range(0, int(len(classes)*0.8))]\n","print(len(test_x))\n","print(len(test_y))\n","print(len(total_x))\n","print(len(total_y))\n","\n","cross_valid_range = 5\n","\n","for k in range(cross_valid_range):\n","  print(\"Start spliting for k = \" + str(k))\n","  trainloader, validloader, testloader = train_valid_test_split(total_x, total_y, test_x, test_y, k)\n","  CROSS_TRAIN_LOADER_PATH = TRAIN_LOADER_PATH + str(k) + '.pkl'\n","  CROSS_VALID_LOADER_PATH = VALID_LOAER_PATH + str(k) + '.pkl'\n","  CROSS_TEST_LOADER_PATH = TEST_LOADER_PATH + str(k) + '.pkl'\n","  torch.save(trainloader, CROSS_TRAIN_LOADER_PATH)\n","  torch.save(validloader, CROSS_VALID_LOADER_PATH)\n","  torch.save(testloader, CROSS_TEST_LOADER_PATH)\n","  print(\"Finish data loading...\")"],"metadata":{"id":"J07sGh-P3ncs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVWa7ZKDH5Vm"},"source":["## Load CNN Model and Other Helper Functions\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"kOhkqUzCH2QT","executionInfo":{"status":"ok","timestamp":1639868706714,"user_tz":300,"elapsed":1289,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}}},"source":["NODE_SIZE = 128\n","KERNAL_SIZE = 10\n","LEARNING_RATE = 0.0001\n","\n","k = 3\n","\n","TRAIN_LOADER_PATH = PATH + '/model/train_loader'\n","VALID_LOAER_PATH = PATH + '/model/valid_loader'\n","TEST_LOADER_PATH = PATH + '/model/test_loader'\n","CROSS_TRAIN_LOADER_PATH = TRAIN_LOADER_PATH + str(k) + '.pkl'\n","CROSS_VALID_LOADER_PATH = VALID_LOAER_PATH + str(k) + '.pkl'\n","CROSS_TEST_LOADER_PATH = TEST_LOADER_PATH + str(k) + '.pkl'\n","trainloader = torch.load(CROSS_TRAIN_LOADER_PATH)\n","validloader = torch.load(CROSS_VALID_LOADER_PATH)\n","testloader = torch.load(CROSS_TEST_LOADER_PATH)\n","\n","class CNN(nn.Module):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","\n","    # Convolutional Layers\n","    self.features = nn.Sequential(\n","      nn.Conv1d(FEATURE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","      nn.Conv1d(NODE_SIZE, NODE_SIZE, kernel_size=KERNAL_SIZE, bias=False),\n","      nn.ReLU(),\n","    )\n","  \n","    self.fc1 = nn.Linear(NODE_SIZE*(WINDOW_SIZE-5*(KERNAL_SIZE-1)), 100)\n","    self.fc2 = nn.Linear(100, LABEL_SIZE)\n","    self.max = nn.Softmax(dim=1)\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = x.view(x.shape[0], -1)\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)\n","    x = self.max(x)\n","    return x\n","\n","def train_save_CNN_model(TYPE, EPOCH_SIZE):\n","  # manually set random seed\n","  torch.backends.cudnn.deterministic = True\n","  torch.manual_seed(SEED)\n","\n","  # set gpu device\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  net = CNN().double().to(device)\n","\n","  # pick the criterion and optimizer\n","  criterion = nn.MultiLabelSoftMarginLoss()\n","  optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n","\n","  print(\"Learning rate %.5f, batch size %d, node size %d, kernal size %d\" % (LEARNING_RATE, BATCH_SIZE, NODE_SIZE, KERNAL_SIZE))\n","\n","  # initialization\n","  train_acc_list = []\n","  val_acc_list = []\n","  test_acc_list = []\n","  accuray = 0\n","\n","  # start to train with epoches\n","  for epoch in range(EPOCH_SIZE):\n","    running_loss = 0.0\n","    train_total = 0\n","    train_correct = 0\n","    valid_total = 0\n","    valid_correct = 0\n","    test_total = 0\n","    test_correct = 0\n","\n","    # for the training dataset\n","    for i, data in enumerate(trainloader, 0):\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      optimizer.zero_grad()\n","      outputs = net(inputs)\n","      train_total += labels.size(0)\n","      train_correct += (torch.max(outputs, 1)[1] == torch.max(labels, 1)[1]).sum().item()\n","      loss = criterion(outputs, labels)\n","      if TYPE == 'l0_norm':\n","        # add group lasso regularization\n","        lgl = 1e-10\n","        cnt = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            cnt = cnt + param.detach().nonzero().size(0)\n","            #cnt = cnt + len(param.detach()[param.detach() > 1e-2]) + len(param.detach()[param.detach() < -1e-2])\n","        loss = loss + lgl * cnt\n","      elif TYPE == 'l1_norm':\n","        # add group lasso regularization\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            regularization = regularization + torch.norm(param, 1)\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'l2_norm':\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            regularization = regularization + torch.norm(param)\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'group_lasso':\n","        # add group lasso regularization\n","        lgl = 0.000001\n","        regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              regularization = regularization + torch.norm(param[i,:,:])\n","        loss = loss + lgl * regularization\n","      elif TYPE == 'l1_group_lasso':\n","        lgl = 0.000001\n","        alpha = 0.5\n","        group_lasso_regularization = torch.tensor([0]).cuda(0)\n","        lasso_regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              group_lasso_regularization = group_lasso_regularization + torch.norm(param[i,:,:])\n","            lasso_regularization = lasso_regularization + torch.norm(param, 1)\n","        loss = loss + (1-alpha) * lgl * group_lasso_regularization + alpha * lgl * lasso_regularization\n","      elif TYPE == 'l0_group_lasso':\n","        l0 = 1e-8\n","        lg = 0.4*1e-4\n","        cnt = torch.tensor([0]).cuda(0)\n","        group_lasso_regularization = torch.tensor([0]).cuda(0)\n","        lasso_regularization = torch.tensor([0]).cuda(0)\n","        for name, param in net.named_parameters():\n","          if \"features\" in name:\n","            for i in range(param.shape[0]):\n","              group_lasso_regularization = group_lasso_regularization + torch.norm(param[i,:,:])\n","            cnt += param.detach().nonzero().size(0)\n","        loss = loss + lg * group_lasso_regularization + l0 * cnt\n","      loss.backward()\n","      optimizer.step()\n","      running_loss += loss.item()\n","\n","    # for the validation dataset\n","    for data in validloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      valid_total += labels.size(0)\n","      valid_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    \n","    # for the test dataset\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    \n","    # obtain the results for training, validation, test dataset\n","    train_acc = 100 * train_correct / train_total\n","    valid_acc = 100 * valid_correct / valid_total\n","    test_acc = 100 * test_correct / test_total\n","    train_acc_list.append(train_acc)\n","    val_acc_list.append(valid_acc)\n","    test_acc_list.append(test_acc)\n","    print(\"epoch %d, loss %.3f, train acc %.2f%%, valid acc %.2f%%, test acc %.2f%%\" % (epoch+1, running_loss, train_acc, valid_acc, test_acc))\n","    \n","    # save the best model\n","    if valid_acc > accuray:\n","      accuray = valid_acc\n","      torch.save(net, PATH + '/model/' + TYPE + str(k) + \".ptl\")\n","      torch.jit.save(torch.jit.script(net), PATH + '/model/' + TYPE + str(k) + \"_git.ptl\")\n","    \n","  return train_acc_list, val_acc_list, test_acc_list"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YK4kfyCCJ75D"},"source":["## Results for CNN Model"]},{"cell_type":"code","metadata":{"id":"GN9tNO78KBag","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639799059538,"user_tz":300,"elapsed":1185481,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"d9722d40-6d4e-42d6-a5fe-63807f719dc9"},"source":["TYPE = 'no_penalty'\n","EPOCH_SIZE = 100\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate 0.00010, batch size 32, node size 128, kernal size 10\n","epoch 1, loss 375.590, train acc 66.53%, valid acc 78.90%, test acc 79.07%\n","epoch 2, loss 365.215, train acc 79.55%, valid acc 80.54%, test acc 80.66%\n","epoch 3, loss 364.266, train acc 80.43%, valid acc 80.13%, test acc 80.64%\n","epoch 4, loss 362.751, train acc 82.05%, valid acc 78.85%, test acc 78.66%\n","epoch 5, loss 361.505, train acc 83.37%, valid acc 84.91%, test acc 85.30%\n","epoch 6, loss 360.190, train acc 84.77%, valid acc 86.14%, test acc 86.23%\n","epoch 7, loss 359.274, train acc 85.87%, valid acc 86.23%, test acc 86.96%\n","epoch 8, loss 358.188, train acc 87.17%, valid acc 86.55%, test acc 87.16%\n","epoch 9, loss 357.482, train acc 87.83%, valid acc 86.16%, test acc 86.89%\n","epoch 10, loss 356.654, train acc 88.75%, valid acc 88.46%, test acc 88.87%\n","epoch 11, loss 355.866, train acc 89.61%, valid acc 88.73%, test acc 89.75%\n","epoch 12, loss 355.320, train acc 90.13%, valid acc 89.39%, test acc 89.60%\n","epoch 13, loss 354.394, train acc 91.26%, valid acc 89.96%, test acc 90.27%\n","epoch 14, loss 354.306, train acc 91.39%, valid acc 90.19%, test acc 90.89%\n","epoch 15, loss 353.723, train acc 92.05%, valid acc 90.26%, test acc 90.93%\n","epoch 16, loss 353.932, train acc 91.91%, valid acc 90.35%, test acc 90.64%\n","epoch 17, loss 353.090, train acc 92.78%, valid acc 88.16%, test acc 89.07%\n","epoch 18, loss 352.966, train acc 92.81%, valid acc 90.87%, test acc 91.00%\n","epoch 19, loss 352.360, train acc 93.50%, valid acc 91.17%, test acc 91.68%\n","epoch 20, loss 352.431, train acc 93.43%, valid acc 91.74%, test acc 92.21%\n","epoch 21, loss 352.068, train acc 93.85%, valid acc 91.94%, test acc 92.90%\n","epoch 22, loss 351.711, train acc 94.22%, valid acc 91.53%, test acc 91.82%\n","epoch 23, loss 351.423, train acc 94.62%, valid acc 91.40%, test acc 92.06%\n","epoch 24, loss 351.353, train acc 94.57%, valid acc 91.83%, test acc 92.10%\n","epoch 25, loss 351.131, train acc 94.83%, valid acc 92.10%, test acc 92.62%\n","epoch 26, loss 351.248, train acc 94.70%, valid acc 92.51%, test acc 92.46%\n","epoch 27, loss 351.096, train acc 94.95%, valid acc 92.78%, test acc 93.44%\n","epoch 28, loss 350.554, train acc 95.52%, valid acc 92.58%, test acc 93.24%\n","epoch 29, loss 350.421, train acc 95.62%, valid acc 92.33%, test acc 93.21%\n","epoch 30, loss 350.341, train acc 95.70%, valid acc 90.28%, test acc 91.84%\n","epoch 31, loss 350.259, train acc 95.81%, valid acc 92.53%, test acc 93.35%\n","epoch 32, loss 349.904, train acc 96.26%, valid acc 91.76%, test acc 92.64%\n","epoch 33, loss 350.107, train acc 95.96%, valid acc 92.35%, test acc 93.68%\n","epoch 34, loss 349.856, train acc 96.22%, valid acc 92.81%, test acc 93.04%\n","epoch 35, loss 349.653, train acc 96.48%, valid acc 91.49%, test acc 92.48%\n","epoch 36, loss 349.518, train acc 96.62%, valid acc 92.92%, test acc 93.99%\n","epoch 37, loss 349.812, train acc 96.31%, valid acc 93.26%, test acc 93.61%\n","epoch 38, loss 349.495, train acc 96.64%, valid acc 93.10%, test acc 93.75%\n","epoch 39, loss 349.228, train acc 96.94%, valid acc 92.90%, test acc 93.57%\n","epoch 40, loss 349.528, train acc 96.59%, valid acc 92.44%, test acc 93.24%\n","epoch 41, loss 349.427, train acc 96.70%, valid acc 92.06%, test acc 92.68%\n","epoch 42, loss 349.415, train acc 96.73%, valid acc 93.08%, test acc 93.30%\n","epoch 43, loss 349.292, train acc 96.84%, valid acc 92.72%, test acc 93.57%\n","epoch 44, loss 349.213, train acc 96.94%, valid acc 93.31%, test acc 94.32%\n","epoch 45, loss 348.876, train acc 97.31%, valid acc 92.51%, test acc 93.97%\n","epoch 46, loss 349.223, train acc 96.91%, valid acc 93.10%, test acc 94.19%\n","epoch 47, loss 349.044, train acc 97.11%, valid acc 93.88%, test acc 94.35%\n","epoch 48, loss 349.036, train acc 97.13%, valid acc 92.56%, test acc 93.61%\n","epoch 49, loss 348.882, train acc 97.30%, valid acc 93.99%, test acc 94.76%\n","epoch 50, loss 348.919, train acc 97.25%, valid acc 93.72%, test acc 94.26%\n","epoch 51, loss 349.098, train acc 97.07%, valid acc 92.76%, test acc 93.06%\n","epoch 52, loss 348.811, train acc 97.40%, valid acc 93.22%, test acc 94.03%\n","epoch 53, loss 348.930, train acc 97.23%, valid acc 93.69%, test acc 94.28%\n","epoch 54, loss 348.800, train acc 97.39%, valid acc 93.63%, test acc 94.79%\n","epoch 55, loss 348.713, train acc 97.50%, valid acc 93.13%, test acc 93.70%\n","epoch 56, loss 349.046, train acc 97.26%, valid acc 91.69%, test acc 92.79%\n","epoch 57, loss 348.706, train acc 97.51%, valid acc 93.69%, test acc 94.45%\n","epoch 58, loss 348.787, train acc 97.39%, valid acc 93.94%, test acc 94.57%\n","epoch 59, loss 348.375, train acc 97.86%, valid acc 94.06%, test acc 94.03%\n","epoch 60, loss 348.628, train acc 97.57%, valid acc 93.44%, test acc 93.84%\n","epoch 61, loss 348.635, train acc 97.55%, valid acc 92.74%, test acc 93.19%\n","epoch 62, loss 348.795, train acc 97.40%, valid acc 93.06%, test acc 94.10%\n","epoch 63, loss 348.437, train acc 97.77%, valid acc 93.76%, test acc 94.83%\n","epoch 64, loss 348.446, train acc 97.79%, valid acc 93.33%, test acc 94.04%\n","epoch 65, loss 348.500, train acc 97.72%, valid acc 93.42%, test acc 93.68%\n","epoch 66, loss 348.627, train acc 97.56%, valid acc 92.83%, test acc 93.41%\n","epoch 67, loss 348.718, train acc 97.48%, valid acc 93.03%, test acc 94.15%\n","epoch 68, loss 348.417, train acc 97.79%, valid acc 93.13%, test acc 93.88%\n","epoch 69, loss 348.206, train acc 98.04%, valid acc 94.17%, test acc 94.70%\n","epoch 70, loss 348.807, train acc 97.38%, valid acc 94.04%, test acc 94.10%\n","epoch 71, loss 348.722, train acc 97.48%, valid acc 91.71%, test acc 92.84%\n","epoch 72, loss 348.299, train acc 97.91%, valid acc 94.04%, test acc 94.81%\n","epoch 73, loss 348.427, train acc 97.80%, valid acc 93.99%, test acc 94.66%\n","epoch 74, loss 348.110, train acc 98.12%, valid acc 93.81%, test acc 94.34%\n","epoch 75, loss 348.032, train acc 98.25%, valid acc 94.15%, test acc 94.32%\n","epoch 76, loss 348.443, train acc 97.77%, valid acc 93.81%, test acc 93.72%\n","epoch 77, loss 348.035, train acc 98.24%, valid acc 94.61%, test acc 94.79%\n","epoch 78, loss 348.187, train acc 98.05%, valid acc 93.33%, test acc 93.77%\n","epoch 79, loss 348.310, train acc 97.93%, valid acc 94.35%, test acc 94.46%\n","epoch 80, loss 348.043, train acc 98.20%, valid acc 94.45%, test acc 94.46%\n","epoch 81, loss 348.075, train acc 98.16%, valid acc 94.17%, test acc 94.32%\n","epoch 82, loss 348.353, train acc 97.85%, valid acc 94.20%, test acc 94.68%\n","epoch 83, loss 348.463, train acc 97.75%, valid acc 93.40%, test acc 94.19%\n","epoch 84, loss 348.365, train acc 97.84%, valid acc 94.17%, test acc 94.35%\n","epoch 85, loss 348.038, train acc 98.22%, valid acc 94.54%, test acc 94.54%\n","epoch 86, loss 347.915, train acc 98.35%, valid acc 94.40%, test acc 94.74%\n","epoch 87, loss 348.086, train acc 98.18%, valid acc 94.49%, test acc 94.70%\n","epoch 88, loss 347.948, train acc 98.32%, valid acc 94.51%, test acc 94.85%\n","epoch 89, loss 347.995, train acc 98.25%, valid acc 94.08%, test acc 94.30%\n","epoch 90, loss 348.091, train acc 98.16%, valid acc 94.24%, test acc 94.35%\n","epoch 91, loss 348.178, train acc 98.06%, valid acc 93.54%, test acc 94.32%\n","epoch 92, loss 348.266, train acc 97.96%, valid acc 94.17%, test acc 94.86%\n","epoch 93, loss 347.955, train acc 98.30%, valid acc 94.26%, test acc 94.52%\n","epoch 94, loss 347.906, train acc 98.37%, valid acc 93.51%, test acc 94.34%\n","epoch 95, loss 348.101, train acc 98.17%, valid acc 94.13%, test acc 94.50%\n","epoch 96, loss 348.113, train acc 98.12%, valid acc 93.88%, test acc 93.39%\n","epoch 97, loss 348.358, train acc 97.84%, valid acc 94.17%, test acc 94.50%\n","epoch 98, loss 348.020, train acc 98.22%, valid acc 94.35%, test acc 94.92%\n","epoch 99, loss 348.156, train acc 98.09%, valid acc 93.69%, test acc 94.32%\n","epoch 100, loss 347.914, train acc 98.34%, valid acc 94.35%, test acc 94.61%\n"]}]},{"cell_type":"code","metadata":{"id":"Pqyj-yaDYlGv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639800859328,"user_tz":300,"elapsed":1799793,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"7e4a2c3d-d8c9-4ead-87de-7083aa11bb4e"},"source":["TYPE = 'l0_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate 0.00010, batch size 32, node size 128, kernal size 10\n","epoch 1, loss 375.626, train acc 66.53%, valid acc 78.90%, test acc 79.07%\n","epoch 2, loss 365.251, train acc 79.55%, valid acc 80.54%, test acc 80.66%\n","epoch 3, loss 364.303, train acc 80.43%, valid acc 80.13%, test acc 80.64%\n","epoch 4, loss 362.787, train acc 82.05%, valid acc 78.85%, test acc 78.66%\n","epoch 5, loss 361.541, train acc 83.37%, valid acc 84.91%, test acc 85.30%\n","epoch 6, loss 360.226, train acc 84.77%, valid acc 86.14%, test acc 86.23%\n","epoch 7, loss 359.311, train acc 85.87%, valid acc 86.23%, test acc 86.96%\n","epoch 8, loss 358.225, train acc 87.17%, valid acc 86.55%, test acc 87.16%\n","epoch 9, loss 357.519, train acc 87.83%, valid acc 86.16%, test acc 86.89%\n","epoch 10, loss 356.691, train acc 88.75%, valid acc 88.46%, test acc 88.87%\n","epoch 11, loss 355.903, train acc 89.61%, valid acc 88.73%, test acc 89.75%\n","epoch 12, loss 355.357, train acc 90.13%, valid acc 89.39%, test acc 89.60%\n","epoch 13, loss 354.430, train acc 91.26%, valid acc 89.96%, test acc 90.27%\n","epoch 14, loss 354.342, train acc 91.39%, valid acc 90.19%, test acc 90.89%\n","epoch 15, loss 353.759, train acc 92.05%, valid acc 90.26%, test acc 90.93%\n","epoch 16, loss 353.968, train acc 91.91%, valid acc 90.35%, test acc 90.64%\n","epoch 17, loss 353.126, train acc 92.78%, valid acc 88.16%, test acc 89.07%\n","epoch 18, loss 353.002, train acc 92.81%, valid acc 90.87%, test acc 91.00%\n","epoch 19, loss 352.396, train acc 93.50%, valid acc 91.17%, test acc 91.68%\n","epoch 20, loss 352.467, train acc 93.43%, valid acc 91.74%, test acc 92.21%\n","epoch 21, loss 352.104, train acc 93.85%, valid acc 91.94%, test acc 92.90%\n","epoch 22, loss 351.747, train acc 94.22%, valid acc 91.53%, test acc 91.82%\n","epoch 23, loss 351.460, train acc 94.62%, valid acc 91.40%, test acc 92.06%\n","epoch 24, loss 351.389, train acc 94.57%, valid acc 91.83%, test acc 92.10%\n","epoch 25, loss 351.167, train acc 94.83%, valid acc 92.10%, test acc 92.62%\n","epoch 26, loss 351.284, train acc 94.70%, valid acc 92.51%, test acc 92.46%\n","epoch 27, loss 351.132, train acc 94.95%, valid acc 92.78%, test acc 93.44%\n","epoch 28, loss 350.590, train acc 95.52%, valid acc 92.58%, test acc 93.24%\n","epoch 29, loss 350.457, train acc 95.62%, valid acc 92.33%, test acc 93.21%\n","epoch 30, loss 350.377, train acc 95.70%, valid acc 90.28%, test acc 91.84%\n","epoch 31, loss 350.296, train acc 95.81%, valid acc 92.53%, test acc 93.35%\n","epoch 32, loss 349.940, train acc 96.26%, valid acc 91.76%, test acc 92.64%\n","epoch 33, loss 350.143, train acc 95.96%, valid acc 92.35%, test acc 93.68%\n","epoch 34, loss 349.892, train acc 96.22%, valid acc 92.81%, test acc 93.04%\n","epoch 35, loss 349.689, train acc 96.48%, valid acc 91.49%, test acc 92.48%\n","epoch 36, loss 349.554, train acc 96.62%, valid acc 92.92%, test acc 93.99%\n","epoch 37, loss 349.848, train acc 96.31%, valid acc 93.26%, test acc 93.61%\n","epoch 38, loss 349.531, train acc 96.64%, valid acc 93.10%, test acc 93.75%\n","epoch 39, loss 349.264, train acc 96.94%, valid acc 92.90%, test acc 93.57%\n","epoch 40, loss 349.565, train acc 96.59%, valid acc 92.44%, test acc 93.24%\n","epoch 41, loss 349.464, train acc 96.70%, valid acc 92.06%, test acc 92.68%\n","epoch 42, loss 349.451, train acc 96.73%, valid acc 93.08%, test acc 93.30%\n","epoch 43, loss 349.328, train acc 96.84%, valid acc 92.72%, test acc 93.57%\n","epoch 44, loss 349.249, train acc 96.94%, valid acc 93.31%, test acc 94.32%\n","epoch 45, loss 348.912, train acc 97.31%, valid acc 92.51%, test acc 93.97%\n","epoch 46, loss 349.260, train acc 96.91%, valid acc 93.10%, test acc 94.19%\n","epoch 47, loss 349.080, train acc 97.11%, valid acc 93.88%, test acc 94.35%\n","epoch 48, loss 349.073, train acc 97.13%, valid acc 92.56%, test acc 93.61%\n","epoch 49, loss 348.918, train acc 97.30%, valid acc 93.99%, test acc 94.76%\n","epoch 50, loss 348.955, train acc 97.25%, valid acc 93.72%, test acc 94.26%\n","epoch 51, loss 349.134, train acc 97.07%, valid acc 92.76%, test acc 93.06%\n","epoch 52, loss 348.847, train acc 97.40%, valid acc 93.22%, test acc 94.03%\n","epoch 53, loss 348.967, train acc 97.23%, valid acc 93.69%, test acc 94.28%\n","epoch 54, loss 348.836, train acc 97.39%, valid acc 93.63%, test acc 94.79%\n","epoch 55, loss 348.749, train acc 97.50%, valid acc 93.13%, test acc 93.70%\n","epoch 56, loss 349.082, train acc 97.26%, valid acc 91.69%, test acc 92.79%\n","epoch 57, loss 348.742, train acc 97.51%, valid acc 93.69%, test acc 94.45%\n","epoch 58, loss 348.824, train acc 97.39%, valid acc 93.94%, test acc 94.57%\n","epoch 59, loss 348.411, train acc 97.86%, valid acc 94.06%, test acc 94.03%\n","epoch 60, loss 348.664, train acc 97.57%, valid acc 93.44%, test acc 93.84%\n","epoch 61, loss 348.671, train acc 97.55%, valid acc 92.74%, test acc 93.19%\n","epoch 62, loss 348.831, train acc 97.40%, valid acc 93.06%, test acc 94.10%\n","epoch 63, loss 348.473, train acc 97.77%, valid acc 93.76%, test acc 94.83%\n","epoch 64, loss 348.483, train acc 97.79%, valid acc 93.33%, test acc 94.04%\n","epoch 65, loss 348.536, train acc 97.72%, valid acc 93.42%, test acc 93.68%\n","epoch 66, loss 348.663, train acc 97.56%, valid acc 92.83%, test acc 93.41%\n","epoch 67, loss 348.755, train acc 97.48%, valid acc 93.03%, test acc 94.15%\n","epoch 68, loss 348.453, train acc 97.79%, valid acc 93.13%, test acc 93.88%\n","epoch 69, loss 348.242, train acc 98.04%, valid acc 94.17%, test acc 94.70%\n","epoch 70, loss 348.843, train acc 97.38%, valid acc 94.04%, test acc 94.10%\n","epoch 71, loss 348.758, train acc 97.48%, valid acc 91.71%, test acc 92.84%\n","epoch 72, loss 348.335, train acc 97.91%, valid acc 94.04%, test acc 94.81%\n","epoch 73, loss 348.464, train acc 97.80%, valid acc 93.99%, test acc 94.66%\n","epoch 74, loss 348.146, train acc 98.12%, valid acc 93.81%, test acc 94.34%\n","epoch 75, loss 348.068, train acc 98.25%, valid acc 94.15%, test acc 94.32%\n","epoch 76, loss 348.479, train acc 97.77%, valid acc 93.81%, test acc 93.72%\n","epoch 77, loss 348.071, train acc 98.24%, valid acc 94.61%, test acc 94.79%\n","epoch 78, loss 348.223, train acc 98.05%, valid acc 93.33%, test acc 93.77%\n","epoch 79, loss 348.346, train acc 97.93%, valid acc 94.35%, test acc 94.46%\n","epoch 80, loss 348.079, train acc 98.20%, valid acc 94.45%, test acc 94.46%\n","epoch 81, loss 348.111, train acc 98.16%, valid acc 94.17%, test acc 94.32%\n","epoch 82, loss 348.389, train acc 97.85%, valid acc 94.20%, test acc 94.68%\n","epoch 83, loss 348.499, train acc 97.75%, valid acc 93.40%, test acc 94.19%\n","epoch 84, loss 348.401, train acc 97.84%, valid acc 94.17%, test acc 94.35%\n","epoch 85, loss 348.074, train acc 98.22%, valid acc 94.54%, test acc 94.54%\n","epoch 86, loss 347.951, train acc 98.35%, valid acc 94.40%, test acc 94.74%\n","epoch 87, loss 348.123, train acc 98.18%, valid acc 94.49%, test acc 94.70%\n","epoch 88, loss 347.984, train acc 98.32%, valid acc 94.51%, test acc 94.85%\n","epoch 89, loss 348.031, train acc 98.25%, valid acc 94.08%, test acc 94.30%\n","epoch 90, loss 348.127, train acc 98.16%, valid acc 94.24%, test acc 94.35%\n","epoch 91, loss 348.214, train acc 98.06%, valid acc 93.54%, test acc 94.32%\n","epoch 92, loss 348.303, train acc 97.96%, valid acc 94.17%, test acc 94.86%\n","epoch 93, loss 347.992, train acc 98.30%, valid acc 94.26%, test acc 94.52%\n","epoch 94, loss 347.942, train acc 98.37%, valid acc 93.51%, test acc 94.34%\n","epoch 95, loss 348.137, train acc 98.17%, valid acc 94.13%, test acc 94.50%\n","epoch 96, loss 348.150, train acc 98.12%, valid acc 93.88%, test acc 93.39%\n","epoch 97, loss 348.394, train acc 97.84%, valid acc 94.17%, test acc 94.50%\n","epoch 98, loss 348.057, train acc 98.22%, valid acc 94.35%, test acc 94.92%\n","epoch 99, loss 348.193, train acc 98.09%, valid acc 93.69%, test acc 94.32%\n","epoch 100, loss 347.950, train acc 98.34%, valid acc 94.35%, test acc 94.61%\n","epoch 101, loss 347.840, train acc 98.47%, valid acc 94.31%, test acc 94.70%\n","epoch 102, loss 347.856, train acc 98.46%, valid acc 94.38%, test acc 95.16%\n","epoch 103, loss 347.798, train acc 98.52%, valid acc 94.49%, test acc 94.61%\n","epoch 104, loss 347.939, train acc 98.36%, valid acc 94.47%, test acc 94.77%\n","epoch 105, loss 347.907, train acc 98.42%, valid acc 94.29%, test acc 94.57%\n","epoch 106, loss 347.972, train acc 98.34%, valid acc 94.49%, test acc 95.03%\n","epoch 107, loss 347.786, train acc 98.53%, valid acc 94.58%, test acc 95.08%\n","epoch 108, loss 348.063, train acc 98.21%, valid acc 94.17%, test acc 94.21%\n","epoch 109, loss 348.307, train acc 97.96%, valid acc 94.22%, test acc 94.39%\n","epoch 110, loss 348.251, train acc 98.10%, valid acc 93.19%, test acc 93.64%\n","epoch 111, loss 348.372, train acc 97.88%, valid acc 94.15%, test acc 94.37%\n","epoch 112, loss 348.011, train acc 98.26%, valid acc 94.81%, test acc 95.05%\n","epoch 113, loss 347.808, train acc 98.47%, valid acc 94.47%, test acc 94.76%\n","epoch 114, loss 347.664, train acc 98.66%, valid acc 94.29%, test acc 94.39%\n","epoch 115, loss 347.689, train acc 98.64%, valid acc 94.61%, test acc 95.03%\n","epoch 116, loss 347.757, train acc 98.57%, valid acc 95.04%, test acc 94.72%\n","epoch 117, loss 347.864, train acc 98.45%, valid acc 93.85%, test acc 94.39%\n","epoch 118, loss 347.856, train acc 98.47%, valid acc 93.17%, test acc 93.77%\n","epoch 119, loss 348.514, train acc 97.72%, valid acc 94.65%, test acc 94.96%\n","epoch 120, loss 347.680, train acc 98.65%, valid acc 94.58%, test acc 94.90%\n","epoch 121, loss 347.677, train acc 98.66%, valid acc 94.61%, test acc 94.94%\n","epoch 122, loss 347.995, train acc 98.30%, valid acc 94.45%, test acc 94.97%\n","epoch 123, loss 348.020, train acc 98.27%, valid acc 94.72%, test acc 94.88%\n","epoch 124, loss 347.611, train acc 98.74%, valid acc 94.54%, test acc 95.05%\n","epoch 125, loss 347.690, train acc 98.63%, valid acc 94.54%, test acc 94.65%\n","epoch 126, loss 347.665, train acc 98.66%, valid acc 94.58%, test acc 94.46%\n","epoch 127, loss 347.792, train acc 98.53%, valid acc 94.92%, test acc 95.03%\n","epoch 128, loss 348.068, train acc 98.21%, valid acc 94.33%, test acc 94.76%\n","epoch 129, loss 347.900, train acc 98.39%, valid acc 93.22%, test acc 93.63%\n","epoch 130, loss 348.269, train acc 98.00%, valid acc 94.22%, test acc 94.52%\n","epoch 131, loss 347.891, train acc 98.41%, valid acc 93.67%, test acc 94.04%\n","epoch 132, loss 347.938, train acc 98.36%, valid acc 93.35%, test acc 93.88%\n","epoch 133, loss 347.870, train acc 98.43%, valid acc 93.83%, test acc 94.35%\n","epoch 134, loss 347.834, train acc 98.49%, valid acc 93.94%, test acc 94.74%\n","epoch 135, loss 347.795, train acc 98.51%, valid acc 94.06%, test acc 94.88%\n","epoch 136, loss 347.815, train acc 98.51%, valid acc 94.06%, test acc 94.43%\n","epoch 137, loss 347.798, train acc 98.53%, valid acc 94.47%, test acc 95.19%\n","epoch 138, loss 347.849, train acc 98.46%, valid acc 93.56%, test acc 93.99%\n","epoch 139, loss 347.920, train acc 98.38%, valid acc 94.24%, test acc 94.39%\n","epoch 140, loss 348.150, train acc 98.22%, valid acc 94.58%, test acc 95.28%\n","epoch 141, loss 347.766, train acc 98.57%, valid acc 94.92%, test acc 94.97%\n","epoch 142, loss 347.840, train acc 98.46%, valid acc 94.17%, test acc 94.63%\n","epoch 143, loss 347.978, train acc 98.33%, valid acc 94.51%, test acc 95.19%\n","epoch 144, loss 347.844, train acc 98.46%, valid acc 94.63%, test acc 95.10%\n","epoch 145, loss 347.629, train acc 98.70%, valid acc 94.13%, test acc 94.65%\n","epoch 146, loss 347.682, train acc 98.65%, valid acc 94.72%, test acc 94.77%\n","epoch 147, loss 347.790, train acc 98.53%, valid acc 93.69%, test acc 94.14%\n","epoch 148, loss 347.861, train acc 98.44%, valid acc 94.83%, test acc 95.03%\n","epoch 149, loss 347.685, train acc 98.63%, valid acc 94.74%, test acc 94.90%\n","epoch 150, loss 347.711, train acc 98.62%, valid acc 94.65%, test acc 94.83%\n"]}]},{"cell_type":"code","metadata":{"id":"Kn-XKXs4SQUR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639802662412,"user_tz":300,"elapsed":1803089,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"98e6d110-2446-40d0-db3d-129426b059f1"},"source":["TYPE = 'l1_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate 0.00010, batch size 32, node size 128, kernal size 10\n","epoch 1, loss 380.463, train acc 65.42%, valid acc 77.24%, test acc 77.36%\n","epoch 2, loss 369.301, train acc 79.13%, valid acc 80.83%, test acc 80.70%\n","epoch 3, loss 367.502, train acc 81.10%, valid acc 81.77%, test acc 81.97%\n","epoch 4, loss 366.659, train acc 81.90%, valid acc 79.56%, test acc 79.22%\n","epoch 5, loss 366.131, train acc 82.36%, valid acc 81.90%, test acc 82.08%\n","epoch 6, loss 365.516, train acc 82.90%, valid acc 81.93%, test acc 81.99%\n","epoch 7, loss 365.016, train acc 83.35%, valid acc 83.88%, test acc 83.96%\n","epoch 8, loss 364.646, train acc 83.73%, valid acc 83.02%, test acc 83.50%\n","epoch 9, loss 364.158, train acc 84.18%, valid acc 81.83%, test acc 81.93%\n","epoch 10, loss 363.541, train acc 84.89%, valid acc 84.89%, test acc 85.27%\n","epoch 11, loss 362.757, train acc 85.36%, valid acc 85.52%, test acc 85.39%\n","epoch 12, loss 361.963, train acc 86.25%, valid acc 86.02%, test acc 86.34%\n","epoch 13, loss 360.848, train acc 87.46%, valid acc 87.71%, test acc 87.51%\n","epoch 14, loss 360.019, train acc 88.50%, valid acc 88.32%, test acc 87.96%\n","epoch 15, loss 359.058, train acc 89.58%, valid acc 87.34%, test acc 87.07%\n","epoch 16, loss 358.694, train acc 90.01%, valid acc 88.87%, test acc 89.56%\n","epoch 17, loss 357.810, train acc 90.96%, valid acc 89.85%, test acc 90.33%\n","epoch 18, loss 357.245, train acc 91.45%, valid acc 88.07%, test acc 89.38%\n","epoch 19, loss 356.802, train acc 91.90%, valid acc 88.55%, test acc 90.04%\n","epoch 20, loss 356.478, train acc 92.19%, valid acc 91.30%, test acc 91.39%\n","epoch 21, loss 356.139, train acc 92.68%, valid acc 90.37%, test acc 91.11%\n","epoch 22, loss 355.462, train acc 93.32%, valid acc 91.44%, test acc 91.37%\n","epoch 23, loss 355.026, train acc 93.82%, valid acc 89.96%, test acc 90.46%\n","epoch 24, loss 355.170, train acc 93.70%, valid acc 91.49%, test acc 91.44%\n","epoch 25, loss 354.909, train acc 93.91%, valid acc 91.51%, test acc 91.75%\n","epoch 26, loss 354.716, train acc 94.10%, valid acc 91.83%, test acc 91.93%\n","epoch 27, loss 354.228, train acc 94.61%, valid acc 91.21%, test acc 92.10%\n","epoch 28, loss 353.966, train acc 94.87%, valid acc 92.15%, test acc 92.41%\n","epoch 29, loss 353.779, train acc 95.02%, valid acc 91.69%, test acc 92.62%\n","epoch 30, loss 353.588, train acc 95.24%, valid acc 91.55%, test acc 92.10%\n","epoch 31, loss 353.364, train acc 95.45%, valid acc 91.49%, test acc 92.48%\n","epoch 32, loss 353.533, train acc 95.34%, valid acc 92.81%, test acc 93.48%\n","epoch 33, loss 353.304, train acc 95.49%, valid acc 92.78%, test acc 93.28%\n","epoch 34, loss 352.756, train acc 96.09%, valid acc 92.40%, test acc 93.35%\n","epoch 35, loss 352.942, train acc 95.81%, valid acc 92.69%, test acc 92.79%\n","epoch 36, loss 352.593, train acc 96.21%, valid acc 92.33%, test acc 92.55%\n","epoch 37, loss 352.671, train acc 96.15%, valid acc 93.69%, test acc 93.79%\n","epoch 38, loss 352.472, train acc 96.33%, valid acc 92.40%, test acc 93.23%\n","epoch 39, loss 352.213, train acc 96.60%, valid acc 92.85%, test acc 93.55%\n","epoch 40, loss 352.181, train acc 96.60%, valid acc 92.17%, test acc 93.04%\n","epoch 41, loss 352.780, train acc 95.91%, valid acc 92.42%, test acc 93.28%\n","epoch 42, loss 351.991, train acc 96.77%, valid acc 92.83%, test acc 93.39%\n","epoch 43, loss 352.120, train acc 96.70%, valid acc 91.24%, test acc 91.57%\n","epoch 44, loss 351.776, train acc 96.96%, valid acc 92.94%, test acc 93.41%\n","epoch 45, loss 351.871, train acc 96.90%, valid acc 92.76%, test acc 93.41%\n","epoch 46, loss 351.860, train acc 96.82%, valid acc 93.10%, test acc 93.70%\n","epoch 47, loss 351.935, train acc 96.71%, valid acc 93.01%, test acc 93.28%\n","epoch 48, loss 351.515, train acc 97.22%, valid acc 93.33%, test acc 94.03%\n","epoch 49, loss 351.488, train acc 97.21%, valid acc 92.99%, test acc 93.83%\n","epoch 50, loss 351.426, train acc 97.29%, valid acc 93.22%, test acc 93.90%\n","epoch 51, loss 351.567, train acc 97.10%, valid acc 92.47%, test acc 93.52%\n","epoch 52, loss 351.645, train acc 96.98%, valid acc 92.83%, test acc 93.37%\n","epoch 53, loss 351.582, train acc 97.08%, valid acc 91.40%, test acc 91.97%\n","epoch 54, loss 351.327, train acc 97.36%, valid acc 92.33%, test acc 93.55%\n","epoch 55, loss 351.309, train acc 97.39%, valid acc 93.44%, test acc 93.55%\n","epoch 56, loss 351.339, train acc 97.30%, valid acc 93.49%, test acc 93.99%\n","epoch 57, loss 351.038, train acc 97.63%, valid acc 93.31%, test acc 93.99%\n","epoch 58, loss 351.124, train acc 97.59%, valid acc 93.56%, test acc 93.43%\n","epoch 59, loss 351.225, train acc 97.42%, valid acc 93.74%, test acc 94.08%\n","epoch 60, loss 351.017, train acc 97.62%, valid acc 92.67%, test acc 92.72%\n","epoch 61, loss 350.966, train acc 97.69%, valid acc 93.92%, test acc 94.14%\n","epoch 62, loss 350.779, train acc 97.87%, valid acc 92.42%, test acc 92.17%\n","epoch 63, loss 351.045, train acc 97.53%, valid acc 93.15%, test acc 92.90%\n","epoch 64, loss 350.758, train acc 97.90%, valid acc 93.67%, test acc 93.88%\n","epoch 65, loss 351.007, train acc 97.57%, valid acc 92.99%, test acc 94.12%\n","epoch 66, loss 350.669, train acc 98.00%, valid acc 93.60%, test acc 94.04%\n","epoch 67, loss 350.793, train acc 97.81%, valid acc 93.81%, test acc 94.03%\n","epoch 68, loss 350.699, train acc 97.88%, valid acc 93.31%, test acc 93.57%\n","epoch 69, loss 350.897, train acc 97.64%, valid acc 93.44%, test acc 94.26%\n","epoch 70, loss 350.608, train acc 97.99%, valid acc 92.88%, test acc 93.26%\n","epoch 71, loss 350.516, train acc 98.09%, valid acc 93.74%, test acc 94.08%\n","epoch 72, loss 350.535, train acc 98.04%, valid acc 93.81%, test acc 94.79%\n","epoch 73, loss 350.481, train acc 98.04%, valid acc 93.26%, test acc 94.46%\n","epoch 74, loss 350.516, train acc 98.09%, valid acc 94.10%, test acc 94.77%\n","epoch 75, loss 350.632, train acc 97.90%, valid acc 93.33%, test acc 93.64%\n","epoch 76, loss 350.484, train acc 98.06%, valid acc 93.90%, test acc 94.32%\n","epoch 77, loss 350.391, train acc 98.16%, valid acc 92.53%, test acc 92.86%\n","epoch 78, loss 350.361, train acc 98.17%, valid acc 93.47%, test acc 94.57%\n","epoch 79, loss 350.286, train acc 98.26%, valid acc 93.94%, test acc 94.17%\n","epoch 80, loss 350.367, train acc 98.18%, valid acc 93.54%, test acc 94.08%\n","epoch 81, loss 350.456, train acc 98.07%, valid acc 93.81%, test acc 94.37%\n","epoch 82, loss 350.268, train acc 98.25%, valid acc 93.06%, test acc 93.52%\n","epoch 83, loss 350.282, train acc 98.22%, valid acc 93.83%, test acc 94.28%\n","epoch 84, loss 350.202, train acc 98.34%, valid acc 92.72%, test acc 93.46%\n","epoch 85, loss 350.308, train acc 98.19%, valid acc 93.90%, test acc 94.25%\n","epoch 86, loss 350.214, train acc 98.25%, valid acc 93.74%, test acc 94.41%\n","epoch 87, loss 350.158, train acc 98.36%, valid acc 93.26%, test acc 93.79%\n","epoch 88, loss 350.413, train acc 98.08%, valid acc 93.65%, test acc 93.83%\n","epoch 89, loss 350.285, train acc 98.25%, valid acc 94.10%, test acc 94.46%\n","epoch 90, loss 349.985, train acc 98.51%, valid acc 93.94%, test acc 94.25%\n","epoch 91, loss 350.221, train acc 98.27%, valid acc 93.49%, test acc 94.15%\n","epoch 92, loss 349.945, train acc 98.55%, valid acc 93.85%, test acc 94.17%\n","epoch 93, loss 350.207, train acc 98.26%, valid acc 94.35%, test acc 94.06%\n","epoch 94, loss 350.166, train acc 98.32%, valid acc 94.49%, test acc 94.30%\n","epoch 95, loss 349.979, train acc 98.53%, valid acc 94.04%, test acc 94.68%\n","epoch 96, loss 349.941, train acc 98.53%, valid acc 93.51%, test acc 93.97%\n","epoch 97, loss 349.898, train acc 98.54%, valid acc 94.06%, test acc 94.15%\n","epoch 98, loss 349.968, train acc 98.47%, valid acc 93.88%, test acc 93.90%\n","epoch 99, loss 350.092, train acc 98.33%, valid acc 93.85%, test acc 94.04%\n","epoch 100, loss 349.936, train acc 98.55%, valid acc 94.10%, test acc 94.39%\n","epoch 101, loss 349.984, train acc 98.46%, valid acc 93.69%, test acc 94.32%\n","epoch 102, loss 349.923, train acc 98.52%, valid acc 93.35%, test acc 93.86%\n","epoch 103, loss 350.126, train acc 98.30%, valid acc 93.24%, test acc 93.74%\n","epoch 104, loss 349.840, train acc 98.61%, valid acc 93.85%, test acc 94.61%\n","epoch 105, loss 349.777, train acc 98.63%, valid acc 93.92%, test acc 94.25%\n","epoch 106, loss 349.772, train acc 98.65%, valid acc 94.08%, test acc 94.79%\n","epoch 107, loss 349.872, train acc 98.51%, valid acc 93.99%, test acc 94.32%\n","epoch 108, loss 349.789, train acc 98.59%, valid acc 94.04%, test acc 94.08%\n","epoch 109, loss 349.760, train acc 98.64%, valid acc 93.79%, test acc 94.41%\n","epoch 110, loss 349.956, train acc 98.45%, valid acc 93.88%, test acc 94.50%\n","epoch 111, loss 349.840, train acc 98.55%, valid acc 93.94%, test acc 94.12%\n","epoch 112, loss 349.764, train acc 98.63%, valid acc 93.97%, test acc 94.46%\n","epoch 113, loss 349.794, train acc 98.59%, valid acc 94.29%, test acc 94.37%\n","epoch 114, loss 349.743, train acc 98.63%, valid acc 93.81%, test acc 94.39%\n","epoch 115, loss 349.773, train acc 98.59%, valid acc 93.10%, test acc 94.12%\n","epoch 116, loss 349.827, train acc 98.52%, valid acc 93.33%, test acc 94.06%\n","epoch 117, loss 349.656, train acc 98.71%, valid acc 93.47%, test acc 93.90%\n","epoch 118, loss 349.843, train acc 98.47%, valid acc 93.42%, test acc 94.28%\n","epoch 119, loss 349.675, train acc 98.69%, valid acc 93.94%, test acc 94.10%\n","epoch 120, loss 349.671, train acc 98.70%, valid acc 93.85%, test acc 94.19%\n","epoch 121, loss 349.582, train acc 98.76%, valid acc 94.42%, test acc 94.57%\n","epoch 122, loss 349.547, train acc 98.82%, valid acc 94.35%, test acc 94.77%\n","epoch 123, loss 349.633, train acc 98.71%, valid acc 93.92%, test acc 94.35%\n","epoch 124, loss 349.766, train acc 98.53%, valid acc 94.17%, test acc 94.35%\n","epoch 125, loss 349.658, train acc 98.67%, valid acc 93.76%, test acc 93.83%\n","epoch 126, loss 349.701, train acc 98.62%, valid acc 93.83%, test acc 93.86%\n","epoch 127, loss 349.593, train acc 98.72%, valid acc 93.08%, test acc 93.86%\n","epoch 128, loss 349.613, train acc 98.71%, valid acc 94.22%, test acc 94.46%\n","epoch 129, loss 349.473, train acc 98.85%, valid acc 94.15%, test acc 94.57%\n","epoch 130, loss 349.516, train acc 98.79%, valid acc 93.85%, test acc 94.12%\n","epoch 131, loss 349.523, train acc 98.76%, valid acc 93.83%, test acc 94.32%\n","epoch 132, loss 349.504, train acc 98.80%, valid acc 93.63%, test acc 93.75%\n","epoch 133, loss 349.541, train acc 98.76%, valid acc 94.31%, test acc 94.17%\n","epoch 134, loss 349.510, train acc 98.78%, valid acc 94.47%, test acc 94.19%\n","epoch 135, loss 349.551, train acc 98.73%, valid acc 93.83%, test acc 94.35%\n","epoch 136, loss 349.473, train acc 98.82%, valid acc 93.94%, test acc 94.25%\n","epoch 137, loss 349.525, train acc 98.76%, valid acc 94.33%, test acc 94.65%\n","epoch 138, loss 349.494, train acc 98.76%, valid acc 93.58%, test acc 93.75%\n","epoch 139, loss 349.461, train acc 98.82%, valid acc 93.69%, test acc 94.50%\n","epoch 140, loss 349.662, train acc 98.65%, valid acc 92.92%, test acc 93.70%\n","epoch 141, loss 349.409, train acc 98.86%, valid acc 94.35%, test acc 94.55%\n","epoch 142, loss 349.333, train acc 98.91%, valid acc 94.13%, test acc 94.39%\n","epoch 143, loss 349.439, train acc 98.81%, valid acc 93.97%, test acc 94.34%\n","epoch 144, loss 349.495, train acc 98.77%, valid acc 93.63%, test acc 94.17%\n","epoch 145, loss 349.392, train acc 98.85%, valid acc 93.99%, test acc 94.30%\n","epoch 146, loss 349.593, train acc 98.61%, valid acc 93.81%, test acc 94.52%\n","epoch 147, loss 349.329, train acc 98.88%, valid acc 94.26%, test acc 94.74%\n","epoch 148, loss 349.288, train acc 98.92%, valid acc 94.15%, test acc 94.48%\n","epoch 149, loss 349.264, train acc 98.92%, valid acc 92.97%, test acc 93.23%\n","epoch 150, loss 349.537, train acc 98.65%, valid acc 93.65%, test acc 94.41%\n"]}]},{"cell_type":"code","metadata":{"id":"R57YYAWJS0yn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639804479214,"user_tz":300,"elapsed":1816805,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"ff8124e5-2f03-4599-8975-d03fc6ac603c"},"source":["TYPE = 'l2_norm'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate 0.00010, batch size 32, node size 128, kernal size 10\n","epoch 1, loss 375.321, train acc 66.84%, valid acc 78.12%, test acc 78.73%\n","epoch 2, loss 365.143, train acc 79.64%, valid acc 80.88%, test acc 81.04%\n","epoch 3, loss 362.997, train acc 81.79%, valid acc 83.29%, test acc 83.23%\n","epoch 4, loss 361.872, train acc 83.02%, valid acc 83.31%, test acc 83.68%\n","epoch 5, loss 360.895, train acc 84.06%, valid acc 83.79%, test acc 84.37%\n","epoch 6, loss 359.976, train acc 85.09%, valid acc 84.45%, test acc 85.18%\n","epoch 7, loss 359.467, train acc 85.62%, valid acc 85.43%, test acc 86.60%\n","epoch 8, loss 358.318, train acc 86.95%, valid acc 86.02%, test acc 86.96%\n","epoch 9, loss 357.612, train acc 87.64%, valid acc 87.43%, test acc 88.40%\n","epoch 10, loss 356.852, train acc 88.63%, valid acc 87.73%, test acc 88.58%\n","epoch 11, loss 356.462, train acc 88.96%, valid acc 87.94%, test acc 88.36%\n","epoch 12, loss 355.372, train acc 90.28%, valid acc 88.60%, test acc 88.75%\n","epoch 13, loss 354.863, train acc 90.71%, valid acc 89.89%, test acc 90.57%\n","epoch 14, loss 354.132, train acc 91.62%, valid acc 90.05%, test acc 90.93%\n","epoch 15, loss 354.314, train acc 91.35%, valid acc 89.60%, test acc 90.02%\n","epoch 16, loss 353.746, train acc 92.15%, valid acc 90.78%, test acc 91.37%\n","epoch 17, loss 353.175, train acc 92.70%, valid acc 90.39%, test acc 91.37%\n","epoch 18, loss 353.074, train acc 92.78%, valid acc 90.58%, test acc 91.55%\n","epoch 19, loss 352.633, train acc 93.22%, valid acc 91.55%, test acc 92.48%\n","epoch 20, loss 352.219, train acc 93.70%, valid acc 90.55%, test acc 91.35%\n","epoch 21, loss 352.141, train acc 93.84%, valid acc 89.92%, test acc 90.71%\n","epoch 22, loss 352.316, train acc 93.55%, valid acc 91.62%, test acc 92.26%\n","epoch 23, loss 351.373, train acc 94.60%, valid acc 91.83%, test acc 93.01%\n","epoch 24, loss 351.241, train acc 94.78%, valid acc 92.19%, test acc 92.61%\n","epoch 25, loss 351.346, train acc 94.70%, valid acc 92.01%, test acc 92.53%\n","epoch 26, loss 351.308, train acc 94.69%, valid acc 92.44%, test acc 93.50%\n","epoch 27, loss 351.227, train acc 94.87%, valid acc 92.44%, test acc 92.92%\n","epoch 28, loss 351.138, train acc 94.85%, valid acc 92.12%, test acc 92.26%\n","epoch 29, loss 350.560, train acc 95.49%, valid acc 91.85%, test acc 92.99%\n","epoch 30, loss 350.768, train acc 95.25%, valid acc 92.03%, test acc 92.19%\n","epoch 31, loss 350.499, train acc 95.55%, valid acc 92.33%, test acc 92.92%\n","epoch 32, loss 350.206, train acc 95.94%, valid acc 92.53%, test acc 93.06%\n","epoch 33, loss 350.318, train acc 95.74%, valid acc 92.26%, test acc 93.12%\n","epoch 34, loss 350.103, train acc 96.00%, valid acc 92.76%, test acc 93.77%\n","epoch 35, loss 350.199, train acc 95.89%, valid acc 92.97%, test acc 93.94%\n","epoch 36, loss 350.046, train acc 96.12%, valid acc 92.99%, test acc 93.92%\n","epoch 37, loss 349.794, train acc 96.33%, valid acc 92.12%, test acc 92.92%\n","epoch 38, loss 349.885, train acc 96.28%, valid acc 92.33%, test acc 93.19%\n","epoch 39, loss 349.879, train acc 96.25%, valid acc 92.85%, test acc 93.74%\n","epoch 40, loss 349.806, train acc 96.33%, valid acc 93.03%, test acc 93.64%\n","epoch 41, loss 349.654, train acc 96.50%, valid acc 93.35%, test acc 93.88%\n","epoch 42, loss 349.405, train acc 96.78%, valid acc 92.42%, test acc 93.21%\n","epoch 43, loss 349.548, train acc 96.63%, valid acc 92.06%, test acc 92.57%\n","epoch 44, loss 349.424, train acc 96.73%, valid acc 93.42%, test acc 93.92%\n","epoch 45, loss 349.320, train acc 96.88%, valid acc 92.56%, test acc 93.53%\n","epoch 46, loss 349.338, train acc 96.82%, valid acc 92.97%, test acc 93.68%\n","epoch 47, loss 349.392, train acc 96.78%, valid acc 92.94%, test acc 93.21%\n","epoch 48, loss 349.061, train acc 97.14%, valid acc 92.69%, test acc 93.53%\n","epoch 49, loss 349.392, train acc 96.76%, valid acc 93.24%, test acc 94.14%\n","epoch 50, loss 349.242, train acc 96.92%, valid acc 93.47%, test acc 93.63%\n","epoch 51, loss 349.344, train acc 96.80%, valid acc 93.79%, test acc 93.97%\n","epoch 52, loss 349.224, train acc 96.94%, valid acc 92.94%, test acc 93.52%\n","epoch 53, loss 348.767, train acc 97.44%, valid acc 93.72%, test acc 94.30%\n","epoch 54, loss 348.805, train acc 97.44%, valid acc 93.22%, test acc 93.95%\n","epoch 55, loss 348.811, train acc 97.41%, valid acc 93.10%, test acc 93.79%\n","epoch 56, loss 349.066, train acc 97.11%, valid acc 93.81%, test acc 94.01%\n","epoch 57, loss 348.722, train acc 97.51%, valid acc 92.90%, test acc 94.06%\n","epoch 58, loss 348.983, train acc 97.23%, valid acc 93.01%, test acc 93.66%\n","epoch 59, loss 348.710, train acc 97.55%, valid acc 93.38%, test acc 94.26%\n","epoch 60, loss 348.936, train acc 97.26%, valid acc 93.38%, test acc 93.23%\n","epoch 61, loss 348.761, train acc 97.44%, valid acc 92.88%, test acc 93.06%\n","epoch 62, loss 348.655, train acc 97.56%, valid acc 93.81%, test acc 94.25%\n","epoch 63, loss 348.742, train acc 97.50%, valid acc 93.13%, test acc 93.35%\n","epoch 64, loss 348.582, train acc 97.63%, valid acc 90.46%, test acc 90.71%\n","epoch 65, loss 348.891, train acc 97.31%, valid acc 93.79%, test acc 94.04%\n","epoch 66, loss 348.615, train acc 97.62%, valid acc 93.54%, test acc 93.79%\n","epoch 67, loss 348.555, train acc 97.67%, valid acc 92.58%, test acc 93.43%\n","epoch 68, loss 348.525, train acc 97.72%, valid acc 92.83%, test acc 93.57%\n","epoch 69, loss 348.638, train acc 97.58%, valid acc 93.44%, test acc 93.90%\n","epoch 70, loss 348.433, train acc 97.84%, valid acc 94.31%, test acc 94.39%\n","epoch 71, loss 348.666, train acc 97.55%, valid acc 94.17%, test acc 94.55%\n","epoch 72, loss 348.872, train acc 97.31%, valid acc 92.88%, test acc 93.94%\n","epoch 73, loss 348.356, train acc 97.92%, valid acc 93.74%, test acc 93.72%\n","epoch 74, loss 348.607, train acc 97.60%, valid acc 93.54%, test acc 94.04%\n","epoch 75, loss 348.385, train acc 97.85%, valid acc 93.13%, test acc 94.01%\n","epoch 76, loss 348.339, train acc 97.90%, valid acc 93.99%, test acc 94.43%\n","epoch 77, loss 348.257, train acc 98.00%, valid acc 94.40%, test acc 94.48%\n","epoch 78, loss 348.524, train acc 97.71%, valid acc 94.35%, test acc 94.26%\n","epoch 79, loss 348.721, train acc 97.49%, valid acc 93.85%, test acc 94.28%\n","epoch 80, loss 348.204, train acc 98.08%, valid acc 93.97%, test acc 94.30%\n","epoch 81, loss 348.169, train acc 98.12%, valid acc 93.72%, test acc 94.35%\n","epoch 82, loss 348.187, train acc 98.09%, valid acc 93.97%, test acc 94.50%\n","epoch 83, loss 348.173, train acc 98.08%, valid acc 94.24%, test acc 94.46%\n","epoch 84, loss 348.171, train acc 98.09%, valid acc 93.99%, test acc 94.39%\n","epoch 85, loss 348.444, train acc 97.78%, valid acc 92.78%, test acc 93.90%\n","epoch 86, loss 348.282, train acc 97.99%, valid acc 94.10%, test acc 94.81%\n","epoch 87, loss 348.138, train acc 98.15%, valid acc 93.85%, test acc 94.19%\n","epoch 88, loss 348.098, train acc 98.20%, valid acc 94.13%, test acc 93.99%\n","epoch 89, loss 348.837, train acc 97.36%, valid acc 94.35%, test acc 94.28%\n","epoch 90, loss 348.220, train acc 98.04%, valid acc 93.90%, test acc 93.95%\n","epoch 91, loss 348.115, train acc 98.17%, valid acc 93.79%, test acc 94.26%\n","epoch 92, loss 348.016, train acc 98.27%, valid acc 94.26%, test acc 94.26%\n","epoch 93, loss 348.086, train acc 98.28%, valid acc 93.26%, test acc 93.64%\n","epoch 94, loss 348.346, train acc 97.89%, valid acc 94.10%, test acc 94.25%\n","epoch 95, loss 348.112, train acc 98.15%, valid acc 94.13%, test acc 94.21%\n","epoch 96, loss 348.021, train acc 98.26%, valid acc 94.06%, test acc 94.37%\n","epoch 97, loss 347.989, train acc 98.30%, valid acc 94.33%, test acc 94.63%\n","epoch 98, loss 348.034, train acc 98.25%, valid acc 93.65%, test acc 94.15%\n","epoch 99, loss 348.187, train acc 98.08%, valid acc 93.51%, test acc 94.06%\n","epoch 100, loss 348.083, train acc 98.21%, valid acc 94.22%, test acc 94.34%\n","epoch 101, loss 348.258, train acc 98.03%, valid acc 94.06%, test acc 94.21%\n","epoch 102, loss 348.129, train acc 98.14%, valid acc 93.99%, test acc 93.48%\n","epoch 103, loss 348.217, train acc 98.04%, valid acc 93.85%, test acc 93.55%\n","epoch 104, loss 348.171, train acc 98.09%, valid acc 94.54%, test acc 94.15%\n","epoch 105, loss 347.976, train acc 98.31%, valid acc 94.10%, test acc 94.37%\n","epoch 106, loss 347.941, train acc 98.35%, valid acc 94.54%, test acc 94.57%\n","epoch 107, loss 347.942, train acc 98.35%, valid acc 93.67%, test acc 93.88%\n","epoch 108, loss 348.466, train acc 97.76%, valid acc 93.08%, test acc 93.57%\n","epoch 109, loss 348.095, train acc 98.20%, valid acc 94.22%, test acc 94.34%\n","epoch 110, loss 348.065, train acc 98.22%, valid acc 94.08%, test acc 94.23%\n","epoch 111, loss 348.230, train acc 98.05%, valid acc 93.85%, test acc 93.88%\n","epoch 112, loss 348.329, train acc 97.92%, valid acc 93.85%, test acc 93.43%\n","epoch 113, loss 347.893, train acc 98.42%, valid acc 94.56%, test acc 94.52%\n","epoch 114, loss 347.993, train acc 98.30%, valid acc 93.51%, test acc 93.57%\n","epoch 115, loss 348.061, train acc 98.21%, valid acc 94.22%, test acc 94.14%\n","epoch 116, loss 347.916, train acc 98.36%, valid acc 94.20%, test acc 94.74%\n","epoch 117, loss 348.026, train acc 98.26%, valid acc 94.26%, test acc 94.85%\n","epoch 118, loss 348.043, train acc 98.24%, valid acc 93.56%, test acc 93.81%\n","epoch 119, loss 347.980, train acc 98.32%, valid acc 93.99%, test acc 94.72%\n","epoch 120, loss 348.242, train acc 98.02%, valid acc 93.92%, test acc 93.84%\n","epoch 121, loss 347.939, train acc 98.37%, valid acc 94.04%, test acc 93.95%\n","epoch 122, loss 347.961, train acc 98.34%, valid acc 93.63%, test acc 94.08%\n","epoch 123, loss 348.054, train acc 98.23%, valid acc 93.49%, test acc 93.66%\n","epoch 124, loss 347.794, train acc 98.50%, valid acc 93.51%, test acc 94.08%\n","epoch 125, loss 348.209, train acc 98.04%, valid acc 94.22%, test acc 94.72%\n","epoch 126, loss 348.307, train acc 97.97%, valid acc 94.65%, test acc 94.54%\n","epoch 127, loss 347.832, train acc 98.49%, valid acc 94.04%, test acc 94.37%\n","epoch 128, loss 348.258, train acc 98.00%, valid acc 93.65%, test acc 93.70%\n","epoch 129, loss 347.847, train acc 98.46%, valid acc 94.83%, test acc 94.59%\n","epoch 130, loss 347.921, train acc 98.38%, valid acc 94.86%, test acc 94.74%\n","epoch 131, loss 347.932, train acc 98.37%, valid acc 94.61%, test acc 94.25%\n","epoch 132, loss 348.330, train acc 97.94%, valid acc 94.24%, test acc 94.15%\n","epoch 133, loss 347.910, train acc 98.39%, valid acc 94.20%, test acc 93.81%\n","epoch 134, loss 347.934, train acc 98.38%, valid acc 94.76%, test acc 94.46%\n","epoch 135, loss 347.674, train acc 98.65%, valid acc 93.99%, test acc 94.10%\n","epoch 136, loss 347.881, train acc 98.43%, valid acc 94.20%, test acc 94.23%\n","epoch 137, loss 347.880, train acc 98.42%, valid acc 94.61%, test acc 94.61%\n","epoch 138, loss 347.773, train acc 98.55%, valid acc 94.24%, test acc 94.65%\n","epoch 139, loss 347.868, train acc 98.46%, valid acc 94.90%, test acc 94.61%\n","epoch 140, loss 347.977, train acc 98.40%, valid acc 94.45%, test acc 94.46%\n","epoch 141, loss 348.471, train acc 97.76%, valid acc 94.47%, test acc 94.54%\n","epoch 142, loss 348.105, train acc 98.18%, valid acc 94.45%, test acc 94.26%\n","epoch 143, loss 347.714, train acc 98.61%, valid acc 94.86%, test acc 94.45%\n","epoch 144, loss 347.802, train acc 98.51%, valid acc 94.33%, test acc 94.23%\n","epoch 145, loss 348.015, train acc 98.26%, valid acc 94.45%, test acc 94.35%\n","epoch 146, loss 347.804, train acc 98.51%, valid acc 94.81%, test acc 94.52%\n","epoch 147, loss 348.364, train acc 97.89%, valid acc 92.90%, test acc 92.90%\n","epoch 148, loss 348.149, train acc 98.13%, valid acc 93.63%, test acc 93.66%\n","epoch 149, loss 347.789, train acc 98.53%, valid acc 94.58%, test acc 95.01%\n","epoch 150, loss 347.959, train acc 98.34%, valid acc 94.31%, test acc 94.48%\n"]}]},{"cell_type":"code","metadata":{"id":"mduN9YkJG7vZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639820180684,"user_tz":300,"elapsed":6246849,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"989eff8d-1bca-4868-f89d-ab63ed6454ca"},"source":["TYPE = 'group_lasso'\n","EPOCH_SIZE = 100\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate 0.00010, batch size 32, node size 128, kernal size 10\n","epoch 1, loss 376.362, train acc 65.86%, valid acc 78.51%, test acc 78.47%\n","epoch 2, loss 365.537, train acc 79.37%, valid acc 79.56%, test acc 79.53%\n","epoch 3, loss 363.904, train acc 80.97%, valid acc 81.17%, test acc 81.04%\n","epoch 4, loss 362.496, train acc 82.50%, valid acc 83.11%, test acc 83.94%\n","epoch 5, loss 361.233, train acc 83.91%, valid acc 83.25%, test acc 84.21%\n","epoch 6, loss 360.563, train acc 84.62%, valid acc 84.57%, test acc 84.92%\n","epoch 7, loss 359.769, train acc 85.51%, valid acc 86.05%, test acc 86.56%\n","epoch 8, loss 359.074, train acc 86.44%, valid acc 85.93%, test acc 85.76%\n","epoch 9, loss 358.125, train acc 87.41%, valid acc 87.94%, test acc 87.20%\n","epoch 10, loss 357.957, train acc 87.65%, valid acc 85.45%, test acc 86.45%\n","epoch 11, loss 356.918, train acc 88.75%, valid acc 88.21%, test acc 88.84%\n","epoch 12, loss 356.462, train acc 89.22%, valid acc 85.11%, test acc 85.76%\n","epoch 13, loss 355.871, train acc 89.85%, valid acc 88.87%, test acc 89.27%\n","epoch 14, loss 355.401, train acc 90.39%, valid acc 87.89%, test acc 88.96%\n","epoch 15, loss 355.107, train acc 90.73%, valid acc 90.17%, test acc 90.02%\n","epoch 16, loss 354.690, train acc 91.16%, valid acc 88.16%, test acc 88.93%\n","epoch 17, loss 354.406, train acc 91.59%, valid acc 89.67%, test acc 90.26%\n","epoch 18, loss 354.111, train acc 91.88%, valid acc 90.94%, test acc 91.06%\n","epoch 19, loss 353.844, train acc 92.09%, valid acc 89.89%, test acc 90.17%\n","epoch 20, loss 353.607, train acc 92.36%, valid acc 90.08%, test acc 90.53%\n","epoch 21, loss 353.454, train acc 92.64%, valid acc 89.12%, test acc 89.47%\n","epoch 22, loss 352.986, train acc 93.07%, valid acc 91.15%, test acc 91.59%\n","epoch 23, loss 352.565, train acc 93.50%, valid acc 91.46%, test acc 91.73%\n","epoch 24, loss 352.343, train acc 93.71%, valid acc 90.96%, test acc 91.57%\n","epoch 25, loss 352.332, train acc 93.85%, valid acc 91.01%, test acc 91.46%\n","epoch 26, loss 352.040, train acc 94.06%, valid acc 91.90%, test acc 92.41%\n","epoch 27, loss 351.838, train acc 94.44%, valid acc 91.83%, test acc 91.59%\n","epoch 28, loss 351.712, train acc 94.44%, valid acc 90.99%, test acc 91.79%\n","epoch 29, loss 351.455, train acc 94.79%, valid acc 91.76%, test acc 92.57%\n","epoch 30, loss 351.207, train acc 95.03%, valid acc 91.24%, test acc 91.80%\n","epoch 31, loss 351.202, train acc 94.97%, valid acc 89.71%, test acc 90.04%\n","epoch 32, loss 350.904, train acc 95.40%, valid acc 92.53%, test acc 93.23%\n","epoch 33, loss 350.895, train acc 95.37%, valid acc 91.55%, test acc 92.62%\n","epoch 34, loss 350.684, train acc 95.63%, valid acc 91.90%, test acc 92.62%\n","epoch 35, loss 350.777, train acc 95.49%, valid acc 91.92%, test acc 93.19%\n","epoch 36, loss 350.906, train acc 95.36%, valid acc 88.96%, test acc 89.62%\n","epoch 37, loss 350.649, train acc 95.66%, valid acc 92.72%, test acc 93.13%\n","epoch 38, loss 350.415, train acc 95.86%, valid acc 92.67%, test acc 92.86%\n","epoch 39, loss 350.191, train acc 96.15%, valid acc 91.74%, test acc 92.57%\n","epoch 40, loss 350.657, train acc 95.62%, valid acc 92.51%, test acc 93.33%\n","epoch 41, loss 350.433, train acc 95.86%, valid acc 92.88%, test acc 93.46%\n","epoch 42, loss 349.654, train acc 96.74%, valid acc 92.15%, test acc 92.61%\n","epoch 43, loss 350.031, train acc 96.31%, valid acc 92.33%, test acc 93.13%\n","epoch 44, loss 350.023, train acc 96.32%, valid acc 92.35%, test acc 92.70%\n","epoch 45, loss 350.152, train acc 96.18%, valid acc 92.88%, test acc 93.46%\n","epoch 46, loss 349.696, train acc 96.65%, valid acc 92.85%, test acc 92.84%\n","epoch 47, loss 349.708, train acc 96.67%, valid acc 92.88%, test acc 93.23%\n","epoch 48, loss 349.890, train acc 96.44%, valid acc 93.40%, test acc 93.53%\n","epoch 49, loss 349.619, train acc 96.76%, valid acc 92.99%, test acc 93.10%\n","epoch 50, loss 349.308, train acc 97.13%, valid acc 93.35%, test acc 94.26%\n","epoch 51, loss 349.292, train acc 97.15%, valid acc 93.13%, test acc 93.68%\n","epoch 52, loss 349.567, train acc 96.82%, valid acc 92.40%, test acc 93.43%\n","epoch 53, loss 349.599, train acc 96.78%, valid acc 92.83%, test acc 93.81%\n","epoch 54, loss 349.237, train acc 97.15%, valid acc 93.35%, test acc 93.84%\n","epoch 55, loss 348.795, train acc 97.69%, valid acc 93.44%, test acc 93.94%\n","epoch 56, loss 349.373, train acc 97.05%, valid acc 93.15%, test acc 93.95%\n","epoch 57, loss 348.859, train acc 97.62%, valid acc 93.58%, test acc 93.26%\n","epoch 58, loss 349.057, train acc 97.39%, valid acc 93.31%, test acc 94.32%\n","epoch 59, loss 349.136, train acc 97.31%, valid acc 93.54%, test acc 94.28%\n","epoch 60, loss 348.812, train acc 97.70%, valid acc 93.65%, test acc 94.35%\n","epoch 61, loss 348.614, train acc 97.91%, valid acc 93.60%, test acc 94.15%\n","epoch 62, loss 348.892, train acc 97.58%, valid acc 93.10%, test acc 93.52%\n","epoch 63, loss 348.922, train acc 97.55%, valid acc 92.99%, test acc 93.77%\n","epoch 64, loss 348.755, train acc 97.75%, valid acc 93.47%, test acc 94.06%\n","epoch 65, loss 348.919, train acc 97.54%, valid acc 92.88%, test acc 93.61%\n","epoch 66, loss 349.024, train acc 97.46%, valid acc 93.42%, test acc 94.01%\n","epoch 67, loss 348.829, train acc 97.64%, valid acc 92.67%, test acc 93.79%\n","epoch 68, loss 348.853, train acc 97.64%, valid acc 93.58%, test acc 94.37%\n","epoch 69, loss 348.371, train acc 98.18%, valid acc 94.61%, test acc 94.68%\n","epoch 70, loss 348.535, train acc 97.99%, valid acc 93.35%, test acc 94.04%\n","epoch 71, loss 348.806, train acc 97.71%, valid acc 93.03%, test acc 93.75%\n","epoch 72, loss 348.647, train acc 97.88%, valid acc 93.28%, test acc 94.12%\n","epoch 73, loss 348.639, train acc 97.88%, valid acc 93.67%, test acc 94.25%\n","epoch 74, loss 348.511, train acc 98.01%, valid acc 93.69%, test acc 94.25%\n","epoch 75, loss 348.661, train acc 97.84%, valid acc 93.97%, test acc 94.45%\n","epoch 76, loss 348.894, train acc 97.55%, valid acc 91.53%, test acc 92.33%\n","epoch 77, loss 348.502, train acc 98.01%, valid acc 93.67%, test acc 94.17%\n","epoch 78, loss 348.515, train acc 98.01%, valid acc 93.22%, test acc 93.94%\n","epoch 79, loss 348.680, train acc 97.84%, valid acc 93.58%, test acc 94.76%\n","epoch 80, loss 348.548, train acc 97.96%, valid acc 93.19%, test acc 94.41%\n","epoch 81, loss 348.795, train acc 97.68%, valid acc 94.06%, test acc 94.52%\n","epoch 82, loss 348.409, train acc 98.14%, valid acc 93.99%, test acc 94.92%\n","epoch 83, loss 348.293, train acc 98.24%, valid acc 94.10%, test acc 94.79%\n","epoch 84, loss 348.427, train acc 98.12%, valid acc 94.01%, test acc 94.32%\n","epoch 85, loss 348.382, train acc 98.16%, valid acc 93.54%, test acc 93.95%\n","epoch 86, loss 348.387, train acc 98.17%, valid acc 93.58%, test acc 94.90%\n","epoch 87, loss 348.891, train acc 97.60%, valid acc 93.69%, test acc 94.74%\n","epoch 88, loss 348.736, train acc 97.76%, valid acc 92.62%, test acc 93.32%\n","epoch 89, loss 348.421, train acc 98.12%, valid acc 93.74%, test acc 94.28%\n","epoch 90, loss 348.309, train acc 98.22%, valid acc 93.88%, test acc 95.08%\n","epoch 91, loss 348.644, train acc 97.90%, valid acc 93.69%, test acc 94.72%\n","epoch 92, loss 348.406, train acc 98.14%, valid acc 94.04%, test acc 94.86%\n","epoch 93, loss 348.278, train acc 98.28%, valid acc 93.65%, test acc 94.66%\n","epoch 94, loss 348.557, train acc 97.98%, valid acc 93.97%, test acc 94.74%\n","epoch 95, loss 348.524, train acc 98.01%, valid acc 93.54%, test acc 94.30%\n","epoch 96, loss 348.479, train acc 98.16%, valid acc 93.22%, test acc 93.92%\n","epoch 97, loss 348.466, train acc 98.08%, valid acc 93.67%, test acc 94.59%\n","epoch 98, loss 348.276, train acc 98.30%, valid acc 93.76%, test acc 94.81%\n","epoch 99, loss 348.233, train acc 98.33%, valid acc 93.58%, test acc 94.34%\n","epoch 100, loss 348.297, train acc 98.25%, valid acc 93.31%, test acc 93.83%\n"]}]},{"cell_type":"code","metadata":{"id":"yOB_WseAoSwC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639830356260,"user_tz":300,"elapsed":9289639,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"788893ae-83f8-4319-f584-3890b1f01ac3"},"source":["TYPE = 'l1_group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate 0.00010, batch size 32, node size 128, kernal size 10\n","epoch 1, loss 377.526, train acc 66.80%, valid acc 78.90%, test acc 79.51%\n","epoch 2, loss 367.475, train acc 79.31%, valid acc 79.92%, test acc 80.55%\n","epoch 3, loss 366.480, train acc 80.29%, valid acc 80.83%, test acc 81.15%\n","epoch 4, loss 365.884, train acc 80.95%, valid acc 79.33%, test acc 79.53%\n","epoch 5, loss 363.769, train acc 83.22%, valid acc 84.04%, test acc 84.39%\n","epoch 6, loss 362.578, train acc 84.53%, valid acc 86.05%, test acc 85.90%\n","epoch 7, loss 361.814, train acc 85.42%, valid acc 84.57%, test acc 84.87%\n","epoch 8, loss 360.747, train acc 86.55%, valid acc 85.93%, test acc 86.34%\n","epoch 9, loss 360.803, train acc 86.40%, valid acc 86.80%, test acc 87.42%\n","epoch 10, loss 359.574, train acc 87.85%, valid acc 87.32%, test acc 88.14%\n","epoch 11, loss 359.064, train acc 88.41%, valid acc 87.50%, test acc 88.20%\n","epoch 12, loss 358.194, train acc 89.39%, valid acc 85.93%, test acc 86.03%\n","epoch 13, loss 357.889, train acc 89.71%, valid acc 88.21%, test acc 89.00%\n","epoch 14, loss 357.145, train acc 90.44%, valid acc 89.51%, test acc 90.09%\n","epoch 15, loss 356.461, train acc 91.22%, valid acc 88.12%, test acc 88.87%\n","epoch 16, loss 356.231, train acc 91.50%, valid acc 89.23%, test acc 89.56%\n","epoch 17, loss 356.040, train acc 91.82%, valid acc 90.92%, test acc 91.53%\n","epoch 18, loss 355.477, train acc 92.37%, valid acc 90.37%, test acc 90.97%\n","epoch 19, loss 355.003, train acc 92.82%, valid acc 91.17%, test acc 91.84%\n","epoch 20, loss 354.937, train acc 92.89%, valid acc 89.69%, test acc 90.55%\n","epoch 21, loss 354.580, train acc 93.40%, valid acc 91.21%, test acc 92.22%\n","epoch 22, loss 354.249, train acc 93.65%, valid acc 91.71%, test acc 92.06%\n","epoch 23, loss 354.054, train acc 93.94%, valid acc 91.65%, test acc 92.33%\n","epoch 24, loss 353.771, train acc 94.19%, valid acc 91.67%, test acc 92.19%\n","epoch 25, loss 353.807, train acc 94.11%, valid acc 91.49%, test acc 92.11%\n","epoch 26, loss 353.630, train acc 94.38%, valid acc 91.55%, test acc 92.02%\n","epoch 27, loss 353.097, train acc 94.90%, valid acc 92.06%, test acc 92.53%\n","epoch 28, loss 353.141, train acc 94.86%, valid acc 92.51%, test acc 92.99%\n","epoch 29, loss 352.656, train acc 95.35%, valid acc 92.51%, test acc 93.06%\n","epoch 30, loss 352.651, train acc 95.36%, valid acc 92.58%, test acc 93.04%\n","epoch 31, loss 352.504, train acc 95.55%, valid acc 92.21%, test acc 92.97%\n","epoch 32, loss 352.368, train acc 95.68%, valid acc 91.74%, test acc 92.46%\n","epoch 33, loss 352.153, train acc 95.87%, valid acc 92.21%, test acc 93.19%\n","epoch 34, loss 352.292, train acc 95.75%, valid acc 91.78%, test acc 92.62%\n","epoch 35, loss 352.017, train acc 96.06%, valid acc 92.56%, test acc 93.08%\n","epoch 36, loss 351.866, train acc 96.21%, valid acc 92.37%, test acc 92.95%\n","epoch 37, loss 351.709, train acc 96.40%, valid acc 92.58%, test acc 92.73%\n","epoch 38, loss 351.846, train acc 96.28%, valid acc 91.53%, test acc 92.39%\n","epoch 39, loss 351.643, train acc 96.44%, valid acc 92.35%, test acc 93.37%\n","epoch 40, loss 351.555, train acc 96.53%, valid acc 93.13%, test acc 93.53%\n","epoch 41, loss 351.515, train acc 96.55%, valid acc 92.65%, test acc 93.52%\n","epoch 42, loss 351.237, train acc 96.85%, valid acc 92.88%, test acc 93.32%\n","epoch 43, loss 351.265, train acc 96.84%, valid acc 92.94%, test acc 93.63%\n","epoch 44, loss 351.223, train acc 96.87%, valid acc 92.65%, test acc 93.23%\n","epoch 45, loss 351.132, train acc 96.94%, valid acc 93.90%, test acc 94.26%\n","epoch 46, loss 351.009, train acc 97.10%, valid acc 92.78%, test acc 93.59%\n","epoch 47, loss 350.909, train acc 97.18%, valid acc 92.67%, test acc 93.33%\n","epoch 48, loss 350.926, train acc 97.16%, valid acc 93.97%, test acc 94.01%\n","epoch 49, loss 350.837, train acc 97.26%, valid acc 93.60%, test acc 93.52%\n","epoch 50, loss 350.752, train acc 97.40%, valid acc 93.26%, test acc 93.68%\n","epoch 51, loss 350.783, train acc 97.30%, valid acc 93.67%, test acc 94.21%\n","epoch 52, loss 350.632, train acc 97.46%, valid acc 93.42%, test acc 93.83%\n","epoch 53, loss 350.429, train acc 97.69%, valid acc 93.47%, test acc 93.63%\n","epoch 54, loss 350.549, train acc 97.54%, valid acc 93.72%, test acc 93.53%\n","epoch 55, loss 350.521, train acc 97.54%, valid acc 94.13%, test acc 94.39%\n","epoch 56, loss 350.369, train acc 97.72%, valid acc 92.92%, test acc 93.06%\n","epoch 57, loss 350.273, train acc 97.79%, valid acc 91.26%, test acc 92.02%\n","epoch 58, loss 350.464, train acc 97.61%, valid acc 93.06%, test acc 93.48%\n","epoch 59, loss 350.273, train acc 97.79%, valid acc 93.60%, test acc 94.12%\n","epoch 60, loss 350.369, train acc 97.67%, valid acc 92.56%, test acc 92.81%\n","epoch 61, loss 350.375, train acc 97.67%, valid acc 93.81%, test acc 93.86%\n","epoch 62, loss 350.286, train acc 97.75%, valid acc 93.28%, test acc 94.25%\n","epoch 63, loss 350.206, train acc 97.84%, valid acc 94.01%, test acc 94.21%\n","epoch 64, loss 350.088, train acc 97.94%, valid acc 93.56%, test acc 94.01%\n","epoch 65, loss 350.028, train acc 98.01%, valid acc 93.67%, test acc 94.21%\n","epoch 66, loss 350.210, train acc 97.83%, valid acc 93.76%, test acc 93.79%\n","epoch 67, loss 350.104, train acc 97.91%, valid acc 93.33%, test acc 93.61%\n","epoch 68, loss 350.107, train acc 97.93%, valid acc 93.97%, test acc 94.46%\n","epoch 69, loss 350.083, train acc 97.93%, valid acc 93.76%, test acc 93.90%\n","epoch 70, loss 349.933, train acc 98.10%, valid acc 93.83%, test acc 94.23%\n","epoch 71, loss 349.847, train acc 98.20%, valid acc 94.35%, test acc 94.41%\n","epoch 72, loss 350.016, train acc 97.99%, valid acc 93.94%, test acc 94.15%\n","epoch 73, loss 350.038, train acc 97.93%, valid acc 93.47%, test acc 94.35%\n","epoch 74, loss 349.843, train acc 98.13%, valid acc 94.20%, test acc 94.26%\n","epoch 75, loss 349.724, train acc 98.26%, valid acc 93.85%, test acc 93.95%\n","epoch 76, loss 350.190, train acc 97.77%, valid acc 92.69%, test acc 93.01%\n","epoch 77, loss 349.904, train acc 98.06%, valid acc 94.06%, test acc 94.10%\n","epoch 78, loss 349.700, train acc 98.29%, valid acc 93.56%, test acc 93.77%\n","epoch 79, loss 349.763, train acc 98.22%, valid acc 93.72%, test acc 94.15%\n","epoch 80, loss 349.873, train acc 98.12%, valid acc 93.99%, test acc 93.86%\n","epoch 81, loss 349.818, train acc 98.20%, valid acc 94.35%, test acc 94.61%\n","epoch 82, loss 349.838, train acc 98.16%, valid acc 93.69%, test acc 93.57%\n","epoch 83, loss 349.812, train acc 98.17%, valid acc 93.65%, test acc 94.28%\n","epoch 84, loss 349.613, train acc 98.36%, valid acc 93.51%, test acc 93.79%\n","epoch 85, loss 349.866, train acc 98.08%, valid acc 93.90%, test acc 94.21%\n","epoch 86, loss 349.807, train acc 98.14%, valid acc 94.35%, test acc 94.30%\n","epoch 87, loss 349.756, train acc 98.18%, valid acc 93.94%, test acc 94.10%\n","epoch 88, loss 349.745, train acc 98.20%, valid acc 93.24%, test acc 94.06%\n","epoch 89, loss 349.726, train acc 98.21%, valid acc 94.01%, test acc 94.26%\n","epoch 90, loss 349.695, train acc 98.26%, valid acc 94.56%, test acc 94.76%\n","epoch 91, loss 349.617, train acc 98.31%, valid acc 93.76%, test acc 94.15%\n","epoch 92, loss 349.526, train acc 98.43%, valid acc 93.88%, test acc 94.15%\n","epoch 93, loss 349.675, train acc 98.25%, valid acc 94.51%, test acc 94.52%\n","epoch 94, loss 349.555, train acc 98.36%, valid acc 93.97%, test acc 94.25%\n","epoch 95, loss 349.716, train acc 98.24%, valid acc 94.70%, test acc 94.04%\n","epoch 96, loss 349.331, train acc 98.59%, valid acc 94.63%, test acc 94.37%\n","epoch 97, loss 349.521, train acc 98.39%, valid acc 92.72%, test acc 93.33%\n","epoch 98, loss 349.680, train acc 98.24%, valid acc 93.67%, test acc 93.50%\n","epoch 99, loss 349.669, train acc 98.21%, valid acc 94.15%, test acc 94.19%\n","epoch 100, loss 349.550, train acc 98.36%, valid acc 94.04%, test acc 94.01%\n","epoch 101, loss 349.544, train acc 98.34%, valid acc 94.17%, test acc 94.28%\n","epoch 102, loss 349.373, train acc 98.54%, valid acc 94.13%, test acc 93.95%\n","epoch 103, loss 349.263, train acc 98.65%, valid acc 93.99%, test acc 94.46%\n","epoch 104, loss 349.459, train acc 98.45%, valid acc 93.94%, test acc 94.10%\n","epoch 105, loss 349.266, train acc 98.64%, valid acc 93.83%, test acc 94.32%\n","epoch 106, loss 349.492, train acc 98.39%, valid acc 94.51%, test acc 94.30%\n","epoch 107, loss 349.318, train acc 98.60%, valid acc 94.33%, test acc 94.10%\n","epoch 108, loss 349.395, train acc 98.49%, valid acc 93.90%, test acc 94.06%\n","epoch 109, loss 349.399, train acc 98.47%, valid acc 93.99%, test acc 94.43%\n","epoch 110, loss 349.235, train acc 98.66%, valid acc 94.45%, test acc 94.14%\n","epoch 111, loss 349.276, train acc 98.62%, valid acc 94.74%, test acc 94.81%\n","epoch 112, loss 349.368, train acc 98.51%, valid acc 94.54%, test acc 94.50%\n","epoch 113, loss 349.268, train acc 98.65%, valid acc 94.35%, test acc 94.10%\n","epoch 114, loss 349.205, train acc 98.69%, valid acc 94.81%, test acc 94.57%\n","epoch 115, loss 349.208, train acc 98.69%, valid acc 94.47%, test acc 94.06%\n","epoch 116, loss 349.229, train acc 98.63%, valid acc 94.56%, test acc 94.28%\n","epoch 117, loss 349.207, train acc 98.68%, valid acc 94.54%, test acc 94.17%\n","epoch 118, loss 349.128, train acc 98.76%, valid acc 94.40%, test acc 94.26%\n","epoch 119, loss 349.001, train acc 98.87%, valid acc 94.97%, test acc 94.90%\n","epoch 120, loss 349.231, train acc 98.62%, valid acc 93.72%, test acc 93.75%\n","epoch 121, loss 349.318, train acc 98.54%, valid acc 93.74%, test acc 93.52%\n","epoch 122, loss 348.968, train acc 98.91%, valid acc 94.88%, test acc 94.48%\n","epoch 123, loss 349.067, train acc 98.80%, valid acc 94.65%, test acc 94.88%\n","epoch 124, loss 348.991, train acc 98.87%, valid acc 94.49%, test acc 94.32%\n","epoch 125, loss 349.167, train acc 98.69%, valid acc 93.94%, test acc 94.46%\n","epoch 126, loss 349.033, train acc 98.83%, valid acc 94.74%, test acc 94.65%\n","epoch 127, loss 348.991, train acc 98.88%, valid acc 94.08%, test acc 93.88%\n","epoch 128, loss 349.089, train acc 98.78%, valid acc 94.49%, test acc 94.15%\n","epoch 129, loss 348.929, train acc 98.96%, valid acc 94.45%, test acc 94.41%\n","epoch 130, loss 348.897, train acc 98.95%, valid acc 95.36%, test acc 94.81%\n","epoch 131, loss 348.988, train acc 98.87%, valid acc 94.40%, test acc 94.37%\n","epoch 132, loss 349.065, train acc 98.76%, valid acc 94.67%, test acc 94.50%\n","epoch 133, loss 348.914, train acc 98.94%, valid acc 94.95%, test acc 94.28%\n","epoch 134, loss 348.889, train acc 98.96%, valid acc 95.22%, test acc 95.12%\n","epoch 135, loss 348.912, train acc 98.91%, valid acc 94.90%, test acc 94.97%\n","epoch 136, loss 348.907, train acc 98.94%, valid acc 95.04%, test acc 95.10%\n","epoch 137, loss 348.850, train acc 99.00%, valid acc 94.08%, test acc 94.34%\n","epoch 138, loss 348.980, train acc 98.89%, valid acc 93.97%, test acc 94.39%\n","epoch 139, loss 349.003, train acc 98.83%, valid acc 94.83%, test acc 94.43%\n","epoch 140, loss 348.773, train acc 99.06%, valid acc 94.22%, test acc 94.28%\n","epoch 141, loss 348.809, train acc 99.01%, valid acc 93.38%, test acc 93.68%\n","epoch 142, loss 349.163, train acc 98.63%, valid acc 94.83%, test acc 94.50%\n","epoch 143, loss 348.821, train acc 99.01%, valid acc 93.79%, test acc 93.83%\n","epoch 144, loss 348.920, train acc 98.89%, valid acc 93.49%, test acc 93.86%\n","epoch 145, loss 348.934, train acc 98.88%, valid acc 94.26%, test acc 94.92%\n","epoch 146, loss 348.881, train acc 98.94%, valid acc 94.61%, test acc 94.72%\n","epoch 147, loss 348.826, train acc 98.99%, valid acc 94.67%, test acc 94.81%\n","epoch 148, loss 349.076, train acc 98.75%, valid acc 94.22%, test acc 94.26%\n","epoch 149, loss 348.869, train acc 98.96%, valid acc 94.95%, test acc 95.26%\n","epoch 150, loss 348.896, train acc 98.91%, valid acc 94.54%, test acc 94.77%\n"]}]},{"cell_type":"code","metadata":{"id":"LA34mdObzBtm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f3d6e4e-4186-4f60-ccb0-a7ce619a5b1e","executionInfo":{"status":"ok","timestamp":1639859003504,"user_tz":300,"elapsed":20540322,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}}},"source":["TYPE = 'l0_group_lasso'\n","EPOCH_SIZE = 150\n","train_acc, valid_acc, test_acc = train_save_CNN_model(TYPE, EPOCH_SIZE)\n","print(train_acc)\n","print(valid_acc)\n","print(test_acc)\n","file_name = 'data'+str(k)+'.txt'\n","with open(file_name, 'w') as f:\n","    for i in train_acc:\n","      f.write(\"%f \" % i)\n","    f.write('\\n')\n","    for i in valid_acc:\n","      f.write(\"%f \" % i)\n","    f.write('\\n')\n","    for i in test_acc:\n","      f.write(\"%f \" % i)\n","    f.write('\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate 0.00010, batch size 32, node size 256, kernal size 10\n","epoch 1, loss 420.618, train acc 42.15%, valid acc 64.76%, test acc 65.00%\n","epoch 2, loss 396.407, train acc 66.30%, valid acc 69.63%, test acc 69.70%\n","epoch 3, loss 391.825, train acc 70.27%, valid acc 71.00%, test acc 71.57%\n","epoch 4, loss 390.335, train acc 71.34%, valid acc 70.54%, test acc 71.35%\n","epoch 5, loss 389.033, train acc 72.66%, valid acc 71.93%, test acc 72.21%\n","epoch 6, loss 388.259, train acc 73.47%, valid acc 74.73%, test acc 75.29%\n","epoch 7, loss 387.539, train acc 74.41%, valid acc 75.10%, test acc 75.60%\n","epoch 8, loss 386.291, train acc 76.04%, valid acc 75.83%, test acc 76.27%\n","epoch 9, loss 385.626, train acc 77.08%, valid acc 76.42%, test acc 76.85%\n","epoch 10, loss 385.092, train acc 78.15%, valid acc 78.47%, test acc 78.47%\n","epoch 11, loss 384.304, train acc 79.14%, valid acc 77.01%, test acc 76.87%\n","epoch 12, loss 383.210, train acc 80.67%, valid acc 81.95%, test acc 81.42%\n","epoch 13, loss 383.643, train acc 80.57%, valid acc 79.44%, test acc 79.77%\n","epoch 14, loss 382.361, train acc 81.67%, valid acc 82.04%, test acc 81.41%\n","epoch 15, loss 381.848, train acc 82.54%, valid acc 76.67%, test acc 76.34%\n","epoch 16, loss 381.291, train acc 83.06%, valid acc 83.50%, test acc 83.37%\n","epoch 17, loss 381.088, train acc 83.27%, valid acc 81.90%, test acc 82.15%\n","epoch 18, loss 380.649, train acc 83.74%, valid acc 80.58%, test acc 81.22%\n","epoch 19, loss 380.388, train acc 84.05%, valid acc 83.72%, test acc 84.06%\n","epoch 20, loss 380.513, train acc 83.51%, valid acc 83.11%, test acc 83.17%\n","epoch 21, loss 379.549, train acc 84.33%, valid acc 82.90%, test acc 82.61%\n","epoch 22, loss 378.669, train acc 85.17%, valid acc 80.49%, test acc 80.30%\n","epoch 23, loss 378.386, train acc 85.54%, valid acc 83.59%, test acc 83.70%\n","epoch 24, loss 378.540, train acc 85.46%, valid acc 85.27%, test acc 85.70%\n","epoch 25, loss 377.844, train acc 86.19%, valid acc 82.95%, test acc 83.26%\n","epoch 26, loss 377.692, train acc 86.31%, valid acc 85.39%, test acc 85.81%\n","epoch 27, loss 377.348, train acc 86.59%, valid acc 86.00%, test acc 86.47%\n","epoch 28, loss 376.875, train acc 87.11%, valid acc 86.21%, test acc 86.45%\n","epoch 29, loss 376.898, train acc 87.15%, valid acc 86.30%, test acc 86.94%\n","epoch 30, loss 376.757, train acc 87.31%, valid acc 85.57%, test acc 86.36%\n","epoch 31, loss 378.067, train acc 85.91%, valid acc 84.91%, test acc 85.85%\n","epoch 32, loss 376.809, train acc 87.34%, valid acc 85.82%, test acc 86.45%\n","epoch 33, loss 376.495, train acc 87.48%, valid acc 85.91%, test acc 86.72%\n","epoch 34, loss 376.462, train acc 87.62%, valid acc 86.43%, test acc 86.87%\n","epoch 35, loss 376.082, train acc 87.93%, valid acc 84.91%, test acc 86.07%\n","epoch 36, loss 375.937, train acc 88.00%, valid acc 87.16%, test acc 87.83%\n","epoch 37, loss 375.241, train acc 88.82%, valid acc 81.58%, test acc 81.92%\n","epoch 38, loss 375.225, train acc 89.10%, valid acc 85.18%, test acc 85.19%\n","epoch 39, loss 374.495, train acc 89.76%, valid acc 83.18%, test acc 83.99%\n","epoch 40, loss 373.701, train acc 90.59%, valid acc 89.62%, test acc 90.26%\n","epoch 41, loss 373.162, train acc 91.33%, valid acc 89.28%, test acc 90.27%\n","epoch 42, loss 372.878, train acc 91.65%, valid acc 87.41%, test acc 88.65%\n","epoch 43, loss 372.886, train acc 91.61%, valid acc 87.78%, test acc 88.49%\n","epoch 44, loss 372.531, train acc 92.01%, valid acc 90.05%, test acc 91.22%\n","epoch 45, loss 371.757, train acc 92.95%, valid acc 90.46%, test acc 91.28%\n","epoch 46, loss 372.003, train acc 92.68%, valid acc 90.39%, test acc 90.75%\n","epoch 47, loss 371.324, train acc 93.41%, valid acc 89.73%, test acc 90.89%\n","epoch 48, loss 371.209, train acc 93.62%, valid acc 90.99%, test acc 91.79%\n","epoch 49, loss 370.926, train acc 93.97%, valid acc 90.76%, test acc 91.82%\n","epoch 50, loss 370.769, train acc 94.09%, valid acc 91.24%, test acc 91.59%\n","epoch 51, loss 370.812, train acc 94.02%, valid acc 89.41%, test acc 90.15%\n","epoch 52, loss 370.413, train acc 94.55%, valid acc 90.83%, test acc 91.95%\n","epoch 53, loss 370.247, train acc 94.63%, valid acc 91.65%, test acc 92.75%\n","epoch 54, loss 370.098, train acc 94.83%, valid acc 90.99%, test acc 92.11%\n","epoch 55, loss 369.983, train acc 95.16%, valid acc 91.05%, test acc 91.22%\n","epoch 56, loss 370.510, train acc 94.40%, valid acc 90.14%, test acc 91.20%\n","epoch 57, loss 369.739, train acc 95.21%, valid acc 91.08%, test acc 91.79%\n","epoch 58, loss 369.825, train acc 95.12%, valid acc 90.44%, test acc 90.17%\n","epoch 59, loss 369.450, train acc 95.59%, valid acc 92.81%, test acc 92.95%\n","epoch 60, loss 369.319, train acc 95.63%, valid acc 92.58%, test acc 93.15%\n","epoch 61, loss 369.304, train acc 95.66%, valid acc 92.26%, test acc 92.81%\n","epoch 62, loss 369.368, train acc 95.66%, valid acc 92.35%, test acc 93.28%\n","epoch 63, loss 368.913, train acc 96.16%, valid acc 91.24%, test acc 91.75%\n","epoch 64, loss 369.034, train acc 96.00%, valid acc 92.58%, test acc 93.06%\n","epoch 65, loss 368.925, train acc 96.11%, valid acc 93.13%, test acc 93.39%\n","epoch 66, loss 368.579, train acc 96.47%, valid acc 92.65%, test acc 92.97%\n","epoch 67, loss 368.543, train acc 96.52%, valid acc 92.72%, test acc 93.37%\n","epoch 68, loss 368.943, train acc 96.10%, valid acc 92.35%, test acc 93.30%\n","epoch 69, loss 368.787, train acc 96.26%, valid acc 90.26%, test acc 91.46%\n","epoch 70, loss 368.626, train acc 96.42%, valid acc 92.83%, test acc 93.32%\n","epoch 71, loss 368.634, train acc 96.35%, valid acc 92.62%, test acc 93.02%\n","epoch 72, loss 368.412, train acc 96.66%, valid acc 91.90%, test acc 92.04%\n","epoch 73, loss 368.494, train acc 96.49%, valid acc 92.81%, test acc 93.01%\n","epoch 74, loss 368.434, train acc 96.62%, valid acc 92.78%, test acc 93.66%\n","epoch 75, loss 368.190, train acc 96.97%, valid acc 88.71%, test acc 89.05%\n","epoch 76, loss 368.013, train acc 97.11%, valid acc 92.94%, test acc 93.64%\n","epoch 77, loss 368.534, train acc 96.51%, valid acc 91.40%, test acc 92.19%\n","epoch 78, loss 368.435, train acc 96.63%, valid acc 92.49%, test acc 92.59%\n","epoch 79, loss 368.104, train acc 96.97%, valid acc 92.78%, test acc 93.23%\n","epoch 80, loss 368.085, train acc 96.93%, valid acc 93.42%, test acc 93.90%\n","epoch 81, loss 367.686, train acc 97.40%, valid acc 93.13%, test acc 93.84%\n","epoch 82, loss 367.701, train acc 97.34%, valid acc 93.03%, test acc 93.46%\n","epoch 83, loss 367.914, train acc 97.13%, valid acc 92.47%, test acc 92.77%\n","epoch 84, loss 367.998, train acc 97.06%, valid acc 92.85%, test acc 93.32%\n","epoch 85, loss 367.797, train acc 97.27%, valid acc 93.13%, test acc 93.39%\n","epoch 86, loss 367.857, train acc 97.13%, valid acc 92.81%, test acc 93.10%\n","epoch 87, loss 367.660, train acc 97.49%, valid acc 92.44%, test acc 93.23%\n","epoch 88, loss 367.533, train acc 97.46%, valid acc 93.13%, test acc 93.81%\n","epoch 89, loss 367.728, train acc 97.25%, valid acc 92.17%, test acc 92.48%\n","epoch 90, loss 367.772, train acc 97.22%, valid acc 92.94%, test acc 93.43%\n","epoch 91, loss 367.712, train acc 97.22%, valid acc 93.40%, test acc 93.66%\n","epoch 92, loss 367.509, train acc 97.51%, valid acc 93.26%, test acc 94.14%\n","epoch 93, loss 367.576, train acc 97.44%, valid acc 93.54%, test acc 93.77%\n","epoch 94, loss 367.327, train acc 97.75%, valid acc 93.42%, test acc 94.08%\n","epoch 95, loss 367.526, train acc 97.47%, valid acc 93.03%, test acc 93.97%\n","epoch 96, loss 367.527, train acc 97.44%, valid acc 93.13%, test acc 94.01%\n","epoch 97, loss 367.587, train acc 97.43%, valid acc 92.69%, test acc 92.88%\n","epoch 98, loss 367.273, train acc 97.66%, valid acc 93.01%, test acc 93.70%\n","epoch 99, loss 367.188, train acc 97.73%, valid acc 92.97%, test acc 93.30%\n","epoch 100, loss 367.279, train acc 97.74%, valid acc 92.92%, test acc 93.43%\n","epoch 101, loss 367.642, train acc 97.31%, valid acc 92.88%, test acc 93.86%\n","epoch 102, loss 367.180, train acc 97.75%, valid acc 93.13%, test acc 93.81%\n","epoch 103, loss 367.115, train acc 97.80%, valid acc 91.62%, test acc 92.17%\n","epoch 104, loss 367.197, train acc 97.69%, valid acc 93.28%, test acc 93.90%\n","epoch 105, loss 367.093, train acc 97.76%, valid acc 93.10%, test acc 93.59%\n","epoch 106, loss 367.562, train acc 97.23%, valid acc 93.58%, test acc 93.92%\n","epoch 107, loss 367.133, train acc 97.76%, valid acc 93.10%, test acc 93.35%\n","epoch 108, loss 367.201, train acc 97.63%, valid acc 93.44%, test acc 94.10%\n","epoch 109, loss 367.084, train acc 97.77%, valid acc 93.26%, test acc 94.25%\n","epoch 110, loss 367.113, train acc 97.72%, valid acc 93.44%, test acc 93.86%\n","epoch 111, loss 366.868, train acc 97.95%, valid acc 94.20%, test acc 94.23%\n","epoch 112, loss 366.864, train acc 97.93%, valid acc 92.67%, test acc 93.08%\n","epoch 113, loss 366.942, train acc 97.88%, valid acc 93.74%, test acc 94.23%\n","epoch 114, loss 367.132, train acc 97.64%, valid acc 93.44%, test acc 93.59%\n","epoch 115, loss 366.909, train acc 97.85%, valid acc 93.08%, test acc 93.41%\n","epoch 116, loss 366.758, train acc 98.00%, valid acc 93.26%, test acc 93.66%\n","epoch 117, loss 366.910, train acc 97.81%, valid acc 93.44%, test acc 93.97%\n","epoch 118, loss 366.939, train acc 97.85%, valid acc 93.51%, test acc 93.52%\n","epoch 119, loss 367.161, train acc 97.62%, valid acc 91.60%, test acc 92.02%\n","epoch 120, loss 367.058, train acc 97.77%, valid acc 93.65%, test acc 93.81%\n","epoch 121, loss 366.683, train acc 98.00%, valid acc 92.92%, test acc 93.28%\n","epoch 122, loss 366.679, train acc 98.08%, valid acc 93.35%, test acc 93.41%\n","epoch 123, loss 366.899, train acc 97.84%, valid acc 93.81%, test acc 93.94%\n","epoch 124, loss 366.739, train acc 97.96%, valid acc 93.10%, test acc 93.08%\n","epoch 125, loss 366.763, train acc 97.92%, valid acc 92.62%, test acc 93.02%\n","epoch 126, loss 366.409, train acc 98.24%, valid acc 93.56%, test acc 93.44%\n","epoch 127, loss 366.495, train acc 98.12%, valid acc 93.15%, test acc 93.64%\n","epoch 128, loss 366.909, train acc 97.77%, valid acc 93.35%, test acc 93.92%\n","epoch 129, loss 366.593, train acc 98.06%, valid acc 93.58%, test acc 93.84%\n","epoch 130, loss 366.427, train acc 98.18%, valid acc 93.38%, test acc 94.14%\n","epoch 131, loss 366.671, train acc 97.96%, valid acc 93.51%, test acc 93.99%\n","epoch 132, loss 366.643, train acc 97.96%, valid acc 93.65%, test acc 94.04%\n","epoch 133, loss 366.739, train acc 97.88%, valid acc 93.10%, test acc 93.32%\n","epoch 134, loss 366.612, train acc 98.01%, valid acc 91.12%, test acc 91.51%\n","epoch 135, loss 366.684, train acc 97.94%, valid acc 93.28%, test acc 93.74%\n","epoch 136, loss 366.397, train acc 98.19%, valid acc 93.65%, test acc 94.03%\n","epoch 137, loss 366.356, train acc 98.22%, valid acc 93.38%, test acc 93.64%\n","epoch 138, loss 366.289, train acc 98.22%, valid acc 93.74%, test acc 93.97%\n","epoch 139, loss 366.460, train acc 98.03%, valid acc 93.51%, test acc 94.26%\n","epoch 140, loss 366.441, train acc 98.12%, valid acc 93.44%, test acc 94.08%\n","epoch 141, loss 366.425, train acc 98.05%, valid acc 93.31%, test acc 93.66%\n","epoch 142, loss 366.240, train acc 98.25%, valid acc 93.60%, test acc 93.68%\n","epoch 143, loss 366.520, train acc 97.96%, valid acc 93.72%, test acc 93.75%\n","epoch 144, loss 366.313, train acc 98.26%, valid acc 93.85%, test acc 94.14%\n","epoch 145, loss 366.053, train acc 98.37%, valid acc 93.60%, test acc 94.17%\n","epoch 146, loss 366.420, train acc 98.00%, valid acc 93.17%, test acc 93.35%\n","epoch 147, loss 366.347, train acc 98.22%, valid acc 93.79%, test acc 94.25%\n","epoch 148, loss 366.544, train acc 97.88%, valid acc 93.10%, test acc 93.57%\n","epoch 149, loss 366.293, train acc 98.18%, valid acc 93.38%, test acc 93.86%\n","epoch 150, loss 366.161, train acc 98.29%, valid acc 93.67%, test acc 94.23%\n","[42.14570290267501, 66.30051223676722, 70.27319294251565, 71.34319863403529, 72.65793966989186, 73.47182697780308, 74.40523619806488, 76.03870233352305, 77.08025042686397, 78.15025611838361, 79.14058053500284, 80.67159931701764, 80.56915196357427, 81.67330677290836, 82.544109277177, 83.0620375640296, 83.27262379055207, 83.73932840068298, 84.05236198064884, 83.51166761525327, 84.32555492316449, 85.16789982925441, 85.5378486055777, 85.45816733067728, 86.18668184405236, 86.3118952760387, 86.58508821855436, 87.11439954467843, 87.14854866249289, 87.30791121229369, 85.90779738190096, 87.34206033010814, 87.47865680136597, 87.61525327262379, 87.92828685258964, 87.99658508821855, 88.82185543540125, 89.09504837791691, 89.75526465566307, 90.59191804211724, 91.32612407512806, 91.65054069436539, 91.60500853727946, 92.01479795105293, 92.95389869095048, 92.67501422879909, 93.4092202618099, 93.61980648833239, 93.9669891861127, 94.08651109846329, 94.02390438247012, 94.54752418895845, 94.63289698349459, 94.83210017074559, 95.16220830961866, 94.39954467842914, 95.21343198634035, 95.12236767216847, 95.58907228229937, 95.63460443938531, 95.66306203756403, 95.66306203756403, 96.15822424587365, 96.0045532157086, 96.11269208878771, 96.46556630620375, 96.52248150256118, 96.10130904951622, 96.26067159931702, 96.42003414911781, 96.3517359134889, 96.65907797381901, 96.48833238474673, 96.62492885600456, 96.96642003414912, 97.11439954467843, 96.5110984632897, 96.63062037564029, 96.97211155378486, 96.93227091633466, 97.39897552646556, 97.34206033010814, 97.13147410358566, 97.057484348321, 97.26807057484348, 97.13147410358566, 97.49003984063745, 97.455890722823, 97.25099601593625, 97.2168468981218, 97.2168468981218, 97.50711439954468, 97.4445076835515, 97.75184974388162, 97.46727376209448, 97.43881616391576, 97.43312464428003, 97.66078542970973, 97.73477518497438, 97.74046670461013, 97.31360273192942, 97.74615822424587, 97.8030734206033, 97.69493454752418, 97.7632327831531, 97.22822993739328, 97.75754126351735, 97.63232783153101, 97.76892430278885, 97.71770062606716, 97.94536141149688, 97.93397837222538, 97.87706317586796, 97.64371087080251, 97.85429709732499, 98.0022766078543, 97.80876494023904, 97.84860557768924, 97.62094479225954, 97.7746158224246, 97.99658508821855, 98.0819578827547, 97.8429140580535, 97.9624359704041, 97.9225953329539, 98.2413204325555, 98.11610700056916, 97.76892430278885, 98.06488332384747, 98.18440523619806, 97.9624359704041, 97.9624359704041, 97.8827546955037, 98.00796812749005, 97.93966989186113, 98.1900967558338, 98.21855435401253, 98.22424587364826, 98.02504268639727, 98.12179852020489, 98.05350028457597, 98.25270347182698, 97.95674445076835, 98.26408651109847, 98.37222538417758, 98.0022766078543, 98.22424587364826, 97.87706317586796, 98.17871371656233, 98.29254410927717]\n","[64.76212155702254, 69.63350785340315, 70.99931709537901, 70.54404734805372, 71.93262007739585, 74.73252902344639, 75.09674482130663, 75.8251764170271, 76.41702708854997, 78.46574095151377, 77.00887776007285, 81.94855451855224, 79.44457090826315, 82.0396084680173, 76.66742544957887, 83.49647165945822, 81.90302754381972, 80.58274527657638, 83.72410653312087, 83.10949237423173, 82.90462098793535, 80.49169132711131, 83.58752560892329, 85.27202367402685, 82.95014796266788, 85.38584111085818, 86.00045526974732, 86.20532665604371, 86.29638060550876, 85.5679490097883, 84.90780787616663, 85.81834737081721, 85.90940132028227, 86.43296152970635, 84.90780787616663, 87.16139312542681, 81.58433872069202, 85.1809697245618, 83.17778283633052, 89.61984976098339, 89.27839745048942, 87.41179148645573, 87.77600728431595, 90.05235602094241, 90.46209879353518, 90.39380833143638, 89.73366719781471, 90.98565900295925, 90.75802412929662, 91.23605736398817, 89.414978374687, 90.8263145913954, 91.64580013658093, 90.98565900295925, 91.05394946505805, 90.14340997040746, 91.07671295242432, 90.4393353061689, 92.80673799226041, 92.57910311859777, 92.26041429547007, 92.35146824493512, 91.23605736398817, 92.57910311859777, 93.12542681538812, 92.64739358069656, 92.71568404279536, 92.35146824493512, 90.25722740723879, 92.82950147962669, 92.6246300933303, 91.89619849760983, 92.80673799226041, 92.78397450489415, 88.7093102663328, 92.943318916458, 91.39540177555202, 92.48804916913271, 92.78397450489415, 93.42135215114956, 93.12542681538812, 93.03437286592306, 92.46528568176645, 92.85226496699295, 93.12542681538812, 92.80673799226041, 92.44252219440018, 93.12542681538812, 92.169360346005, 92.943318916458, 93.39858866378329, 93.2620077395857, 93.53516958798087, 93.42135215114956, 93.03437286592306, 93.12542681538812, 92.6929205554291, 93.0116093785568, 92.96608240382426, 92.92055542909173, 92.87502845435921, 93.12542681538812, 91.62303664921465, 93.28477122695197, 93.10266332802185, 93.58069656271341, 93.10266332802185, 93.44411563851583, 93.2620077395857, 93.44411563851583, 94.19531072160255, 92.67015706806282, 93.74004097427726, 93.44411563851583, 93.07989984065559, 93.2620077395857, 93.44411563851583, 93.51240610061461, 91.60027316184839, 93.6489870248122, 92.92055542909173, 93.35306168905076, 93.80833143637605, 93.10266332802185, 92.6246300933303, 93.55793307534714, 93.14819030275439, 93.35306168905076, 93.58069656271341, 93.37582517641702, 93.51240610061461, 93.6489870248122, 93.10266332802185, 91.12223992715684, 93.28477122695197, 93.6489870248122, 93.37582517641702, 93.74004097427726, 93.51240610061461, 93.44411563851583, 93.30753471431824, 93.60346005007968, 93.717277486911, 93.85385841110858, 93.60346005007968, 93.17095379012065, 93.78556794900979, 93.10266332802185, 93.37582517641702, 93.67175051217846]\n","[64.99726825714806, 69.69586596248406, 71.57166272081588, 71.35312329266071, 72.20906938626844, 75.28683299945365, 75.59643052267347, 76.27026042615188, 76.85303223456565, 78.47386632671645, 76.87124385357858, 81.42414860681114, 79.7668912766345, 81.40593698779821, 76.3431069022036, 83.37279184119468, 82.15261336732836, 81.2238207976689, 84.06483336368603, 83.17246403205245, 82.60790384265161, 80.29502822800947, 83.70060098342742, 85.70387907484975, 83.2635221271171, 85.81314878892734, 86.46876707339283, 86.4505554543799, 86.94226916772901, 86.35949735931524, 85.8495720269532, 86.4505554543799, 86.72372973957386, 86.86942269167729, 86.06811145510837, 87.8346384993626, 81.91586232016026, 85.19395374248771, 83.99198688763431, 90.25678382808232, 90.27499544709525, 88.65416135494445, 88.49025678382809, 91.22199963576762, 91.27663449280641, 90.74849754143143, 90.89419049353488, 91.78655982516845, 91.82298306319431, 91.58623201602623, 90.14751411400474, 91.95046439628483, 92.75177563285376, 92.1143689674012, 91.22199963576762, 91.20378801675469, 91.78655982516845, 90.16572573301767, 92.95210344199599, 93.15243125113822, 92.80641048989256, 93.27991258422874, 91.75013658714259, 93.06137315607357, 93.38918229830632, 92.97031506100892, 93.37097067929339, 93.29812420324167, 91.45875068293572, 93.3163358222546, 93.02494991804771, 92.04152249134948, 93.00673829903478, 93.66235658350027, 89.05481697322892, 93.64414496448734, 92.18721544345293, 92.58787106173739, 93.22527772718995, 93.89910763066837, 93.84447277362958, 93.46202877435805, 92.7699872518667, 93.3163358222546, 93.38918229830632, 93.09779639409943, 93.22527772718995, 93.80804953560371, 92.4786013476598, 93.42560553633218, 93.66235658350027, 94.13585867783645, 93.77162629757785, 94.08122382079767, 93.97195410672009, 94.00837734474595, 92.87925696594428, 93.69877982152613, 93.29812420324167, 93.42560553633218, 93.8626843926425, 93.80804953560371, 92.16900382444, 93.89910763066837, 93.58951010744855, 93.9173192496813, 93.35275906028046, 94.0994354398106, 94.24512839191404, 93.8626843926425, 94.2269167729011, 93.0795847750865, 94.2269167729011, 93.58951010744855, 93.40739391731925, 93.66235658350027, 93.97195410672009, 93.51666363139684, 92.02331087233655, 93.80804953560371, 93.27991258422874, 93.40739391731925, 93.93553086869423, 93.0795847750865, 93.02494991804771, 93.44381715534512, 93.64414496448734, 93.9173192496813, 93.84447277362958, 94.13585867783645, 93.99016572573302, 94.04480058277181, 93.3163358222546, 91.51338553997451, 93.73520305955199, 94.02658896375888, 93.64414496448734, 93.97195410672009, 94.26334001092697, 94.08122382079767, 93.66235658350027, 93.6805682025132, 93.75341467856492, 94.13585867783645, 94.17228191586231, 93.35275906028046, 94.24512839191404, 93.57129848843562, 93.8626843926425, 94.2269167729011]\n"]}]},{"cell_type":"markdown","metadata":{"id":"V3Ec06IhSBTr"},"source":["## Results after Pruning the above Models"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UfoSSZdPRrt2","executionInfo":{"status":"ok","timestamp":1639868739704,"user_tz":300,"elapsed":21195,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"8d45b614-d821-420b-e27b-f2dc100a18ef"},"source":["PRUNE_THRESHOLD = 0.015\n","\n","class ThresholdPruning(prune.BasePruningMethod):\n","    PRUNING_TYPE = \"unstructured\"\n","\n","    def __init__(self, threshold):\n","        self.threshold = threshold\n","\n","    def compute_mask(self, tensor, default_mask):\n","      return torch.abs(tensor) > self.threshold\n","\n","PATHS = {'No penalty - 3 128 0.0001':            PATH + '/model/final/lr0.0001/no_penalty3.ptl',\n","         'l0 norm - 3 128 0.0001':               PATH + '/model/final/lr0.0001/l0_norm3.ptl',\n","         'l1 norm - 3 128 0.0001':               PATH + '/model/final/lr0.0001/l1_norm3.ptl',\n","         'l2 norm - 3 128 0.0001':               PATH + '/model/final/lr0.0001/l2_norm3.ptl',\n","         'group lasso - 3 128 0.0001':           PATH + '/model/final/lr0.0001/group_lasso3.ptl',\n","         'l1 group lasso - 3 128 0.0001':        PATH + '/model/final/lr0.0001/l1_group_lasso3.ptl',\n","         'l0 group lasso - 3 128 0.0001 (BEST)': PATH + '/model/final/lr0.0001/l0_group_lasso3.ptl',\n","         'l0 group lasso - 3 128 0.00005':       PATH + '/model/final/lr0.00005/l0_group_lasso3.ptl',\n","         'l0 group lasso - 3 128 0.00015':       PATH + '/model/final/lr0.00015/l0_group_lasso3.ptl',\n","         'l0 group lasso - 3 128 0.0002':        PATH + '/model/final/lr0.0002/l0_group_lasso3.ptl',\n","         'l0 group lasso - 3 128 0.00001':       PATH + '/model/final/lr0.00001/l0_group_lasso3.ptl',\n","         'l0 group lasso - 3 256 0.0001':        PATH + '/model/final/256node/l0_group_lasso3.ptl',\n","         'l0 group lasso - 3 64 0.0001':         PATH + '/model/final/64node/l0_group_lasso3.ptl',\n","         'l0 group lasso - 0 128 0.0001':        PATH + '/model/final/lr0.0001/l0_group_lasso0.ptl',\n","         'l0 group lasso - 1 128 0.0001':        PATH + '/model/final/lr0.0001/l0_group_lasso1.ptl',\n","         'l0 group lasso - 2 128 0.0001':        PATH + '/model/final/lr0.0001/l0_group_lasso2.ptl',\n","         'l0 group lasso - 4 128 0.0001':        PATH + '/model/final/lr0.0001/l0_group_lasso4.ptl',\n","        }\n","\n","for name in PATHS:\n","  print('Here are the results for {}:'.format(name))\n","  # load the model\n","  net = torch.load(PATHS[name])\n","\n","  # display the results before compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% before compression' % (test_total, test_acc))\n","\n","  # prune the model\n","  parameters_to_prune = []\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      parameters_to_prune.append((child, \"weight\"))\n","  prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold=PRUNE_THRESHOLD)\n","\n","  # calculate the sparsity\n","  total_weight = 0\n","  total_nonzero = 0\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      total_weight += torch.numel(child.weight)\n","      total_nonzero += torch.count_nonzero(child.weight)\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_nonzero / total_weight)))\n","\n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for No penalty - 3 128 0.0001:\n","Accuracy of the network on the 5491 test data: 94.79 % before compression\n","Sparity for the compressed model: 60.10 %\n","Accuracy of the network on the 5491 test data: 94.63 % after compression\n","\n","Here are the results for l0 norm - 3 128 0.0001:\n","Accuracy of the network on the 5491 test data: 94.72 % before compression\n","Sparity for the compressed model: 63.36 %\n","Accuracy of the network on the 5491 test data: 94.79 % after compression\n","\n","Here are the results for l1 norm - 3 128 0.0001:\n","Accuracy of the network on the 5491 test data: 94.30 % before compression\n","Sparity for the compressed model: 13.58 %\n","Accuracy of the network on the 5491 test data: 93.84 % after compression\n","\n","Here are the results for l2 norm - 3 128 0.0001:\n","Accuracy of the network on the 5491 test data: 94.61 % before compression\n","Sparity for the compressed model: 56.28 %\n","Accuracy of the network on the 5491 test data: 94.54 % after compression\n","\n","Here are the results for group lasso - 3 128 0.0001:\n","Accuracy of the network on the 5491 test data: 94.68 % before compression\n","Sparity for the compressed model: 48.23 %\n","Accuracy of the network on the 5491 test data: 94.32 % after compression\n","\n","Here are the results for l1 group lasso - 3 128 0.0001:\n","Accuracy of the network on the 5491 test data: 94.81 % before compression\n","Sparity for the compressed model: 17.91 %\n","Accuracy of the network on the 5491 test data: 94.79 % after compression\n","\n","Here are the results for l0 group lasso - 3 128 0.0001 (BEST):\n","Accuracy of the network on the 5491 test data: 94.97 % before compression\n","Sparity for the compressed model: 9.52 %\n","Accuracy of the network on the 5491 test data: 94.65 % after compression\n","\n","Here are the results for l0 group lasso - 3 128 0.00005:\n","Accuracy of the network on the 5491 test data: 92.93 % before compression\n","Sparity for the compressed model: 9.41 %\n","Accuracy of the network on the 5491 test data: 84.36 % after compression\n","\n","Here are the results for l0 group lasso - 3 128 0.00015:\n","Accuracy of the network on the 5491 test data: 94.96 % before compression\n","Sparity for the compressed model: 10.38 %\n","Accuracy of the network on the 5491 test data: 94.88 % after compression\n","\n","Here are the results for l0 group lasso - 3 128 0.0002:\n","Accuracy of the network on the 5491 test data: 94.65 % before compression\n","Sparity for the compressed model: 10.54 %\n","Accuracy of the network on the 5491 test data: 94.57 % after compression\n","\n","Here are the results for l0 group lasso - 3 128 0.00001:\n","Accuracy of the network on the 5491 test data: 89.55 % before compression\n","Sparity for the compressed model: 27.09 %\n","Accuracy of the network on the 5491 test data: 86.72 % after compression\n","\n","Here are the results for l0 group lasso - 3 256 0.0001:\n","Accuracy of the network on the 5491 test data: 94.23 % before compression\n","Sparity for the compressed model: 2.61 %\n","Accuracy of the network on the 5491 test data: 91.93 % after compression\n","\n","Here are the results for l0 group lasso - 3 64 0.0001:\n","Accuracy of the network on the 5491 test data: 92.66 % before compression\n","Sparity for the compressed model: 25.63 %\n","Accuracy of the network on the 5491 test data: 92.24 % after compression\n","\n","Here are the results for l0 group lasso - 0 128 0.0001:\n","Accuracy of the network on the 5491 test data: 93.52 % before compression\n","Sparity for the compressed model: 11.64 %\n","Accuracy of the network on the 5491 test data: 92.68 % after compression\n","\n","Here are the results for l0 group lasso - 1 128 0.0001:\n","Accuracy of the network on the 5491 test data: 94.88 % before compression\n","Sparity for the compressed model: 10.03 %\n","Accuracy of the network on the 5491 test data: 93.70 % after compression\n","\n","Here are the results for l0 group lasso - 2 128 0.0001:\n","Accuracy of the network on the 5491 test data: 94.45 % before compression\n","Sparity for the compressed model: 9.45 %\n","Accuracy of the network on the 5491 test data: 93.48 % after compression\n","\n","Here are the results for l0 group lasso - 4 128 0.0001:\n","Accuracy of the network on the 5491 test data: 93.52 % before compression\n","Sparity for the compressed model: 11.64 %\n","Accuracy of the network on the 5491 test data: 92.68 % after compression\n","\n"]}]},{"cell_type":"markdown","source":["# Threthold check"],"metadata":{"id":"vWcs68OG2AiN"}},{"cell_type":"code","source":["PRUNE_THRESHOLD = np.arange(0.005, 0.03, 0.005)\n","\n","class ThresholdPruning(prune.BasePruningMethod):\n","    PRUNING_TYPE = \"unstructured\"\n","\n","    def __init__(self, threshold):\n","        self.threshold = threshold\n","\n","    def compute_mask(self, tensor, default_mask):\n","      return torch.abs(tensor) > self.threshold\n","\n","PATHS = PATH + '/model/final/lr0.0001/l0_group_lasso3.ptl'\n","\n","for threshold in PRUNE_THRESHOLD:\n","  print('Here are the results for threshold {}:'.format(threshold))\n","  # load the model\n","  net = torch.load(PATHS)\n","\n","  # display the results before compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% before compression' % (test_total, test_acc))\n","\n","  # prune the model\n","  parameters_to_prune = []\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      parameters_to_prune.append((child, \"weight\"))\n","  prune.global_unstructured(parameters_to_prune, pruning_method=ThresholdPruning, threshold=threshold)\n","\n","  # calculate the sparsity\n","  total_weight = 0\n","  total_nonzero = 0\n","  for name, child in net.features.named_children():\n","    if int(name) % 2 == 0:\n","      total_weight += torch.numel(child.weight)\n","      total_nonzero += torch.count_nonzero(child.weight)\n","  print('Sparity for the compressed model: %.2f %%' % (100*float(total_nonzero / total_weight)))\n","\n","  # display the results after compressed model\n","  test_correct = 0\n","  test_total = 0\n","  with torch.no_grad():\n","    for data in testloader:\n","      inputs, labels = data\n","      inputs, labels = inputs.cuda(0), labels.cuda(0)\n","      outputs = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      test_total += labels.size(0)\n","      test_correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n","    test_acc = 100 * test_correct / test_total\n","  print('Accuracy of the network on the %d test data: %.2f %% after compression\\n' % (test_total, test_acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxyOo-fd1yQj","executionInfo":{"status":"ok","timestamp":1639836757306,"user_tz":300,"elapsed":3348,"user":{"displayName":"Yubo Shao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02031292746005113172"}},"outputId":"116f8835-7756-4bfd-f6bd-b2389a49fa28"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Here are the results for threshold 0.005:\n","Accuracy of the network on the 5491 test data: 94.97 % before compression\n","Sparity for the compressed model: 18.96 %\n","Accuracy of the network on the 5491 test data: 95.12 % after compression\n","\n","Here are the results for threshold 0.01:\n","Accuracy of the network on the 5491 test data: 94.97 % before compression\n","Sparity for the compressed model: 13.54 %\n","Accuracy of the network on the 5491 test data: 94.92 % after compression\n","\n","Here are the results for threshold 0.015:\n","Accuracy of the network on the 5491 test data: 94.97 % before compression\n","Sparity for the compressed model: 9.52 %\n","Accuracy of the network on the 5491 test data: 94.65 % after compression\n","\n","Here are the results for threshold 0.02:\n","Accuracy of the network on the 5491 test data: 94.97 % before compression\n","Sparity for the compressed model: 6.59 %\n","Accuracy of the network on the 5491 test data: 94.01 % after compression\n","\n","Here are the results for threshold 0.025:\n","Accuracy of the network on the 5491 test data: 94.97 % before compression\n","Sparity for the compressed model: 4.44 %\n","Accuracy of the network on the 5491 test data: 91.42 % after compression\n","\n"]}]}]}